{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Random seed\n",
    "# np.random.seed(500)\n",
    "\n",
    "# Add the Data using pandas\n",
    "df = pd.read_csv(r\"C:\\Users\\Acer\\FYP_1.1_pro\\audio\\metadata_new\\final_metadata\\combined_csv_new_updated - Copy.csv\",encoding='latin-1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_of_audio</th>\n",
       "      <th>feature</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2_P1.wav</td>\n",
       "      <td>[-3.6422656e+02  1.3304326e+02 -2.4707306e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3_P1.wav</td>\n",
       "      <td>[-3.7986017e+02  1.1947180e+02 -3.1631770e+00 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E2_P10.wav</td>\n",
       "      <td>[-3.5354391e+02  1.4500945e+02 -3.1722630e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E2_P11.wav</td>\n",
       "      <td>[-3.5276407e+02  1.3659334e+02 -3.1263487e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E1_P12.wav</td>\n",
       "      <td>[-4.27301239e+02  1.29095963e+02 -3.96940780e+...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_of_audio                                            feature  lable\n",
       "0     E2_P1.wav  [-3.6422656e+02  1.3304326e+02 -2.4707306e+01 ...      1\n",
       "1     E3_P1.wav  [-3.7986017e+02  1.1947180e+02 -3.1631770e+00 ...      1\n",
       "2    E2_P10.wav  [-3.5354391e+02  1.4500945e+02 -3.1722630e+01 ...      1\n",
       "3    E2_P11.wav  [-3.5276407e+02  1.3659334e+02 -3.1263487e+01 ...      1\n",
       "4    E1_P12.wav  [-4.27301239e+02  1.29095963e+02 -3.96940780e+...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with CNN*** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:159: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_of_audio</th>\n",
       "      <th>feature</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2_P1.wav</td>\n",
       "      <td>[-3.6422656e+02  1.3304326e+02 -2.4707306e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3_P1.wav</td>\n",
       "      <td>[-3.7986017e+02  1.1947180e+02 -3.1631770e+00 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E2_P10.wav</td>\n",
       "      <td>[-3.5354391e+02  1.4500945e+02 -3.1722630e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E2_P11.wav</td>\n",
       "      <td>[-3.5276407e+02  1.3659334e+02 -3.1263487e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E1_P12.wav</td>\n",
       "      <td>[-4.27301239e+02  1.29095963e+02 -3.96940780e+...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_of_audio                                            feature  lable\n",
       "0     E2_P1.wav  [-3.6422656e+02  1.3304326e+02 -2.4707306e+01 ...      1\n",
       "1     E3_P1.wav  [-3.7986017e+02  1.1947180e+02 -3.1631770e+00 ...      1\n",
       "2    E2_P10.wav  [-3.5354391e+02  1.4500945e+02 -3.1722630e+01 ...      1\n",
       "3    E2_P11.wav  [-3.5276407e+02  1.3659334e+02 -3.1263487e+01 ...      1\n",
       "4    E1_P12.wav  [-4.27301239e+02  1.29095963e+02 -3.96940780e+...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata=pd.read_csv('audio/metadata_new/final_metadata/combined_csv_new_updated - Copy.csv')\n",
    "df = pd.read_csv('audio/metadata_new/final_metadata/combined_csv_new_updated - Copy.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio('audio/rec/fold1/S1_P15_F.wav')\n",
    "data, sampling_rate = librosa.load('audio/rec - Copy/S1_P1_F.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.00143572,\n",
       "       -0.00139209, -0.00141668], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1a361676940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1a361b22280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = random.choice(df.index)\n",
    "\n",
    "print('lable:', df['lable'][index])\n",
    "data, sampling_rate = librosa.load('audio/rec - Copy/'+str(df['name_of_audio'][index]))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='lable', ylabel='count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.countplot(df['lable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "508it [06:34,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join('audio/rec - Copy/', str(row.name_of_audio))\n",
    "    final_class_labels=row[\"lable\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-364.22656, 133.04326, -24.707306, 8.948652, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-379.86017, 119.4718, -3.163177, 25.166592, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-353.5439, 145.00945, -31.72263, 8.87748, 17....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-352.76407, 136.59334, -31.263487, 8.083315, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-427.30124, 129.09596, -3.9694078, 8.760965, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  class\n",
       "0  [-364.22656, 133.04326, -24.707306, 8.948652, ...      1\n",
       "1  [-379.86017, 119.4718, -3.163177, 25.166592, -...      1\n",
       "2  [-353.5439, 145.00945, -31.72263, 8.87748, 17....      1\n",
       "3  [-352.76407, 136.59334, -31.263487, 8.083315, ...      1\n",
       "4  [-427.30124, 129.09596, -3.9694078, 8.760965, ...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "extracted_features_df.to_csv('audio/metadata_new/final_metadata/extracted_features_df.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(508, 40)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding\n",
    "###y=np.array(pd.get_dummies(y))\n",
    "### Label Encoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-384.79132  ,  126.83416  ,   22.044748 , ...,    3.6588137,\n",
       "           4.157282 ,    4.989248 ],\n",
       "       [-332.5388   ,   92.562675 ,   -3.9492354, ...,    5.115012 ,\n",
       "           4.4112234,    2.8012795],\n",
       "       [-315.8667   ,  120.47542  ,   20.410995 , ...,   -2.1051085,\n",
       "          -1.2823454,   -1.5356203],\n",
       "       ...,\n",
       "       [-392.28574  ,   88.3984   ,   -4.571233 , ...,   -1.4070433,\n",
       "          -2.783642 ,   -1.0058346],\n",
       "       [-393.10715  ,  157.4563   ,  -13.183333 , ...,    4.2225337,\n",
       "           3.0959253,    2.988069 ],\n",
       "       [-339.63928  ,  100.59851  ,   -1.3436158, ...,   -1.5343856,\n",
       "          -0.6541061,   -2.4956307]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 40)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### -------- Model Creation -------- ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               4100      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3742 - accuracy: 0.7188\n",
      "Epoch 00001: val_loss improved from inf to 0.48944, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4597 - accuracy: 0.7315 - val_loss: 0.4894 - val_accuracy: 0.7157\n",
      "Epoch 2/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3414 - accuracy: 0.8125\n",
      "Epoch 00002: val_loss improved from 0.48944 to 0.48195, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4332 - accuracy: 0.7512 - val_loss: 0.4819 - val_accuracy: 0.7157\n",
      "Epoch 3/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4215 - accuracy: 0.8125\n",
      "Epoch 00003: val_loss improved from 0.48195 to 0.47618, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.7857 - val_loss: 0.4762 - val_accuracy: 0.7255\n",
      "Epoch 4/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4196 - accuracy: 0.8125\n",
      "Epoch 00004: val_loss did not improve from 0.47618\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4195 - accuracy: 0.7783 - val_loss: 0.4918 - val_accuracy: 0.7549\n",
      "Epoch 5/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3776 - accuracy: 0.8438\n",
      "Epoch 00005: val_loss did not improve from 0.47618\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.7488 - val_loss: 0.4773 - val_accuracy: 0.7157\n",
      "Epoch 6/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3495 - accuracy: 0.7812\n",
      "Epoch 00006: val_loss improved from 0.47618 to 0.47568, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.7463 - val_loss: 0.4757 - val_accuracy: 0.7255\n",
      "Epoch 7/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4126 - accuracy: 0.6875\n",
      "Epoch 00007: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.7611 - val_loss: 0.4782 - val_accuracy: 0.7255\n",
      "Epoch 8/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8125\n",
      "Epoch 00008: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.7660 - val_loss: 0.4846 - val_accuracy: 0.7353\n",
      "Epoch 9/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3606 - accuracy: 0.7812\n",
      "Epoch 00009: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.7512 - val_loss: 0.4838 - val_accuracy: 0.7549\n",
      "Epoch 10/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4952 - accuracy: 0.7188\n",
      "Epoch 00010: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4109 - accuracy: 0.7685 - val_loss: 0.4860 - val_accuracy: 0.7549\n",
      "Epoch 11/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5170 - accuracy: 0.7188\n",
      "Epoch 00011: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.7562 - val_loss: 0.5150 - val_accuracy: 0.7549\n",
      "Epoch 12/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4091 - accuracy: 0.7500\n",
      "Epoch 00012: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4021 - accuracy: 0.7586 - val_loss: 0.5117 - val_accuracy: 0.7549\n",
      "Epoch 13/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3454 - accuracy: 0.7188\n",
      "Epoch 00013: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.7512 - val_loss: 0.4796 - val_accuracy: 0.7451\n",
      "Epoch 14/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3475 - accuracy: 0.8125\n",
      "Epoch 00014: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.7414 - val_loss: 0.4890 - val_accuracy: 0.7255\n",
      "Epoch 15/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4272 - accuracy: 0.7500\n",
      "Epoch 00015: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.7315 - val_loss: 0.4896 - val_accuracy: 0.7255\n",
      "Epoch 16/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4388 - accuracy: 0.7188\n",
      "Epoch 00016: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.7586 - val_loss: 0.5000 - val_accuracy: 0.7255\n",
      "Epoch 17/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4202 - accuracy: 0.7500\n",
      "Epoch 00017: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.7463 - val_loss: 0.4957 - val_accuracy: 0.7451\n",
      "Epoch 18/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3931 - accuracy: 0.6875\n",
      "Epoch 00018: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.7685 - val_loss: 0.4828 - val_accuracy: 0.7451\n",
      "Epoch 19/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3015 - accuracy: 0.9375\n",
      "Epoch 00019: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4019 - accuracy: 0.7857 - val_loss: 0.4844 - val_accuracy: 0.7549\n",
      "Epoch 20/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3137 - accuracy: 0.7812\n",
      "Epoch 00020: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4169 - accuracy: 0.7562 - val_loss: 0.5020 - val_accuracy: 0.7353\n",
      "Epoch 21/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3189 - accuracy: 0.8750\n",
      "Epoch 00021: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.7808 - val_loss: 0.4885 - val_accuracy: 0.7451\n",
      "Epoch 22/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3614 - accuracy: 0.8125\n",
      "Epoch 00022: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3969 - accuracy: 0.7488 - val_loss: 0.4990 - val_accuracy: 0.7451\n",
      "Epoch 23/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3684 - accuracy: 0.8125\n",
      "Epoch 00023: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.7586 - val_loss: 0.5081 - val_accuracy: 0.7549\n",
      "Epoch 24/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4043 - accuracy: 0.8125\n",
      "Epoch 00024: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.7759 - val_loss: 0.5023 - val_accuracy: 0.7549\n",
      "Epoch 25/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3483 - accuracy: 0.7500\n",
      "Epoch 00025: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.7414 - val_loss: 0.5031 - val_accuracy: 0.7549\n",
      "Epoch 26/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3284 - accuracy: 0.7812\n",
      "Epoch 00026: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.7709 - val_loss: 0.5149 - val_accuracy: 0.7451\n",
      "Epoch 27/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5019 - accuracy: 0.7812\n",
      "Epoch 00027: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3924 - accuracy: 0.7956 - val_loss: 0.4951 - val_accuracy: 0.7451\n",
      "Epoch 28/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3124 - accuracy: 0.8125\n",
      "Epoch 00028: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.7956 - val_loss: 0.5077 - val_accuracy: 0.7451\n",
      "Epoch 29/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3102 - accuracy: 0.7812\n",
      "Epoch 00029: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.7956 - val_loss: 0.5169 - val_accuracy: 0.7647\n",
      "Epoch 30/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3767 - accuracy: 0.7500\n",
      "Epoch 00030: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.7734 - val_loss: 0.5240 - val_accuracy: 0.7843\n",
      "Epoch 31/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4082 - accuracy: 0.7188\n",
      "Epoch 00031: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.7709 - val_loss: 0.4923 - val_accuracy: 0.7647\n",
      "Epoch 32/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3881 - accuracy: 0.6875\n",
      "Epoch 00032: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.7734 - val_loss: 0.4984 - val_accuracy: 0.7647\n",
      "Epoch 33/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3708 - accuracy: 0.7500\n",
      "Epoch 00033: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.7833 - val_loss: 0.4922 - val_accuracy: 0.7745\n",
      "Epoch 34/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4108 - accuracy: 0.6250\n",
      "Epoch 00034: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3506 - accuracy: 0.7906 - val_loss: 0.5005 - val_accuracy: 0.7745\n",
      "Epoch 35/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3396 - accuracy: 0.7500\n",
      "Epoch 00035: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.7980 - val_loss: 0.5008 - val_accuracy: 0.7745\n",
      "Epoch 36/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3862 - accuracy: 0.7500\n",
      "Epoch 00036: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.7808 - val_loss: 0.4845 - val_accuracy: 0.7647\n",
      "Epoch 37/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3955 - accuracy: 0.7500\n",
      "Epoch 00037: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.7980 - val_loss: 0.4839 - val_accuracy: 0.7647\n",
      "Epoch 38/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4735 - accuracy: 0.8125\n",
      "Epoch 00038: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.7956 - val_loss: 0.5033 - val_accuracy: 0.7647\n",
      "Epoch 39/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3232 - accuracy: 0.8125\n",
      "Epoch 00039: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3519 - accuracy: 0.7931 - val_loss: 0.5169 - val_accuracy: 0.7647\n",
      "Epoch 40/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3988 - accuracy: 0.6875\n",
      "Epoch 00040: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3741 - accuracy: 0.8005 - val_loss: 0.5047 - val_accuracy: 0.7745\n",
      "Epoch 41/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4431 - accuracy: 0.8438\n",
      "Epoch 00041: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3539 - accuracy: 0.7956 - val_loss: 0.4961 - val_accuracy: 0.7843\n",
      "Epoch 42/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2453 - accuracy: 0.9062\n",
      "Epoch 00042: val_loss did not improve from 0.47568\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.7857 - val_loss: 0.4948 - val_accuracy: 0.7745\n",
      "Epoch 43/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4418 - accuracy: 0.7500\n",
      "Epoch 00043: val_loss improved from 0.47568 to 0.45900, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3831 - accuracy: 0.8054 - val_loss: 0.4590 - val_accuracy: 0.7451\n",
      "Epoch 44/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3142 - accuracy: 0.8750\n",
      "Epoch 00044: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3815 - accuracy: 0.8030 - val_loss: 0.4878 - val_accuracy: 0.7843\n",
      "Epoch 45/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2821 - accuracy: 0.8125\n",
      "Epoch 00045: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.7833 - val_loss: 0.5129 - val_accuracy: 0.7843\n",
      "Epoch 46/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3553 - accuracy: 0.7500\n",
      "Epoch 00046: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.7931 - val_loss: 0.4955 - val_accuracy: 0.8039\n",
      "Epoch 47/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2447 - accuracy: 0.8125\n",
      "Epoch 00047: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.7759 - val_loss: 0.4991 - val_accuracy: 0.7647\n",
      "Epoch 48/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2831 - accuracy: 0.8438\n",
      "Epoch 00048: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3478 - accuracy: 0.7956 - val_loss: 0.5105 - val_accuracy: 0.7941\n",
      "Epoch 49/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3379 - accuracy: 0.7812\n",
      "Epoch 00049: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.8202 - val_loss: 0.4990 - val_accuracy: 0.8039\n",
      "Epoch 50/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3476 - accuracy: 0.8125\n",
      "Epoch 00050: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8202 - val_loss: 0.4830 - val_accuracy: 0.7843\n",
      "Epoch 51/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2621 - accuracy: 0.8750\n",
      "Epoch 00051: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8079 - val_loss: 0.5008 - val_accuracy: 0.7745\n",
      "Epoch 52/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2429 - accuracy: 0.8438\n",
      "Epoch 00052: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8103 - val_loss: 0.4996 - val_accuracy: 0.7745\n",
      "Epoch 53/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3975 - accuracy: 0.8125\n",
      "Epoch 00053: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.7980 - val_loss: 0.4731 - val_accuracy: 0.7843\n",
      "Epoch 54/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3974 - accuracy: 0.6562\n",
      "Epoch 00054: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8227 - val_loss: 0.4953 - val_accuracy: 0.7843\n",
      "Epoch 55/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2547 - accuracy: 0.9062\n",
      "Epoch 00055: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3406 - accuracy: 0.8079 - val_loss: 0.5255 - val_accuracy: 0.7647\n",
      "Epoch 56/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4652 - accuracy: 0.7188\n",
      "Epoch 00056: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3479 - accuracy: 0.8202 - val_loss: 0.5295 - val_accuracy: 0.7745\n",
      "Epoch 57/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2726 - accuracy: 0.8750\n",
      "Epoch 00057: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3514 - accuracy: 0.8030 - val_loss: 0.5119 - val_accuracy: 0.7745\n",
      "Epoch 58/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3395 - accuracy: 0.8438\n",
      "Epoch 00058: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8177 - val_loss: 0.5428 - val_accuracy: 0.7941\n",
      "Epoch 59/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3553 - accuracy: 0.8438\n",
      "Epoch 00059: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.8103 - val_loss: 0.5015 - val_accuracy: 0.7745\n",
      "Epoch 60/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5150 - accuracy: 0.8125\n",
      "Epoch 00060: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.7931 - val_loss: 0.4749 - val_accuracy: 0.7549\n",
      "Epoch 61/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2782 - accuracy: 0.9062\n",
      "Epoch 00061: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8227 - val_loss: 0.5215 - val_accuracy: 0.7745\n",
      "Epoch 62/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4310 - accuracy: 0.7188\n",
      "Epoch 00062: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8153 - val_loss: 0.5400 - val_accuracy: 0.7647\n",
      "Epoch 63/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2990 - accuracy: 0.8438\n",
      "Epoch 00063: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.7857 - val_loss: 0.4735 - val_accuracy: 0.7843\n",
      "Epoch 64/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2767 - accuracy: 0.9375\n",
      "Epoch 00064: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8227 - val_loss: 0.4912 - val_accuracy: 0.7549\n",
      "Epoch 65/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2300 - accuracy: 0.8125\n",
      "Epoch 00065: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.7759 - val_loss: 0.4823 - val_accuracy: 0.7647\n",
      "Epoch 66/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2513 - accuracy: 0.8125\n",
      "Epoch 00066: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.7956 - val_loss: 0.5172 - val_accuracy: 0.7745\n",
      "Epoch 67/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2995 - accuracy: 0.8750\n",
      "Epoch 00067: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.7931 - val_loss: 0.4734 - val_accuracy: 0.7745\n",
      "Epoch 68/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3768 - accuracy: 0.8438\n",
      "Epoch 00068: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8153 - val_loss: 0.5068 - val_accuracy: 0.7843\n",
      "Epoch 69/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4049 - accuracy: 0.7500\n",
      "Epoch 00069: val_loss did not improve from 0.45900\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8030 - val_loss: 0.5004 - val_accuracy: 0.7647\n",
      "Epoch 70/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3316 - accuracy: 0.7812\n",
      "Epoch 00070: val_loss improved from 0.45900 to 0.45837, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.7808 - val_loss: 0.4584 - val_accuracy: 0.7647\n",
      "Epoch 71/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4753 - accuracy: 0.6875\n",
      "Epoch 00071: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8079 - val_loss: 0.5114 - val_accuracy: 0.7745\n",
      "Epoch 72/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1990 - accuracy: 0.9062\n",
      "Epoch 00072: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8251 - val_loss: 0.5202 - val_accuracy: 0.7647\n",
      "Epoch 73/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3524 - accuracy: 0.8125\n",
      "Epoch 00073: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.7857 - val_loss: 0.5549 - val_accuracy: 0.7647\n",
      "Epoch 74/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3114 - accuracy: 0.8438\n",
      "Epoch 00074: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3294 - accuracy: 0.8030 - val_loss: 0.5312 - val_accuracy: 0.7647\n",
      "Epoch 75/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3036 - accuracy: 0.8438\n",
      "Epoch 00075: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.8276 - val_loss: 0.5198 - val_accuracy: 0.7549\n",
      "Epoch 76/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3566 - accuracy: 0.7812\n",
      "Epoch 00076: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.7980 - val_loss: 0.5045 - val_accuracy: 0.7647\n",
      "Epoch 77/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3841 - accuracy: 0.6875\n",
      "Epoch 00077: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8005 - val_loss: 0.5014 - val_accuracy: 0.7647\n",
      "Epoch 78/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3288 - accuracy: 0.8125\n",
      "Epoch 00078: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3075 - accuracy: 0.7931 - val_loss: 0.5324 - val_accuracy: 0.7647\n",
      "Epoch 79/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2822 - accuracy: 0.8438\n",
      "Epoch 00079: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.7906 - val_loss: 0.5027 - val_accuracy: 0.7745\n",
      "Epoch 80/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2082 - accuracy: 0.9062\n",
      "Epoch 00080: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3304 - accuracy: 0.8300 - val_loss: 0.5287 - val_accuracy: 0.7745\n",
      "Epoch 81/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3077 - accuracy: 0.7500\n",
      "Epoch 00081: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8177 - val_loss: 0.5408 - val_accuracy: 0.7647\n",
      "Epoch 82/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2937 - accuracy: 0.7812\n",
      "Epoch 00082: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8448 - val_loss: 0.5260 - val_accuracy: 0.7647\n",
      "Epoch 83/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2335 - accuracy: 0.8125\n",
      "Epoch 00083: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3311 - accuracy: 0.8054 - val_loss: 0.5367 - val_accuracy: 0.7549\n",
      "Epoch 84/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3249 - accuracy: 0.7188\n",
      "Epoch 00084: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8473 - val_loss: 0.4912 - val_accuracy: 0.7549\n",
      "Epoch 85/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2115 - accuracy: 0.9375\n",
      "Epoch 00085: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8054 - val_loss: 0.5603 - val_accuracy: 0.7745\n",
      "Epoch 86/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3132 - accuracy: 0.8125\n",
      "Epoch 00086: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8153 - val_loss: 0.5345 - val_accuracy: 0.7647\n",
      "Epoch 87/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2667 - accuracy: 0.7812\n",
      "Epoch 00087: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.8227 - val_loss: 0.5519 - val_accuracy: 0.7745\n",
      "Epoch 88/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2537 - accuracy: 0.8125\n",
      "Epoch 00088: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.8325 - val_loss: 0.5812 - val_accuracy: 0.7843\n",
      "Epoch 89/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2132 - accuracy: 0.9375\n",
      "Epoch 00089: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8128 - val_loss: 0.4794 - val_accuracy: 0.7843\n",
      "Epoch 90/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2314 - accuracy: 0.8750\n",
      "Epoch 00090: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8251 - val_loss: 0.4909 - val_accuracy: 0.7843\n",
      "Epoch 91/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3942 - accuracy: 0.7500\n",
      "Epoch 00091: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8079 - val_loss: 0.5366 - val_accuracy: 0.7745\n",
      "Epoch 92/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3194 - accuracy: 0.7500\n",
      "Epoch 00092: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3389 - accuracy: 0.8227 - val_loss: 0.5109 - val_accuracy: 0.7647\n",
      "Epoch 93/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2458 - accuracy: 0.8750\n",
      "Epoch 00093: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2907 - accuracy: 0.8374 - val_loss: 0.5561 - val_accuracy: 0.7843\n",
      "Epoch 94/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4470 - accuracy: 0.7188\n",
      "Epoch 00094: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8177 - val_loss: 0.5278 - val_accuracy: 0.7745\n",
      "Epoch 95/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1714 - accuracy: 0.9375\n",
      "Epoch 00095: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2810 - accuracy: 0.8547 - val_loss: 0.5758 - val_accuracy: 0.7745\n",
      "Epoch 96/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1879 - accuracy: 0.8438\n",
      "Epoch 00096: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2821 - accuracy: 0.8596 - val_loss: 0.5942 - val_accuracy: 0.7745\n",
      "Epoch 97/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2575 - accuracy: 0.9062\n",
      "Epoch 00097: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3030 - accuracy: 0.8128 - val_loss: 0.5459 - val_accuracy: 0.7647\n",
      "Epoch 98/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3259 - accuracy: 0.7500\n",
      "Epoch 00098: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8325 - val_loss: 0.5104 - val_accuracy: 0.7451\n",
      "Epoch 99/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2591 - accuracy: 0.8750\n",
      "Epoch 00099: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3012 - accuracy: 0.8153 - val_loss: 0.5631 - val_accuracy: 0.7647\n",
      "Epoch 100/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4445 - accuracy: 0.6875\n",
      "Epoch 00100: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.8103 - val_loss: 0.5322 - val_accuracy: 0.7549\n",
      "Epoch 101/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1321 - accuracy: 0.9062\n",
      "Epoch 00101: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2954 - accuracy: 0.8448 - val_loss: 0.5336 - val_accuracy: 0.7549\n",
      "Epoch 102/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3045 - accuracy: 0.8125\n",
      "Epoch 00102: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.8079 - val_loss: 0.5843 - val_accuracy: 0.7745\n",
      "Epoch 103/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2621 - accuracy: 0.9375\n",
      "Epoch 00103: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3145 - accuracy: 0.8276 - val_loss: 0.5153 - val_accuracy: 0.7843\n",
      "Epoch 104/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3063 - accuracy: 0.8125\n",
      "Epoch 00104: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8473 - val_loss: 0.5357 - val_accuracy: 0.7549\n",
      "Epoch 105/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2890 - accuracy: 0.8125\n",
      "Epoch 00105: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8522 - val_loss: 0.5279 - val_accuracy: 0.7549\n",
      "Epoch 106/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2118 - accuracy: 0.8750\n",
      "Epoch 00106: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8374 - val_loss: 0.5828 - val_accuracy: 0.7647\n",
      "Epoch 107/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3447 - accuracy: 0.7812\n",
      "Epoch 00107: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8300 - val_loss: 0.5309 - val_accuracy: 0.7549\n",
      "Epoch 108/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1575 - accuracy: 0.9062\n",
      "Epoch 00108: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8300 - val_loss: 0.5361 - val_accuracy: 0.7549\n",
      "Epoch 109/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4226 - accuracy: 0.7500\n",
      "Epoch 00109: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8448 - val_loss: 0.5283 - val_accuracy: 0.7451\n",
      "Epoch 110/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4137 - accuracy: 0.7500\n",
      "Epoch 00110: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8202 - val_loss: 0.5007 - val_accuracy: 0.7647\n",
      "Epoch 111/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2666 - accuracy: 0.8125\n",
      "Epoch 00111: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3119 - accuracy: 0.8300 - val_loss: 0.5642 - val_accuracy: 0.8137\n",
      "Epoch 112/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4035 - accuracy: 0.8438\n",
      "Epoch 00112: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2862 - accuracy: 0.8473 - val_loss: 0.6237 - val_accuracy: 0.8039\n",
      "Epoch 113/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3404 - accuracy: 0.8750\n",
      "Epoch 00113: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3000 - accuracy: 0.8350 - val_loss: 0.6022 - val_accuracy: 0.7647\n",
      "Epoch 114/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3536 - accuracy: 0.7812\n",
      "Epoch 00114: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8399 - val_loss: 0.6470 - val_accuracy: 0.7745\n",
      "Epoch 115/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2043 - accuracy: 0.8750\n",
      "Epoch 00115: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8350 - val_loss: 0.5489 - val_accuracy: 0.7745\n",
      "Epoch 116/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2785 - accuracy: 0.8125\n",
      "Epoch 00116: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8350 - val_loss: 0.5175 - val_accuracy: 0.7647\n",
      "Epoch 117/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2905 - accuracy: 0.9375\n",
      "Epoch 00117: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2878 - accuracy: 0.8522 - val_loss: 0.5195 - val_accuracy: 0.7353\n",
      "Epoch 118/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4119 - accuracy: 0.7500\n",
      "Epoch 00118: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2956 - accuracy: 0.8374 - val_loss: 0.4991 - val_accuracy: 0.7255\n",
      "Epoch 119/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2086 - accuracy: 0.9062\n",
      "Epoch 00119: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2717 - accuracy: 0.8424 - val_loss: 0.5587 - val_accuracy: 0.7353\n",
      "Epoch 120/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1829 - accuracy: 0.8438\n",
      "Epoch 00120: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8473 - val_loss: 0.5202 - val_accuracy: 0.7353\n",
      "Epoch 121/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2410 - accuracy: 0.8750\n",
      "Epoch 00121: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.8448 - val_loss: 0.5023 - val_accuracy: 0.7451\n",
      "Epoch 122/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1848 - accuracy: 0.9062\n",
      "Epoch 00122: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2824 - accuracy: 0.8448 - val_loss: 0.5690 - val_accuracy: 0.7549\n",
      "Epoch 123/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2731 - accuracy: 0.7812\n",
      "Epoch 00123: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8350 - val_loss: 0.5007 - val_accuracy: 0.7549\n",
      "Epoch 124/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2429 - accuracy: 0.9062\n",
      "Epoch 00124: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2727 - accuracy: 0.8498 - val_loss: 0.5360 - val_accuracy: 0.7745\n",
      "Epoch 125/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3616 - accuracy: 0.7812\n",
      "Epoch 00125: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8473 - val_loss: 0.5906 - val_accuracy: 0.7745\n",
      "Epoch 126/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2916 - accuracy: 0.8750\n",
      "Epoch 00126: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.8596 - val_loss: 0.6192 - val_accuracy: 0.7745\n",
      "Epoch 127/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2406 - accuracy: 0.8125\n",
      "Epoch 00127: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8596 - val_loss: 0.5532 - val_accuracy: 0.7745\n",
      "Epoch 128/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3643 - accuracy: 0.8125\n",
      "Epoch 00128: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8596 - val_loss: 0.6149 - val_accuracy: 0.7647\n",
      "Epoch 129/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4165 - accuracy: 0.8125\n",
      "Epoch 00129: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3085 - accuracy: 0.8596 - val_loss: 0.5897 - val_accuracy: 0.7549\n",
      "Epoch 130/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2569 - accuracy: 0.8750\n",
      "Epoch 00130: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2904 - accuracy: 0.8399 - val_loss: 0.5509 - val_accuracy: 0.7549\n",
      "Epoch 131/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3132 - accuracy: 0.8125\n",
      "Epoch 00131: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8374 - val_loss: 0.5891 - val_accuracy: 0.7549\n",
      "Epoch 132/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1971 - accuracy: 0.9062\n",
      "Epoch 00132: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2493 - accuracy: 0.8645 - val_loss: 0.6596 - val_accuracy: 0.7647\n",
      "Epoch 133/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2734 - accuracy: 0.7812\n",
      "Epoch 00133: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2985 - accuracy: 0.8202 - val_loss: 0.6696 - val_accuracy: 0.7647\n",
      "Epoch 134/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3674 - accuracy: 0.7812\n",
      "Epoch 00134: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2984 - accuracy: 0.8645 - val_loss: 0.5530 - val_accuracy: 0.7451\n",
      "Epoch 135/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2766 - accuracy: 0.9062\n",
      "Epoch 00135: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.8522 - val_loss: 0.5528 - val_accuracy: 0.7549\n",
      "Epoch 136/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3534 - accuracy: 0.7812\n",
      "Epoch 00136: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2770 - accuracy: 0.8596 - val_loss: 0.6119 - val_accuracy: 0.7451\n",
      "Epoch 137/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2406 - accuracy: 0.7812\n",
      "Epoch 00137: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2716 - accuracy: 0.8276 - val_loss: 0.6623 - val_accuracy: 0.7549\n",
      "Epoch 138/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2871 - accuracy: 0.8750\n",
      "Epoch 00138: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2732 - accuracy: 0.8793 - val_loss: 0.6735 - val_accuracy: 0.7843\n",
      "Epoch 139/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3216 - accuracy: 0.8438\n",
      "Epoch 00139: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8571 - val_loss: 0.7674 - val_accuracy: 0.7745\n",
      "Epoch 140/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2474 - accuracy: 0.8750\n",
      "Epoch 00140: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8399 - val_loss: 0.5606 - val_accuracy: 0.7451\n",
      "Epoch 141/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2448 - accuracy: 0.8438\n",
      "Epoch 00141: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.8695 - val_loss: 0.6263 - val_accuracy: 0.7843\n",
      "Epoch 142/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3685 - accuracy: 0.7812\n",
      "Epoch 00142: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8571 - val_loss: 0.5368 - val_accuracy: 0.7549\n",
      "Epoch 143/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2085 - accuracy: 0.9062\n",
      "Epoch 00143: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8547 - val_loss: 0.5331 - val_accuracy: 0.7843\n",
      "Epoch 144/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2586 - accuracy: 0.9375\n",
      "Epoch 00144: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.8768 - val_loss: 0.5290 - val_accuracy: 0.7941\n",
      "Epoch 145/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2975 - accuracy: 0.8750\n",
      "Epoch 00145: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8522 - val_loss: 0.5639 - val_accuracy: 0.7843\n",
      "Epoch 146/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2662 - accuracy: 0.8750\n",
      "Epoch 00146: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2712 - accuracy: 0.8498 - val_loss: 0.6099 - val_accuracy: 0.7647\n",
      "Epoch 147/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1932 - accuracy: 0.8750\n",
      "Epoch 00147: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.8670 - val_loss: 0.5292 - val_accuracy: 0.7843\n",
      "Epoch 148/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1625 - accuracy: 0.9062\n",
      "Epoch 00148: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.8719 - val_loss: 0.6114 - val_accuracy: 0.7647\n",
      "Epoch 149/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2314 - accuracy: 0.8438\n",
      "Epoch 00149: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.8645 - val_loss: 0.6643 - val_accuracy: 0.7941\n",
      "Epoch 150/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3393 - accuracy: 0.8125\n",
      "Epoch 00150: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.8719 - val_loss: 0.5972 - val_accuracy: 0.7843\n",
      "Epoch 151/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4641 - accuracy: 0.7812\n",
      "Epoch 00151: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8596 - val_loss: 0.6162 - val_accuracy: 0.7843\n",
      "Epoch 152/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2310 - accuracy: 0.8438\n",
      "Epoch 00152: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8448 - val_loss: 0.6192 - val_accuracy: 0.7941\n",
      "Epoch 153/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2848 - accuracy: 0.8438\n",
      "Epoch 00153: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2593 - accuracy: 0.8768 - val_loss: 0.6132 - val_accuracy: 0.8039\n",
      "Epoch 154/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1923 - accuracy: 0.8750\n",
      "Epoch 00154: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8621 - val_loss: 0.6260 - val_accuracy: 0.7745\n",
      "Epoch 155/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3546 - accuracy: 0.7812\n",
      "Epoch 00155: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8645 - val_loss: 0.6568 - val_accuracy: 0.7549\n",
      "Epoch 156/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4083 - accuracy: 0.7500\n",
      "Epoch 00156: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.8547 - val_loss: 0.7587 - val_accuracy: 0.7843\n",
      "Epoch 157/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2281 - accuracy: 0.8750\n",
      "Epoch 00157: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8670 - val_loss: 0.6714 - val_accuracy: 0.7647\n",
      "Epoch 158/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1203 - accuracy: 0.9375\n",
      "Epoch 00158: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8571 - val_loss: 0.5456 - val_accuracy: 0.7843\n",
      "Epoch 159/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9062\n",
      "Epoch 00159: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.8670 - val_loss: 0.5922 - val_accuracy: 0.7843\n",
      "Epoch 160/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2466 - accuracy: 0.8750\n",
      "Epoch 00160: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8596 - val_loss: 0.5413 - val_accuracy: 0.7451\n",
      "Epoch 161/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2949 - accuracy: 0.8438\n",
      "Epoch 00161: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8768 - val_loss: 0.6165 - val_accuracy: 0.7843\n",
      "Epoch 162/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2837 - accuracy: 0.8750\n",
      "Epoch 00162: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.8670 - val_loss: 0.6615 - val_accuracy: 0.7941\n",
      "Epoch 163/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2848 - accuracy: 0.8125\n",
      "Epoch 00163: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.8621 - val_loss: 0.6593 - val_accuracy: 0.8039\n",
      "Epoch 164/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1547 - accuracy: 0.8750\n",
      "Epoch 00164: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.8498 - val_loss: 0.6587 - val_accuracy: 0.7745\n",
      "Epoch 165/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2448 - accuracy: 0.9062\n",
      "Epoch 00165: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8621 - val_loss: 0.6405 - val_accuracy: 0.7745\n",
      "Epoch 166/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3371 - accuracy: 0.7812\n",
      "Epoch 00166: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.8621 - val_loss: 0.6540 - val_accuracy: 0.8137\n",
      "Epoch 167/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3117 - accuracy: 0.7812\n",
      "Epoch 00167: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8621 - val_loss: 0.6417 - val_accuracy: 0.8137\n",
      "Epoch 168/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3750 - accuracy: 0.8438\n",
      "Epoch 00168: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.8941 - val_loss: 0.7172 - val_accuracy: 0.7843\n",
      "Epoch 169/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1534 - accuracy: 0.9375\n",
      "Epoch 00169: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8793 - val_loss: 0.6472 - val_accuracy: 0.7549\n",
      "Epoch 170/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2358 - accuracy: 0.9062\n",
      "Epoch 00170: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.8818 - val_loss: 0.6710 - val_accuracy: 0.7549\n",
      "Epoch 171/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1878 - accuracy: 0.9375\n",
      "Epoch 00171: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.8695 - val_loss: 0.6310 - val_accuracy: 0.7549\n",
      "Epoch 172/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2411 - accuracy: 0.9062\n",
      "Epoch 00172: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2347 - accuracy: 0.8793 - val_loss: 0.6882 - val_accuracy: 0.7941\n",
      "Epoch 173/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2151 - accuracy: 0.8125\n",
      "Epoch 00173: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.8719 - val_loss: 0.6758 - val_accuracy: 0.8235\n",
      "Epoch 174/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2529 - accuracy: 0.8750\n",
      "Epoch 00174: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8645 - val_loss: 0.7330 - val_accuracy: 0.8235\n",
      "Epoch 175/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3637 - accuracy: 0.7188\n",
      "Epoch 00175: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8177 - val_loss: 0.7005 - val_accuracy: 0.7941\n",
      "Epoch 176/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3500 - accuracy: 0.9375\n",
      "Epoch 00176: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.8547 - val_loss: 0.6103 - val_accuracy: 0.7941\n",
      "Epoch 177/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2456 - accuracy: 0.8750\n",
      "Epoch 00177: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2426 - accuracy: 0.8768 - val_loss: 0.8281 - val_accuracy: 0.7941\n",
      "Epoch 178/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2311 - accuracy: 0.9062\n",
      "Epoch 00178: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2881 - accuracy: 0.8448 - val_loss: 0.5672 - val_accuracy: 0.7941\n",
      "Epoch 179/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1984 - accuracy: 0.9688\n",
      "Epoch 00179: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8793 - val_loss: 0.5558 - val_accuracy: 0.7941\n",
      "Epoch 180/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3482 - accuracy: 0.7812\n",
      "Epoch 00180: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.8645 - val_loss: 0.6100 - val_accuracy: 0.8039\n",
      "Epoch 181/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2630 - accuracy: 0.8438\n",
      "Epoch 00181: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.8621 - val_loss: 0.6822 - val_accuracy: 0.8137\n",
      "Epoch 182/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1471 - accuracy: 0.9375\n",
      "Epoch 00182: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8744 - val_loss: 0.7131 - val_accuracy: 0.7941\n",
      "Epoch 183/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2551 - accuracy: 0.8750\n",
      "Epoch 00183: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.8670 - val_loss: 0.6503 - val_accuracy: 0.7647\n",
      "Epoch 184/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3061 - accuracy: 0.8125\n",
      "Epoch 00184: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.8695 - val_loss: 0.6640 - val_accuracy: 0.7647\n",
      "Epoch 185/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2636 - accuracy: 0.8125\n",
      "Epoch 00185: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8571 - val_loss: 0.6977 - val_accuracy: 0.8039\n",
      "Epoch 186/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3396 - accuracy: 0.7812\n",
      "Epoch 00186: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8744 - val_loss: 0.7070 - val_accuracy: 0.7941\n",
      "Epoch 187/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1521 - accuracy: 0.9375\n",
      "Epoch 00187: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.8768 - val_loss: 0.5393 - val_accuracy: 0.7745\n",
      "Epoch 188/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2700 - accuracy: 0.8750\n",
      "Epoch 00188: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.9015 - val_loss: 0.5594 - val_accuracy: 0.7745\n",
      "Epoch 189/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2132 - accuracy: 0.9062\n",
      "Epoch 00189: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.8571 - val_loss: 0.6321 - val_accuracy: 0.7941\n",
      "Epoch 190/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1535 - accuracy: 0.9375\n",
      "Epoch 00190: val_loss did not improve from 0.45837\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2590 - accuracy: 0.8867 - val_loss: 0.4589 - val_accuracy: 0.7941\n",
      "Epoch 191/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2140 - accuracy: 0.8750\n",
      "Epoch 00191: val_loss improved from 0.45837 to 0.45315, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8670 - val_loss: 0.4532 - val_accuracy: 0.7843\n",
      "Epoch 192/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2499 - accuracy: 0.8438\n",
      "Epoch 00192: val_loss improved from 0.45315 to 0.45224, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.8793 - val_loss: 0.4522 - val_accuracy: 0.7843\n",
      "Epoch 193/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1893 - accuracy: 0.9062\n",
      "Epoch 00193: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8498 - val_loss: 0.5408 - val_accuracy: 0.7843\n",
      "Epoch 194/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1747 - accuracy: 0.9375\n",
      "Epoch 00194: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9015 - val_loss: 0.6058 - val_accuracy: 0.8039\n",
      "Epoch 195/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2003 - accuracy: 0.9375\n",
      "Epoch 00195: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8571 - val_loss: 0.4907 - val_accuracy: 0.7843\n",
      "Epoch 196/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2105 - accuracy: 0.9062\n",
      "Epoch 00196: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.8645 - val_loss: 0.5150 - val_accuracy: 0.7941\n",
      "Epoch 197/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2130 - accuracy: 0.9062\n",
      "Epoch 00197: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.8916 - val_loss: 0.5403 - val_accuracy: 0.7843\n",
      "Epoch 198/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2502 - accuracy: 0.8750\n",
      "Epoch 00198: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.8916 - val_loss: 0.7151 - val_accuracy: 0.7843\n",
      "Epoch 199/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1606 - accuracy: 0.9375\n",
      "Epoch 00199: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8621 - val_loss: 0.5181 - val_accuracy: 0.7549\n",
      "Epoch 200/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2928 - accuracy: 0.8125\n",
      "Epoch 00200: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.8621 - val_loss: 0.5538 - val_accuracy: 0.7745\n",
      "Epoch 201/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2554 - accuracy: 0.8750\n",
      "Epoch 00201: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8793 - val_loss: 0.5840 - val_accuracy: 0.8235\n",
      "Epoch 202/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2040 - accuracy: 0.8750\n",
      "Epoch 00202: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.8744 - val_loss: 0.5418 - val_accuracy: 0.7941\n",
      "Epoch 203/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3310 - accuracy: 0.8125\n",
      "Epoch 00203: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.8670 - val_loss: 0.5613 - val_accuracy: 0.7843\n",
      "Epoch 204/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1882 - accuracy: 0.8750\n",
      "Epoch 00204: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2276 - accuracy: 0.8793 - val_loss: 0.6730 - val_accuracy: 0.8137\n",
      "Epoch 205/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2380 - accuracy: 0.8750\n",
      "Epoch 00205: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8768 - val_loss: 0.5936 - val_accuracy: 0.7745\n",
      "Epoch 206/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2752 - accuracy: 0.8125\n",
      "Epoch 00206: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.8473 - val_loss: 0.5582 - val_accuracy: 0.7451\n",
      "Epoch 207/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3942 - accuracy: 0.7812\n",
      "Epoch 00207: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.8719 - val_loss: 0.5609 - val_accuracy: 0.7451\n",
      "Epoch 208/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2130 - accuracy: 0.8750\n",
      "Epoch 00208: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.8621 - val_loss: 0.5797 - val_accuracy: 0.7941\n",
      "Epoch 209/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1945 - accuracy: 0.9062\n",
      "Epoch 00209: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.8867 - val_loss: 0.5499 - val_accuracy: 0.7941\n",
      "Epoch 210/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2422 - accuracy: 0.8438\n",
      "Epoch 00210: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.8867 - val_loss: 0.6377 - val_accuracy: 0.8039\n",
      "Epoch 211/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1581 - accuracy: 0.9062\n",
      "Epoch 00211: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.8842 - val_loss: 0.7005 - val_accuracy: 0.7843\n",
      "Epoch 212/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1114 - accuracy: 0.9688\n",
      "Epoch 00212: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8596 - val_loss: 0.5880 - val_accuracy: 0.7745\n",
      "Epoch 213/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1116 - accuracy: 0.9375\n",
      "Epoch 00213: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2232 - accuracy: 0.8768 - val_loss: 0.6004 - val_accuracy: 0.7745\n",
      "Epoch 214/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2973 - accuracy: 0.8438\n",
      "Epoch 00214: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.8867 - val_loss: 0.6204 - val_accuracy: 0.7745\n",
      "Epoch 215/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1953 - accuracy: 0.9375\n",
      "Epoch 00215: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.8768 - val_loss: 0.7119 - val_accuracy: 0.7647\n",
      "Epoch 216/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1697 - accuracy: 0.9062\n",
      "Epoch 00216: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.8892 - val_loss: 0.7199 - val_accuracy: 0.8137\n",
      "Epoch 217/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1744 - accuracy: 0.9375\n",
      "Epoch 00217: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.8941 - val_loss: 0.6689 - val_accuracy: 0.7843\n",
      "Epoch 218/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1686 - accuracy: 0.8750\n",
      "Epoch 00218: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.8744 - val_loss: 0.6811 - val_accuracy: 0.8137\n",
      "Epoch 219/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2169 - accuracy: 0.8750\n",
      "Epoch 00219: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8744 - val_loss: 0.7302 - val_accuracy: 0.7941\n",
      "Epoch 220/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1702 - accuracy: 0.9375\n",
      "Epoch 00220: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.8867 - val_loss: 0.6285 - val_accuracy: 0.7647\n",
      "Epoch 221/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1477 - accuracy: 1.0000\n",
      "Epoch 00221: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8818 - val_loss: 0.6984 - val_accuracy: 0.7843\n",
      "Epoch 222/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3153 - accuracy: 0.8125\n",
      "Epoch 00222: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8768 - val_loss: 0.6324 - val_accuracy: 0.7745\n",
      "Epoch 223/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3555 - accuracy: 0.8125\n",
      "Epoch 00223: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.8867 - val_loss: 0.6017 - val_accuracy: 0.7745\n",
      "Epoch 224/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2365 - accuracy: 0.8750\n",
      "Epoch 00224: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9015 - val_loss: 0.6624 - val_accuracy: 0.7941\n",
      "Epoch 225/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2970 - accuracy: 0.8438\n",
      "Epoch 00225: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.8768 - val_loss: 0.6460 - val_accuracy: 0.7745\n",
      "Epoch 226/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1570 - accuracy: 0.9062\n",
      "Epoch 00226: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.8768 - val_loss: 0.6275 - val_accuracy: 0.7843\n",
      "Epoch 227/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2324 - accuracy: 0.8750\n",
      "Epoch 00227: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.8744 - val_loss: 0.7490 - val_accuracy: 0.7843\n",
      "Epoch 228/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1639 - accuracy: 0.9375\n",
      "Epoch 00228: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.8892 - val_loss: 0.6717 - val_accuracy: 0.7843\n",
      "Epoch 229/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0775 - accuracy: 0.9688\n",
      "Epoch 00229: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2186 - accuracy: 0.8990 - val_loss: 0.6171 - val_accuracy: 0.8039\n",
      "Epoch 230/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1532 - accuracy: 0.9062\n",
      "Epoch 00230: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.8842 - val_loss: 0.6469 - val_accuracy: 0.7843\n",
      "Epoch 231/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1592 - accuracy: 0.9375\n",
      "Epoch 00231: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8522 - val_loss: 0.5702 - val_accuracy: 0.8039\n",
      "Epoch 232/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2950 - accuracy: 0.9375\n",
      "Epoch 00232: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9113 - val_loss: 0.6334 - val_accuracy: 0.8235\n",
      "Epoch 233/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4080 - accuracy: 0.8750\n",
      "Epoch 00233: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8818 - val_loss: 0.5089 - val_accuracy: 0.7843\n",
      "Epoch 234/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2006 - accuracy: 0.8438\n",
      "Epoch 00234: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.8695 - val_loss: 0.5712 - val_accuracy: 0.7843\n",
      "Epoch 235/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1751 - accuracy: 0.9375\n",
      "Epoch 00235: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8916 - val_loss: 0.4992 - val_accuracy: 0.7745\n",
      "Epoch 236/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1843 - accuracy: 0.9375\n",
      "Epoch 00236: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.8966 - val_loss: 0.5372 - val_accuracy: 0.7745\n",
      "Epoch 237/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1806 - accuracy: 0.8438\n",
      "Epoch 00237: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.8793 - val_loss: 0.6002 - val_accuracy: 0.7941\n",
      "Epoch 238/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2636 - accuracy: 0.9062\n",
      "Epoch 00238: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8867 - val_loss: 0.5655 - val_accuracy: 0.7941\n",
      "Epoch 239/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2336 - accuracy: 0.8438\n",
      "Epoch 00239: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.8941 - val_loss: 0.6271 - val_accuracy: 0.8039\n",
      "Epoch 240/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4657 - accuracy: 0.7812\n",
      "Epoch 00240: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.8867 - val_loss: 0.7393 - val_accuracy: 0.8039\n",
      "Epoch 241/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1926 - accuracy: 0.8750\n",
      "Epoch 00241: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8768 - val_loss: 0.7461 - val_accuracy: 0.8137\n",
      "Epoch 242/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2567 - accuracy: 0.8438\n",
      "Epoch 00242: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.8916 - val_loss: 0.6373 - val_accuracy: 0.7941\n",
      "Epoch 243/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2299 - accuracy: 0.8438\n",
      "Epoch 00243: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.8596 - val_loss: 0.6968 - val_accuracy: 0.7941\n",
      "Epoch 244/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1241 - accuracy: 0.9375\n",
      "Epoch 00244: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.8867 - val_loss: 0.7232 - val_accuracy: 0.7647\n",
      "Epoch 245/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2070 - accuracy: 0.8750\n",
      "Epoch 00245: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.8768 - val_loss: 0.7314 - val_accuracy: 0.7843\n",
      "Epoch 246/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2479 - accuracy: 0.8438\n",
      "Epoch 00246: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9089 - val_loss: 0.7319 - val_accuracy: 0.7941\n",
      "Epoch 247/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1925 - accuracy: 0.9062\n",
      "Epoch 00247: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9064 - val_loss: 0.7382 - val_accuracy: 0.8039\n",
      "Epoch 248/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1286 - accuracy: 0.9375\n",
      "Epoch 00248: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.8990 - val_loss: 0.7137 - val_accuracy: 0.8039\n",
      "Epoch 249/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2161 - accuracy: 0.9375\n",
      "Epoch 00249: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1902 - accuracy: 0.8990 - val_loss: 0.8224 - val_accuracy: 0.8137\n",
      "Epoch 250/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2283 - accuracy: 0.9062\n",
      "Epoch 00250: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9113 - val_loss: 0.7775 - val_accuracy: 0.7647\n",
      "Epoch 251/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1812 - accuracy: 0.8438\n",
      "Epoch 00251: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.8768 - val_loss: 0.7471 - val_accuracy: 0.7843\n",
      "Epoch 252/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3213 - accuracy: 0.7812\n",
      "Epoch 00252: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.8818 - val_loss: 0.7555 - val_accuracy: 0.7941\n",
      "Epoch 253/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0956 - accuracy: 0.9062\n",
      "Epoch 00253: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.8818 - val_loss: 0.7884 - val_accuracy: 0.8039\n",
      "Epoch 254/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0594 - accuracy: 0.9688\n",
      "Epoch 00254: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2319 - accuracy: 0.8941 - val_loss: 0.8593 - val_accuracy: 0.8235\n",
      "Epoch 255/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2133 - accuracy: 0.8125\n",
      "Epoch 00255: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.8793 - val_loss: 0.7362 - val_accuracy: 0.8137\n",
      "Epoch 256/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2938 - accuracy: 0.8125\n",
      "Epoch 00256: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9015 - val_loss: 0.5607 - val_accuracy: 0.7941\n",
      "Epoch 257/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2364 - accuracy: 0.8438\n",
      "Epoch 00257: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.8768 - val_loss: 0.6303 - val_accuracy: 0.8039\n",
      "Epoch 258/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2590 - accuracy: 0.9375\n",
      "Epoch 00258: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.8941 - val_loss: 0.6527 - val_accuracy: 0.7843\n",
      "Epoch 259/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1491 - accuracy: 0.9062\n",
      "Epoch 00259: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.8990 - val_loss: 0.6811 - val_accuracy: 0.7745\n",
      "Epoch 260/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1618 - accuracy: 0.9062\n",
      "Epoch 00260: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9113 - val_loss: 0.6637 - val_accuracy: 0.7745\n",
      "Epoch 261/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3303 - accuracy: 0.8125\n",
      "Epoch 00261: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9039 - val_loss: 0.5769 - val_accuracy: 0.7941\n",
      "Epoch 262/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2140 - accuracy: 0.9062\n",
      "Epoch 00262: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.8990 - val_loss: 0.5383 - val_accuracy: 0.7941\n",
      "Epoch 263/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1618 - accuracy: 0.9688\n",
      "Epoch 00263: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9113 - val_loss: 0.5525 - val_accuracy: 0.7941\n",
      "Epoch 264/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2638 - accuracy: 0.8438\n",
      "Epoch 00264: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9212 - val_loss: 0.6025 - val_accuracy: 0.7843\n",
      "Epoch 265/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4061 - accuracy: 0.8438\n",
      "Epoch 00265: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.8892 - val_loss: 0.6725 - val_accuracy: 0.7941\n",
      "Epoch 266/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1223 - accuracy: 0.9375\n",
      "Epoch 00266: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8867 - val_loss: 0.5542 - val_accuracy: 0.7647\n",
      "Epoch 267/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0986 - accuracy: 0.9688\n",
      "Epoch 00267: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2298 - accuracy: 0.8941 - val_loss: 0.6273 - val_accuracy: 0.7941\n",
      "Epoch 268/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2042 - accuracy: 0.8750\n",
      "Epoch 00268: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8966 - val_loss: 0.6719 - val_accuracy: 0.7745\n",
      "Epoch 269/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1239 - accuracy: 0.9688\n",
      "Epoch 00269: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.8941 - val_loss: 0.6186 - val_accuracy: 0.7843\n",
      "Epoch 270/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9375\n",
      "Epoch 00270: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9015 - val_loss: 0.7354 - val_accuracy: 0.7941\n",
      "Epoch 271/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3970 - accuracy: 0.7812\n",
      "Epoch 00271: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.8719 - val_loss: 0.6148 - val_accuracy: 0.7745\n",
      "Epoch 272/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1855 - accuracy: 0.9375\n",
      "Epoch 00272: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1959 - accuracy: 0.9064 - val_loss: 0.6138 - val_accuracy: 0.8137\n",
      "Epoch 273/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1566 - accuracy: 0.9062\n",
      "Epoch 00273: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.8867 - val_loss: 0.6213 - val_accuracy: 0.8137\n",
      "Epoch 274/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2001 - accuracy: 0.9375\n",
      "Epoch 00274: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9039 - val_loss: 0.6685 - val_accuracy: 0.7745\n",
      "Epoch 275/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2805 - accuracy: 0.8125\n",
      "Epoch 00275: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.8966 - val_loss: 0.7588 - val_accuracy: 0.8039\n",
      "Epoch 276/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2171 - accuracy: 0.8750\n",
      "Epoch 00276: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9089 - val_loss: 0.6669 - val_accuracy: 0.7941\n",
      "Epoch 277/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1436 - accuracy: 0.9375\n",
      "Epoch 00277: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9113 - val_loss: 0.6378 - val_accuracy: 0.7941\n",
      "Epoch 278/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9375\n",
      "Epoch 00278: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.8966 - val_loss: 0.6857 - val_accuracy: 0.8137\n",
      "Epoch 279/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2010 - accuracy: 0.8750\n",
      "Epoch 00279: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9187 - val_loss: 0.7660 - val_accuracy: 0.7843\n",
      "Epoch 280/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1632 - accuracy: 0.9062\n",
      "Epoch 00280: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2028 - accuracy: 0.8990 - val_loss: 0.7755 - val_accuracy: 0.8137\n",
      "Epoch 281/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2189 - accuracy: 0.9062\n",
      "Epoch 00281: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8966 - val_loss: 0.7100 - val_accuracy: 0.8039\n",
      "Epoch 282/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1780 - accuracy: 0.9062\n",
      "Epoch 00282: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9163 - val_loss: 0.6715 - val_accuracy: 0.7843\n",
      "Epoch 283/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1630 - accuracy: 0.9688\n",
      "Epoch 00283: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 0.8990 - val_loss: 0.7307 - val_accuracy: 0.8039\n",
      "Epoch 284/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1682 - accuracy: 0.8438\n",
      "Epoch 00284: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9163 - val_loss: 0.6947 - val_accuracy: 0.8039\n",
      "Epoch 285/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2063 - accuracy: 0.8750\n",
      "Epoch 00285: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.8966 - val_loss: 0.6952 - val_accuracy: 0.8039\n",
      "Epoch 286/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2307 - accuracy: 0.8750\n",
      "Epoch 00286: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9163 - val_loss: 0.7664 - val_accuracy: 0.8137\n",
      "Epoch 287/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1691 - accuracy: 0.9062\n",
      "Epoch 00287: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1734 - accuracy: 0.8916 - val_loss: 0.8823 - val_accuracy: 0.7941\n",
      "Epoch 288/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1285 - accuracy: 0.9062\n",
      "Epoch 00288: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.8966 - val_loss: 0.6783 - val_accuracy: 0.8137\n",
      "Epoch 289/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1047 - accuracy: 0.9375\n",
      "Epoch 00289: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8818 - val_loss: 0.6285 - val_accuracy: 0.7843\n",
      "Epoch 290/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2405 - accuracy: 0.9062\n",
      "Epoch 00290: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2223 - accuracy: 0.8842 - val_loss: 0.6968 - val_accuracy: 0.7941\n",
      "Epoch 291/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0948 - accuracy: 0.9375\n",
      "Epoch 00291: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9064 - val_loss: 0.7842 - val_accuracy: 0.7843\n",
      "Epoch 292/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1342 - accuracy: 0.9375\n",
      "Epoch 00292: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.9089 - val_loss: 0.8175 - val_accuracy: 0.8039\n",
      "Epoch 293/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2847 - accuracy: 0.7812\n",
      "Epoch 00293: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.8941 - val_loss: 0.8159 - val_accuracy: 0.7941\n",
      "Epoch 294/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1771 - accuracy: 0.9062\n",
      "Epoch 00294: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.8916 - val_loss: 0.7588 - val_accuracy: 0.8039\n",
      "Epoch 295/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1373 - accuracy: 0.9688\n",
      "Epoch 00295: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1751 - accuracy: 0.9089 - val_loss: 0.7872 - val_accuracy: 0.8039\n",
      "Epoch 296/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1873 - accuracy: 0.9062\n",
      "Epoch 00296: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9138 - val_loss: 0.8494 - val_accuracy: 0.8235\n",
      "Epoch 297/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1367 - accuracy: 0.9375\n",
      "Epoch 00297: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9039 - val_loss: 0.8295 - val_accuracy: 0.8235\n",
      "Epoch 298/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1624 - accuracy: 0.8750\n",
      "Epoch 00298: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.9089 - val_loss: 0.8274 - val_accuracy: 0.8333\n",
      "Epoch 299/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1908 - accuracy: 0.9062\n",
      "Epoch 00299: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9261 - val_loss: 0.8822 - val_accuracy: 0.8333\n",
      "Epoch 300/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4315 - accuracy: 0.7500\n",
      "Epoch 00300: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.9064 - val_loss: 0.8325 - val_accuracy: 0.7745\n",
      "Epoch 301/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9688\n",
      "Epoch 00301: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.9113 - val_loss: 0.7661 - val_accuracy: 0.8137\n",
      "Epoch 302/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1308 - accuracy: 0.9062\n",
      "Epoch 00302: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.8990 - val_loss: 0.6453 - val_accuracy: 0.8137\n",
      "Epoch 303/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4079 - accuracy: 0.7500\n",
      "Epoch 00303: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.8818 - val_loss: 0.6352 - val_accuracy: 0.7941\n",
      "Epoch 304/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1774 - accuracy: 0.9062\n",
      "Epoch 00304: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.8916 - val_loss: 0.7448 - val_accuracy: 0.8137\n",
      "Epoch 305/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2178 - accuracy: 0.8750\n",
      "Epoch 00305: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1995 - accuracy: 0.8966 - val_loss: 0.6530 - val_accuracy: 0.8333\n",
      "Epoch 306/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2419 - accuracy: 0.8438\n",
      "Epoch 00306: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.8818 - val_loss: 0.6442 - val_accuracy: 0.8431\n",
      "Epoch 307/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1844 - accuracy: 0.9688\n",
      "Epoch 00307: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.8990 - val_loss: 0.6961 - val_accuracy: 0.8039\n",
      "Epoch 308/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1793 - accuracy: 0.9062\n",
      "Epoch 00308: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9015 - val_loss: 0.6411 - val_accuracy: 0.7941\n",
      "Epoch 309/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1627 - accuracy: 0.9062\n",
      "Epoch 00309: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9163 - val_loss: 0.7004 - val_accuracy: 0.7941\n",
      "Epoch 310/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2144 - accuracy: 0.9062\n",
      "Epoch 00310: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.8867 - val_loss: 0.7357 - val_accuracy: 0.8333\n",
      "Epoch 311/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2005 - accuracy: 0.9375\n",
      "Epoch 00311: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9064 - val_loss: 0.7365 - val_accuracy: 0.8431\n",
      "Epoch 312/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1335 - accuracy: 0.9375\n",
      "Epoch 00312: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.9187 - val_loss: 0.7785 - val_accuracy: 0.8431\n",
      "Epoch 313/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1453 - accuracy: 0.8750\n",
      "Epoch 00313: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9113 - val_loss: 0.8383 - val_accuracy: 0.8431\n",
      "Epoch 314/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1445 - accuracy: 0.9375\n",
      "Epoch 00314: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.8867 - val_loss: 0.8216 - val_accuracy: 0.8235\n",
      "Epoch 315/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2758 - accuracy: 0.8750\n",
      "Epoch 00315: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1927 - accuracy: 0.9015 - val_loss: 0.7472 - val_accuracy: 0.8529\n",
      "Epoch 316/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1236 - accuracy: 0.9062\n",
      "Epoch 00316: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 0.9039 - val_loss: 0.7455 - val_accuracy: 0.8333\n",
      "Epoch 317/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1761 - accuracy: 0.8750\n",
      "Epoch 00317: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.8818 - val_loss: 0.7986 - val_accuracy: 0.8529\n",
      "Epoch 318/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0562 - accuracy: 0.9375\n",
      "Epoch 00318: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2177 - accuracy: 0.8990 - val_loss: 0.6469 - val_accuracy: 0.8235\n",
      "Epoch 319/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2203 - accuracy: 0.8750\n",
      "Epoch 00319: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.8793 - val_loss: 0.5555 - val_accuracy: 0.8431\n",
      "Epoch 320/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1775 - accuracy: 0.8750\n",
      "Epoch 00320: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.8941 - val_loss: 0.6262 - val_accuracy: 0.8333\n",
      "Epoch 321/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1705 - accuracy: 0.8750\n",
      "Epoch 00321: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.8892 - val_loss: 0.5920 - val_accuracy: 0.8333\n",
      "Epoch 322/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1789 - accuracy: 0.9062\n",
      "Epoch 00322: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.8793 - val_loss: 0.6200 - val_accuracy: 0.8137\n",
      "Epoch 323/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1309 - accuracy: 0.9375\n",
      "Epoch 00323: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9163 - val_loss: 0.7271 - val_accuracy: 0.7941\n",
      "Epoch 324/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1858 - accuracy: 0.8750\n",
      "Epoch 00324: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9015 - val_loss: 0.6304 - val_accuracy: 0.8137\n",
      "Epoch 325/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1811 - accuracy: 0.9062\n",
      "Epoch 00325: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.8892 - val_loss: 0.6708 - val_accuracy: 0.8039\n",
      "Epoch 326/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9688\n",
      "Epoch 00326: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.8867 - val_loss: 0.7436 - val_accuracy: 0.8333\n",
      "Epoch 327/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2160 - accuracy: 0.8438\n",
      "Epoch 00327: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2021 - accuracy: 0.8719 - val_loss: 0.6877 - val_accuracy: 0.8039\n",
      "Epoch 328/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1297 - accuracy: 0.9375\n",
      "Epoch 00328: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9039 - val_loss: 0.6666 - val_accuracy: 0.8137\n",
      "Epoch 329/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2650 - accuracy: 0.8438\n",
      "Epoch 00329: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.8719 - val_loss: 0.6512 - val_accuracy: 0.8235\n",
      "Epoch 330/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1578 - accuracy: 0.8750\n",
      "Epoch 00330: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9113 - val_loss: 0.7374 - val_accuracy: 0.8137\n",
      "Epoch 331/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1820 - accuracy: 0.8750\n",
      "Epoch 00331: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.8941 - val_loss: 0.8459 - val_accuracy: 0.7941\n",
      "Epoch 332/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1669 - accuracy: 0.9062\n",
      "Epoch 00332: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1914 - accuracy: 0.8892 - val_loss: 0.8092 - val_accuracy: 0.8235\n",
      "Epoch 333/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3252 - accuracy: 0.8750\n",
      "Epoch 00333: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.8719 - val_loss: 0.6695 - val_accuracy: 0.8235\n",
      "Epoch 334/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1602 - accuracy: 0.9062\n",
      "Epoch 00334: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.8966 - val_loss: 0.7321 - val_accuracy: 0.8039\n",
      "Epoch 335/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2086 - accuracy: 0.8125\n",
      "Epoch 00335: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9039 - val_loss: 0.7506 - val_accuracy: 0.8235\n",
      "Epoch 336/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2764 - accuracy: 0.9375\n",
      "Epoch 00336: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1827 - accuracy: 0.8990 - val_loss: 0.7581 - val_accuracy: 0.8137\n",
      "Epoch 337/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1227 - accuracy: 0.9375\n",
      "Epoch 00337: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.8990 - val_loss: 0.7547 - val_accuracy: 0.8137\n",
      "Epoch 338/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0923 - accuracy: 0.9375\n",
      "Epoch 00338: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9089 - val_loss: 0.6014 - val_accuracy: 0.8039\n",
      "Epoch 339/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2876 - accuracy: 0.8750\n",
      "Epoch 00339: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9015 - val_loss: 0.6902 - val_accuracy: 0.8235\n",
      "Epoch 340/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2102 - accuracy: 0.8750\n",
      "Epoch 00340: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9039 - val_loss: 0.6760 - val_accuracy: 0.8235\n",
      "Epoch 341/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0807 - accuracy: 0.9375\n",
      "Epoch 00341: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9286 - val_loss: 0.8338 - val_accuracy: 0.8137\n",
      "Epoch 342/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1009 - accuracy: 0.9688\n",
      "Epoch 00342: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9064 - val_loss: 0.8257 - val_accuracy: 0.8431\n",
      "Epoch 343/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1880 - accuracy: 0.9062\n",
      "Epoch 00343: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1604 - accuracy: 0.9212 - val_loss: 0.9059 - val_accuracy: 0.8235\n",
      "Epoch 344/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0780 - accuracy: 0.9375\n",
      "Epoch 00344: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.8916 - val_loss: 0.7429 - val_accuracy: 0.8137\n",
      "Epoch 345/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1150 - accuracy: 0.9062\n",
      "Epoch 00345: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.8966 - val_loss: 0.7179 - val_accuracy: 0.8235\n",
      "Epoch 346/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1740 - accuracy: 0.9375\n",
      "Epoch 00346: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2118 - accuracy: 0.8941 - val_loss: 0.6669 - val_accuracy: 0.8333\n",
      "Epoch 347/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1911 - accuracy: 0.9062\n",
      "Epoch 00347: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.9187 - val_loss: 0.7324 - val_accuracy: 0.7941\n",
      "Epoch 348/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3412 - accuracy: 0.7500\n",
      "Epoch 00348: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.8793 - val_loss: 0.7341 - val_accuracy: 0.7843\n",
      "Epoch 349/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1051 - accuracy: 0.9375\n",
      "Epoch 00349: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9015 - val_loss: 0.7521 - val_accuracy: 0.7843\n",
      "Epoch 350/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1961 - accuracy: 0.9062\n",
      "Epoch 00350: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.8941 - val_loss: 0.7754 - val_accuracy: 0.7941\n",
      "Epoch 351/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2145 - accuracy: 0.8438\n",
      "Epoch 00351: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9039 - val_loss: 0.8298 - val_accuracy: 0.8039\n",
      "Epoch 352/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1550 - accuracy: 0.9062\n",
      "Epoch 00352: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.8990 - val_loss: 0.6490 - val_accuracy: 0.8235\n",
      "Epoch 353/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0987 - accuracy: 0.9375\n",
      "Epoch 00353: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.8990 - val_loss: 0.6366 - val_accuracy: 0.8333\n",
      "Epoch 354/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1894 - accuracy: 0.8750\n",
      "Epoch 00354: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8793 - val_loss: 0.5499 - val_accuracy: 0.8039\n",
      "Epoch 355/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1878 - accuracy: 0.9375\n",
      "Epoch 00355: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.8966 - val_loss: 0.6375 - val_accuracy: 0.8235\n",
      "Epoch 356/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1820 - accuracy: 0.9375\n",
      "Epoch 00356: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.8990 - val_loss: 0.7701 - val_accuracy: 0.7941\n",
      "Epoch 357/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2703 - accuracy: 0.8750\n",
      "Epoch 00357: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9187 - val_loss: 0.7727 - val_accuracy: 0.8137\n",
      "Epoch 358/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1289 - accuracy: 0.9062\n",
      "Epoch 00358: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9064 - val_loss: 0.6803 - val_accuracy: 0.8235\n",
      "Epoch 359/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2704 - accuracy: 0.8750\n",
      "Epoch 00359: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9089 - val_loss: 0.6665 - val_accuracy: 0.8235\n",
      "Epoch 360/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2425 - accuracy: 0.8750\n",
      "Epoch 00360: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1888 - accuracy: 0.9138 - val_loss: 0.7774 - val_accuracy: 0.8039\n",
      "Epoch 361/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1723 - accuracy: 0.8750\n",
      "Epoch 00361: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9039 - val_loss: 0.6879 - val_accuracy: 0.7451\n",
      "Epoch 362/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1277 - accuracy: 1.0000\n",
      "Epoch 00362: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1934 - accuracy: 0.9138 - val_loss: 0.7154 - val_accuracy: 0.7941\n",
      "Epoch 363/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1960 - accuracy: 0.9375\n",
      "Epoch 00363: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9187 - val_loss: 0.6480 - val_accuracy: 0.8431\n",
      "Epoch 364/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1432 - accuracy: 0.9375\n",
      "Epoch 00364: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9310 - val_loss: 0.6187 - val_accuracy: 0.8039\n",
      "Epoch 365/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1418 - accuracy: 0.9062\n",
      "Epoch 00365: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9039 - val_loss: 0.7055 - val_accuracy: 0.7941\n",
      "Epoch 366/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2420 - accuracy: 0.8750\n",
      "Epoch 00366: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9089 - val_loss: 0.8245 - val_accuracy: 0.7745\n",
      "Epoch 367/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1706 - accuracy: 0.9375\n",
      "Epoch 00367: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9015 - val_loss: 0.7638 - val_accuracy: 0.7745\n",
      "Epoch 368/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2265 - accuracy: 0.8125\n",
      "Epoch 00368: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9039 - val_loss: 0.6425 - val_accuracy: 0.8039\n",
      "Epoch 369/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3259 - accuracy: 0.7500\n",
      "Epoch 00369: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.8842 - val_loss: 0.7635 - val_accuracy: 0.8137\n",
      "Epoch 370/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2158 - accuracy: 0.8750\n",
      "Epoch 00370: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9138 - val_loss: 0.7692 - val_accuracy: 0.8039\n",
      "Epoch 371/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2534 - accuracy: 0.8438\n",
      "Epoch 00371: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.8990 - val_loss: 0.6693 - val_accuracy: 0.7941\n",
      "Epoch 372/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1778 - accuracy: 0.9062\n",
      "Epoch 00372: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2144 - accuracy: 0.9089 - val_loss: 0.7624 - val_accuracy: 0.8235\n",
      "Epoch 373/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1410 - accuracy: 0.9062\n",
      "Epoch 00373: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9039 - val_loss: 0.6987 - val_accuracy: 0.7843\n",
      "Epoch 374/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1773 - accuracy: 0.8438\n",
      "Epoch 00374: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.8818 - val_loss: 0.6104 - val_accuracy: 0.8333\n",
      "Epoch 375/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1477 - accuracy: 0.9375\n",
      "Epoch 00375: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.9286 - val_loss: 0.6135 - val_accuracy: 0.8235\n",
      "Epoch 376/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1753 - accuracy: 0.9062\n",
      "Epoch 00376: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9163 - val_loss: 0.7271 - val_accuracy: 0.7843\n",
      "Epoch 377/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1999 - accuracy: 0.9688\n",
      "Epoch 00377: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.8867 - val_loss: 0.6868 - val_accuracy: 0.7647\n",
      "Epoch 378/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1950 - accuracy: 0.9062\n",
      "Epoch 00378: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9064 - val_loss: 0.7273 - val_accuracy: 0.8039\n",
      "Epoch 379/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 00379: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1639 - accuracy: 0.9335 - val_loss: 0.7765 - val_accuracy: 0.8137\n",
      "Epoch 380/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2299 - accuracy: 0.8438\n",
      "Epoch 00380: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.8916 - val_loss: 0.7259 - val_accuracy: 0.8235\n",
      "Epoch 381/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9688\n",
      "Epoch 00381: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.8990 - val_loss: 0.7232 - val_accuracy: 0.8235\n",
      "Epoch 382/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1325 - accuracy: 0.9375\n",
      "Epoch 00382: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9039 - val_loss: 0.7007 - val_accuracy: 0.8039\n",
      "Epoch 383/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1127 - accuracy: 0.9688\n",
      "Epoch 00383: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.9064 - val_loss: 0.6901 - val_accuracy: 0.7941\n",
      "Epoch 384/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1041 - accuracy: 0.9375\n",
      "Epoch 00384: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.8941 - val_loss: 0.7325 - val_accuracy: 0.7941\n",
      "Epoch 385/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1407 - accuracy: 0.9062\n",
      "Epoch 00385: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1934 - accuracy: 0.8892 - val_loss: 0.7176 - val_accuracy: 0.8137\n",
      "Epoch 386/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1807 - accuracy: 0.9062\n",
      "Epoch 00386: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9039 - val_loss: 0.8044 - val_accuracy: 0.8137\n",
      "Epoch 387/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2218 - accuracy: 0.8750\n",
      "Epoch 00387: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.8990 - val_loss: 0.7274 - val_accuracy: 0.8333\n",
      "Epoch 388/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1669 - accuracy: 0.9062\n",
      "Epoch 00388: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.9039 - val_loss: 0.8146 - val_accuracy: 0.8333\n",
      "Epoch 389/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2504 - accuracy: 0.9062\n",
      "Epoch 00389: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9187 - val_loss: 0.8115 - val_accuracy: 0.8039\n",
      "Epoch 390/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0795 - accuracy: 1.0000\n",
      "Epoch 00390: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9310 - val_loss: 0.8712 - val_accuracy: 0.8235\n",
      "Epoch 391/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1454 - accuracy: 0.8750\n",
      "Epoch 00391: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9039 - val_loss: 0.8776 - val_accuracy: 0.8137\n",
      "Epoch 392/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1046 - accuracy: 0.9375\n",
      "Epoch 00392: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.8990 - val_loss: 0.8847 - val_accuracy: 0.7941\n",
      "Epoch 393/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1191 - accuracy: 1.0000\n",
      "Epoch 00393: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9163 - val_loss: 0.8928 - val_accuracy: 0.7647\n",
      "Epoch 394/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9062\n",
      "Epoch 00394: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1709 - accuracy: 0.9064 - val_loss: 0.9012 - val_accuracy: 0.8333\n",
      "Epoch 395/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1048 - accuracy: 0.9375\n",
      "Epoch 00395: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9409 - val_loss: 0.9446 - val_accuracy: 0.8137\n",
      "Epoch 396/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1451 - accuracy: 0.9688\n",
      "Epoch 00396: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9286 - val_loss: 1.0200 - val_accuracy: 0.7941\n",
      "Epoch 397/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1334 - accuracy: 0.8750\n",
      "Epoch 00397: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9089 - val_loss: 0.8615 - val_accuracy: 0.8039\n",
      "Epoch 398/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2291 - accuracy: 0.7500\n",
      "Epoch 00398: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.8793 - val_loss: 0.8866 - val_accuracy: 0.8039\n",
      "Epoch 399/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1281 - accuracy: 0.9375\n",
      "Epoch 00399: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9113 - val_loss: 0.9667 - val_accuracy: 0.8137\n",
      "Epoch 400/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1569 - accuracy: 0.8750\n",
      "Epoch 00400: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1733 - accuracy: 0.8990 - val_loss: 0.8929 - val_accuracy: 0.8039\n",
      "Epoch 401/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1951 - accuracy: 0.9375\n",
      "Epoch 00401: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9384 - val_loss: 0.9355 - val_accuracy: 0.8235\n",
      "Epoch 402/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1301 - accuracy: 0.9062\n",
      "Epoch 00402: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1312 - accuracy: 0.9261 - val_loss: 0.9435 - val_accuracy: 0.8039\n",
      "Epoch 403/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2613 - accuracy: 0.8438\n",
      "Epoch 00403: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1800 - accuracy: 0.9015 - val_loss: 0.8647 - val_accuracy: 0.8137\n",
      "Epoch 404/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1439 - accuracy: 0.9062\n",
      "Epoch 00404: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9236 - val_loss: 0.8753 - val_accuracy: 0.8137\n",
      "Epoch 405/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2220 - accuracy: 0.9062\n",
      "Epoch 00405: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9138 - val_loss: 0.8663 - val_accuracy: 0.8431\n",
      "Epoch 406/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1685 - accuracy: 0.9375\n",
      "Epoch 00406: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.8990 - val_loss: 0.9075 - val_accuracy: 0.8627\n",
      "Epoch 407/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1284 - accuracy: 0.9375\n",
      "Epoch 00407: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.9163 - val_loss: 0.8434 - val_accuracy: 0.8137\n",
      "Epoch 408/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1013 - accuracy: 0.9688\n",
      "Epoch 00408: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.9310 - val_loss: 0.9635 - val_accuracy: 0.8137\n",
      "Epoch 409/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1145 - accuracy: 0.9688\n",
      "Epoch 00409: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9236 - val_loss: 1.0451 - val_accuracy: 0.8039\n",
      "Epoch 410/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1654 - accuracy: 0.9062\n",
      "Epoch 00410: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9015 - val_loss: 0.8535 - val_accuracy: 0.8235\n",
      "Epoch 411/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1261 - accuracy: 0.9688\n",
      "Epoch 00411: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9310 - val_loss: 1.0034 - val_accuracy: 0.8333\n",
      "Epoch 412/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1892 - accuracy: 0.9062\n",
      "Epoch 00412: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9089 - val_loss: 0.9662 - val_accuracy: 0.8039\n",
      "Epoch 413/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1040 - accuracy: 0.9062\n",
      "Epoch 00413: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9113 - val_loss: 0.9116 - val_accuracy: 0.8039\n",
      "Epoch 414/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1973 - accuracy: 0.9062\n",
      "Epoch 00414: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.9138 - val_loss: 0.8728 - val_accuracy: 0.8431\n",
      "Epoch 415/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1480 - accuracy: 0.9688\n",
      "Epoch 00415: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9187 - val_loss: 0.8254 - val_accuracy: 0.8137\n",
      "Epoch 416/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2061 - accuracy: 0.9062\n",
      "Epoch 00416: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9113 - val_loss: 1.0143 - val_accuracy: 0.8137\n",
      "Epoch 417/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2080 - accuracy: 0.9375\n",
      "Epoch 00417: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.8892 - val_loss: 0.8655 - val_accuracy: 0.7941\n",
      "Epoch 418/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1118 - accuracy: 0.9688\n",
      "Epoch 00418: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9163 - val_loss: 0.9022 - val_accuracy: 0.7941\n",
      "Epoch 419/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1921 - accuracy: 0.9375\n",
      "Epoch 00419: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1661 - accuracy: 0.9187 - val_loss: 0.9793 - val_accuracy: 0.8039\n",
      "Epoch 420/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0968 - accuracy: 1.0000\n",
      "Epoch 00420: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9064 - val_loss: 0.9347 - val_accuracy: 0.7941\n",
      "Epoch 421/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2315 - accuracy: 0.8750\n",
      "Epoch 00421: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9261 - val_loss: 1.0319 - val_accuracy: 0.8039\n",
      "Epoch 422/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0839 - accuracy: 0.9688\n",
      "Epoch 00422: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9187 - val_loss: 0.9314 - val_accuracy: 0.8039\n",
      "Epoch 423/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1908 - accuracy: 0.8438\n",
      "Epoch 00423: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.8966 - val_loss: 0.9247 - val_accuracy: 0.7941\n",
      "Epoch 424/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0916 - accuracy: 0.9375\n",
      "Epoch 00424: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9089 - val_loss: 0.9344 - val_accuracy: 0.8235\n",
      "Epoch 425/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2920 - accuracy: 0.7812\n",
      "Epoch 00425: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9113 - val_loss: 1.0280 - val_accuracy: 0.8235\n",
      "Epoch 426/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1176 - accuracy: 0.9375\n",
      "Epoch 00426: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9089 - val_loss: 1.0495 - val_accuracy: 0.8333\n",
      "Epoch 427/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1386 - accuracy: 0.9375\n",
      "Epoch 00427: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1498 - accuracy: 0.9286 - val_loss: 0.9916 - val_accuracy: 0.8137\n",
      "Epoch 428/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1671 - accuracy: 0.9062\n",
      "Epoch 00428: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9310 - val_loss: 1.0365 - val_accuracy: 0.8431\n",
      "Epoch 429/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1669 - accuracy: 0.9375\n",
      "Epoch 00429: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9286 - val_loss: 0.9844 - val_accuracy: 0.7941\n",
      "Epoch 430/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0660 - accuracy: 0.9688\n",
      "Epoch 00430: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9163 - val_loss: 0.6960 - val_accuracy: 0.8039\n",
      "Epoch 431/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0871 - accuracy: 1.0000\n",
      "Epoch 00431: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9138 - val_loss: 0.8230 - val_accuracy: 0.8137\n",
      "Epoch 432/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0915 - accuracy: 0.9375\n",
      "Epoch 00432: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.9236 - val_loss: 0.9531 - val_accuracy: 0.8235\n",
      "Epoch 433/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1225 - accuracy: 0.8750\n",
      "Epoch 00433: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9187 - val_loss: 0.8958 - val_accuracy: 0.8431\n",
      "Epoch 434/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1069 - accuracy: 0.9375\n",
      "Epoch 00434: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9163 - val_loss: 0.9725 - val_accuracy: 0.8529\n",
      "Epoch 435/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1215 - accuracy: 0.9375\n",
      "Epoch 00435: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9212 - val_loss: 0.9421 - val_accuracy: 0.8039\n",
      "Epoch 436/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1271 - accuracy: 0.9062\n",
      "Epoch 00436: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.9089 - val_loss: 0.8241 - val_accuracy: 0.8137\n",
      "Epoch 437/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2185 - accuracy: 0.8750\n",
      "Epoch 00437: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.8966 - val_loss: 0.8527 - val_accuracy: 0.7941\n",
      "Epoch 438/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1559 - accuracy: 0.9688\n",
      "Epoch 00438: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9212 - val_loss: 0.8906 - val_accuracy: 0.8039\n",
      "Epoch 439/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2045 - accuracy: 0.7812\n",
      "Epoch 00439: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.9163 - val_loss: 0.9430 - val_accuracy: 0.8627\n",
      "Epoch 440/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2494 - accuracy: 0.9375\n",
      "Epoch 00440: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9212 - val_loss: 0.9030 - val_accuracy: 0.8431\n",
      "Epoch 441/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1312 - accuracy: 0.9688\n",
      "Epoch 00441: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9163 - val_loss: 0.8859 - val_accuracy: 0.8333\n",
      "Epoch 442/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2437 - accuracy: 0.8750\n",
      "Epoch 00442: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.8916 - val_loss: 0.8575 - val_accuracy: 0.8333\n",
      "Epoch 443/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0752 - accuracy: 0.9688\n",
      "Epoch 00443: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9212 - val_loss: 0.9451 - val_accuracy: 0.7745\n",
      "Epoch 444/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2319 - accuracy: 0.8125\n",
      "Epoch 00444: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.8867 - val_loss: 1.0656 - val_accuracy: 0.8039\n",
      "Epoch 445/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0981 - accuracy: 0.9375\n",
      "Epoch 00445: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.8941 - val_loss: 0.9272 - val_accuracy: 0.7941\n",
      "Epoch 446/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2141 - accuracy: 0.9062\n",
      "Epoch 00446: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.8990 - val_loss: 0.8194 - val_accuracy: 0.8333\n",
      "Epoch 447/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1193 - accuracy: 0.9375\n",
      "Epoch 00447: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.9138 - val_loss: 0.8499 - val_accuracy: 0.8431\n",
      "Epoch 448/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1676 - accuracy: 0.9062\n",
      "Epoch 00448: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9286 - val_loss: 0.9822 - val_accuracy: 0.8039\n",
      "Epoch 449/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1449 - accuracy: 0.9375\n",
      "Epoch 00449: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9015 - val_loss: 1.1353 - val_accuracy: 0.8235\n",
      "Epoch 450/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1072 - accuracy: 0.9062\n",
      "Epoch 00450: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.8793 - val_loss: 1.2136 - val_accuracy: 0.8431\n",
      "Epoch 451/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1270 - accuracy: 0.9375\n",
      "Epoch 00451: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9286 - val_loss: 0.9738 - val_accuracy: 0.8333\n",
      "Epoch 452/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2318 - accuracy: 0.9375\n",
      "Epoch 00452: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9236 - val_loss: 0.8958 - val_accuracy: 0.8235\n",
      "Epoch 453/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0516 - accuracy: 0.9688\n",
      "Epoch 00453: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9212 - val_loss: 0.9512 - val_accuracy: 0.8333\n",
      "Epoch 454/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9062\n",
      "Epoch 00454: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.8966 - val_loss: 0.9046 - val_accuracy: 0.8333\n",
      "Epoch 455/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0939 - accuracy: 0.9062\n",
      "Epoch 00455: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.8990 - val_loss: 0.8919 - val_accuracy: 0.8235\n",
      "Epoch 456/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0752 - accuracy: 1.0000\n",
      "Epoch 00456: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.8966 - val_loss: 0.9033 - val_accuracy: 0.8235\n",
      "Epoch 457/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2132 - accuracy: 0.8750\n",
      "Epoch 00457: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9064 - val_loss: 1.0352 - val_accuracy: 0.8333\n",
      "Epoch 458/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2085 - accuracy: 0.9062\n",
      "Epoch 00458: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9163 - val_loss: 1.0591 - val_accuracy: 0.8235\n",
      "Epoch 459/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1639 - accuracy: 0.8750\n",
      "Epoch 00459: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.9138 - val_loss: 0.9584 - val_accuracy: 0.8039\n",
      "Epoch 460/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1679 - accuracy: 0.9062\n",
      "Epoch 00460: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9187 - val_loss: 1.0063 - val_accuracy: 0.8137\n",
      "Epoch 461/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1278 - accuracy: 0.9062\n",
      "Epoch 00461: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9064 - val_loss: 1.0630 - val_accuracy: 0.8235\n",
      "Epoch 462/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 00462: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1444 - accuracy: 0.9212 - val_loss: 1.1591 - val_accuracy: 0.8235\n",
      "Epoch 463/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0971 - accuracy: 0.9375\n",
      "Epoch 00463: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.9113 - val_loss: 1.0524 - val_accuracy: 0.8235\n",
      "Epoch 464/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2229 - accuracy: 0.8750\n",
      "Epoch 00464: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.8818 - val_loss: 1.0292 - val_accuracy: 0.8137\n",
      "Epoch 465/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0905 - accuracy: 0.9688\n",
      "Epoch 00465: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9163 - val_loss: 0.8994 - val_accuracy: 0.8235\n",
      "Epoch 466/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1700 - accuracy: 0.9062\n",
      "Epoch 00466: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9261 - val_loss: 0.8479 - val_accuracy: 0.8235\n",
      "Epoch 467/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1216 - accuracy: 0.9375\n",
      "Epoch 00467: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9286 - val_loss: 0.8557 - val_accuracy: 0.8529\n",
      "Epoch 468/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0916 - accuracy: 0.9375\n",
      "Epoch 00468: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1418 - accuracy: 0.9187 - val_loss: 0.9141 - val_accuracy: 0.8333\n",
      "Epoch 469/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1922 - accuracy: 0.9062\n",
      "Epoch 00469: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9163 - val_loss: 1.0308 - val_accuracy: 0.8333\n",
      "Epoch 470/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2371 - accuracy: 0.8438\n",
      "Epoch 00470: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.8990 - val_loss: 0.8884 - val_accuracy: 0.8137\n",
      "Epoch 471/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0767 - accuracy: 0.9375\n",
      "Epoch 00471: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.9212 - val_loss: 0.8350 - val_accuracy: 0.8039\n",
      "Epoch 472/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3598 - accuracy: 0.9062\n",
      "Epoch 00472: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9212 - val_loss: 0.9177 - val_accuracy: 0.8039\n",
      "Epoch 473/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9375\n",
      "Epoch 00473: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.9212 - val_loss: 0.9585 - val_accuracy: 0.8137\n",
      "Epoch 474/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1769 - accuracy: 0.9062\n",
      "Epoch 00474: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.8966 - val_loss: 0.9872 - val_accuracy: 0.8137\n",
      "Epoch 475/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9375\n",
      "Epoch 00475: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.9212 - val_loss: 0.8930 - val_accuracy: 0.8235\n",
      "Epoch 476/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1435 - accuracy: 0.9062\n",
      "Epoch 00476: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9113 - val_loss: 0.9189 - val_accuracy: 0.7843\n",
      "Epoch 477/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1805 - accuracy: 0.9062\n",
      "Epoch 00477: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9015 - val_loss: 0.8154 - val_accuracy: 0.8039\n",
      "Epoch 478/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3068 - accuracy: 0.8438\n",
      "Epoch 00478: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.8966 - val_loss: 0.7581 - val_accuracy: 0.8333\n",
      "Epoch 479/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2031 - accuracy: 0.7812\n",
      "Epoch 00479: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.9089 - val_loss: 0.8155 - val_accuracy: 0.8333\n",
      "Epoch 480/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1568 - accuracy: 0.9062\n",
      "Epoch 00480: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1738 - accuracy: 0.8966 - val_loss: 0.8985 - val_accuracy: 0.8431\n",
      "Epoch 481/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2654 - accuracy: 0.8125\n",
      "Epoch 00481: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1792 - accuracy: 0.8621 - val_loss: 0.8095 - val_accuracy: 0.8627\n",
      "Epoch 482/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1730 - accuracy: 0.9062\n",
      "Epoch 00482: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.8842 - val_loss: 0.9150 - val_accuracy: 0.8529\n",
      "Epoch 483/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0663 - accuracy: 0.9375\n",
      "Epoch 00483: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 0.9113 - val_loss: 0.9931 - val_accuracy: 0.8627\n",
      "Epoch 484/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1594 - accuracy: 0.9375\n",
      "Epoch 00484: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9261 - val_loss: 0.9713 - val_accuracy: 0.8529\n",
      "Epoch 485/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1117 - accuracy: 1.0000\n",
      "Epoch 00485: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9261 - val_loss: 0.9948 - val_accuracy: 0.8137\n",
      "Epoch 486/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1686 - accuracy: 0.9688\n",
      "Epoch 00486: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9261 - val_loss: 1.0605 - val_accuracy: 0.8137\n",
      "Epoch 487/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1127 - accuracy: 0.9375\n",
      "Epoch 00487: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.9163 - val_loss: 0.9416 - val_accuracy: 0.8431\n",
      "Epoch 488/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1732 - accuracy: 0.8750\n",
      "Epoch 00488: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1744 - accuracy: 0.8818 - val_loss: 1.0158 - val_accuracy: 0.8333\n",
      "Epoch 489/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1386 - accuracy: 0.9062\n",
      "Epoch 00489: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.9187 - val_loss: 0.8546 - val_accuracy: 0.7941\n",
      "Epoch 490/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1496 - accuracy: 0.9062\n",
      "Epoch 00490: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1538 - accuracy: 0.9212 - val_loss: 0.8836 - val_accuracy: 0.7941\n",
      "Epoch 491/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0764 - accuracy: 1.0000\n",
      "Epoch 00491: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.9064 - val_loss: 0.8924 - val_accuracy: 0.8235\n",
      "Epoch 492/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1468 - accuracy: 0.9375\n",
      "Epoch 00492: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.9113 - val_loss: 0.8831 - val_accuracy: 0.8137\n",
      "Epoch 493/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1902 - accuracy: 0.8750\n",
      "Epoch 00493: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.8941 - val_loss: 0.7831 - val_accuracy: 0.8137\n",
      "Epoch 494/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1150 - accuracy: 0.9688\n",
      "Epoch 00494: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9433 - val_loss: 0.7497 - val_accuracy: 0.8333\n",
      "Epoch 495/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1463 - accuracy: 0.9375\n",
      "Epoch 00495: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1736 - accuracy: 0.8990 - val_loss: 0.7958 - val_accuracy: 0.8137\n",
      "Epoch 496/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1811 - accuracy: 0.8750\n",
      "Epoch 00496: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1859 - accuracy: 0.8990 - val_loss: 0.8801 - val_accuracy: 0.7941\n",
      "Epoch 497/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1247 - accuracy: 0.9375\n",
      "Epoch 00497: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9187 - val_loss: 0.9036 - val_accuracy: 0.8137\n",
      "Epoch 498/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1082 - accuracy: 0.9375\n",
      "Epoch 00498: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1683 - accuracy: 0.9113 - val_loss: 1.0265 - val_accuracy: 0.8039\n",
      "Epoch 499/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0621 - accuracy: 1.0000\n",
      "Epoch 00499: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9310 - val_loss: 1.0620 - val_accuracy: 0.8137\n",
      "Epoch 500/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2770 - accuracy: 0.8750\n",
      "Epoch 00500: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.9163 - val_loss: 0.8652 - val_accuracy: 0.8235\n",
      "Epoch 501/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0907 - accuracy: 0.9688\n",
      "Epoch 00501: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9212 - val_loss: 0.9058 - val_accuracy: 0.8039\n",
      "Epoch 502/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1694 - accuracy: 0.8438\n",
      "Epoch 00502: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9015 - val_loss: 0.8570 - val_accuracy: 0.8235\n",
      "Epoch 503/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1333 - accuracy: 0.9062\n",
      "Epoch 00503: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9261 - val_loss: 0.7897 - val_accuracy: 0.8039\n",
      "Epoch 504/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1121 - accuracy: 0.9375\n",
      "Epoch 00504: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9286 - val_loss: 0.9736 - val_accuracy: 0.8333\n",
      "Epoch 505/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2899 - accuracy: 0.9062\n",
      "Epoch 00505: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1586 - accuracy: 0.9236 - val_loss: 0.9623 - val_accuracy: 0.8137\n",
      "Epoch 506/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2110 - accuracy: 0.8750\n",
      "Epoch 00506: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9163 - val_loss: 0.8545 - val_accuracy: 0.8431\n",
      "Epoch 507/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0826 - accuracy: 1.0000\n",
      "Epoch 00507: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1561 - accuracy: 0.9236 - val_loss: 0.8384 - val_accuracy: 0.8235\n",
      "Epoch 508/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2416 - accuracy: 0.8438\n",
      "Epoch 00508: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1473 - accuracy: 0.9212 - val_loss: 1.1029 - val_accuracy: 0.8333\n",
      "Epoch 509/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2538 - accuracy: 0.9062\n",
      "Epoch 00509: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9113 - val_loss: 0.9516 - val_accuracy: 0.8039\n",
      "Epoch 510/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2053 - accuracy: 0.9062\n",
      "Epoch 00510: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1504 - accuracy: 0.9286 - val_loss: 0.9148 - val_accuracy: 0.8235\n",
      "Epoch 511/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2957 - accuracy: 0.9062\n",
      "Epoch 00511: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9212 - val_loss: 0.7783 - val_accuracy: 0.7745\n",
      "Epoch 512/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2846 - accuracy: 0.9062\n",
      "Epoch 00512: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1992 - accuracy: 0.8867 - val_loss: 0.8164 - val_accuracy: 0.8235\n",
      "Epoch 513/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2357 - accuracy: 0.9062\n",
      "Epoch 00513: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.8941 - val_loss: 0.8191 - val_accuracy: 0.8431\n",
      "Epoch 514/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0640 - accuracy: 0.9688\n",
      "Epoch 00514: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9212 - val_loss: 0.8101 - val_accuracy: 0.8333\n",
      "Epoch 515/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0736 - accuracy: 1.0000\n",
      "Epoch 00515: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1647 - accuracy: 0.9286 - val_loss: 0.9446 - val_accuracy: 0.8333\n",
      "Epoch 516/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1043 - accuracy: 0.9062\n",
      "Epoch 00516: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.9015 - val_loss: 1.0087 - val_accuracy: 0.8137\n",
      "Epoch 517/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2502 - accuracy: 0.8750\n",
      "Epoch 00517: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9212 - val_loss: 0.9511 - val_accuracy: 0.8333\n",
      "Epoch 518/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2535 - accuracy: 0.9062\n",
      "Epoch 00518: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9138 - val_loss: 0.8522 - val_accuracy: 0.8235\n",
      "Epoch 519/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2804 - accuracy: 0.8750\n",
      "Epoch 00519: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9212 - val_loss: 0.9010 - val_accuracy: 0.8235\n",
      "Epoch 520/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1914 - accuracy: 0.9062\n",
      "Epoch 00520: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9286 - val_loss: 0.9192 - val_accuracy: 0.8333\n",
      "Epoch 521/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1450 - accuracy: 0.9062\n",
      "Epoch 00521: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1987 - accuracy: 0.9064 - val_loss: 0.9496 - val_accuracy: 0.8235\n",
      "Epoch 522/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1380 - accuracy: 0.9375\n",
      "Epoch 00522: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9138 - val_loss: 0.9007 - val_accuracy: 0.8333\n",
      "Epoch 523/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0753 - accuracy: 0.9688\n",
      "Epoch 00523: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9138 - val_loss: 0.9998 - val_accuracy: 0.8431\n",
      "Epoch 524/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1840 - accuracy: 0.8750\n",
      "Epoch 00524: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1437 - accuracy: 0.9261 - val_loss: 0.9668 - val_accuracy: 0.8235\n",
      "Epoch 525/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0880 - accuracy: 0.9375\n",
      "Epoch 00525: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1193 - accuracy: 0.9335 - val_loss: 1.0556 - val_accuracy: 0.8039\n",
      "Epoch 526/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0543 - accuracy: 1.0000\n",
      "Epoch 00526: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9113 - val_loss: 0.9752 - val_accuracy: 0.7941\n",
      "Epoch 527/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0780 - accuracy: 0.9375\n",
      "Epoch 00527: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9212 - val_loss: 1.0355 - val_accuracy: 0.7941\n",
      "Epoch 528/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1408 - accuracy: 0.9375\n",
      "Epoch 00528: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1498 - accuracy: 0.9187 - val_loss: 0.8988 - val_accuracy: 0.8039\n",
      "Epoch 529/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2182 - accuracy: 0.8438\n",
      "Epoch 00529: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9163 - val_loss: 0.9179 - val_accuracy: 0.7843\n",
      "Epoch 530/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1128 - accuracy: 0.9062\n",
      "Epoch 00530: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1469 - accuracy: 0.9187 - val_loss: 0.9237 - val_accuracy: 0.7941\n",
      "Epoch 531/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1872 - accuracy: 0.9062\n",
      "Epoch 00531: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9163 - val_loss: 0.9667 - val_accuracy: 0.7941\n",
      "Epoch 532/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1764 - accuracy: 0.8750\n",
      "Epoch 00532: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9409 - val_loss: 0.9813 - val_accuracy: 0.8235\n",
      "Epoch 533/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0796 - accuracy: 0.9688\n",
      "Epoch 00533: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.9310 - val_loss: 1.1187 - val_accuracy: 0.8137\n",
      "Epoch 534/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3185 - accuracy: 0.8125\n",
      "Epoch 00534: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9039 - val_loss: 1.0874 - val_accuracy: 0.7843\n",
      "Epoch 535/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0823 - accuracy: 0.9688\n",
      "Epoch 00535: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9310 - val_loss: 1.1210 - val_accuracy: 0.7941\n",
      "Epoch 536/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1108 - accuracy: 0.9375\n",
      "Epoch 00536: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.9064 - val_loss: 1.1181 - val_accuracy: 0.8039\n",
      "Epoch 537/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9688\n",
      "Epoch 00537: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9360 - val_loss: 1.1180 - val_accuracy: 0.8137\n",
      "Epoch 538/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2014 - accuracy: 0.8750\n",
      "Epoch 00538: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9384 - val_loss: 1.2863 - val_accuracy: 0.8039\n",
      "Epoch 539/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9688\n",
      "Epoch 00539: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9212 - val_loss: 1.1006 - val_accuracy: 0.7941\n",
      "Epoch 540/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1345 - accuracy: 0.9062\n",
      "Epoch 00540: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9138 - val_loss: 0.9379 - val_accuracy: 0.8333\n",
      "Epoch 541/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2479 - accuracy: 0.8438\n",
      "Epoch 00541: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9015 - val_loss: 0.8805 - val_accuracy: 0.8039\n",
      "Epoch 542/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1212 - accuracy: 0.8750\n",
      "Epoch 00542: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.9187 - val_loss: 0.9884 - val_accuracy: 0.8431\n",
      "Epoch 543/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1289 - accuracy: 0.9375\n",
      "Epoch 00543: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.9261 - val_loss: 0.9933 - val_accuracy: 0.8333\n",
      "Epoch 544/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1402 - accuracy: 0.9062\n",
      "Epoch 00544: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9089 - val_loss: 0.9762 - val_accuracy: 0.7941\n",
      "Epoch 545/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1610 - accuracy: 0.9062\n",
      "Epoch 00545: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1449 - accuracy: 0.9187 - val_loss: 0.8312 - val_accuracy: 0.7843\n",
      "Epoch 546/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1644 - accuracy: 0.9375\n",
      "Epoch 00546: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9286 - val_loss: 0.8871 - val_accuracy: 0.8333\n",
      "Epoch 547/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1067 - accuracy: 0.9375\n",
      "Epoch 00547: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9138 - val_loss: 0.6962 - val_accuracy: 0.8235\n",
      "Epoch 548/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2393 - accuracy: 0.9375\n",
      "Epoch 00548: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1466 - accuracy: 0.9409 - val_loss: 0.7933 - val_accuracy: 0.8137\n",
      "Epoch 549/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1883 - accuracy: 0.9062\n",
      "Epoch 00549: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1571 - accuracy: 0.9360 - val_loss: 0.8805 - val_accuracy: 0.8235\n",
      "Epoch 550/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1577 - accuracy: 0.9062\n",
      "Epoch 00550: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9212 - val_loss: 0.9436 - val_accuracy: 0.8333\n",
      "Epoch 551/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1657 - accuracy: 0.8750\n",
      "Epoch 00551: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9286 - val_loss: 0.7003 - val_accuracy: 0.8235\n",
      "Epoch 552/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1990 - accuracy: 0.8750\n",
      "Epoch 00552: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9163 - val_loss: 0.7005 - val_accuracy: 0.8137\n",
      "Epoch 553/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1928 - accuracy: 0.8750\n",
      "Epoch 00553: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9187 - val_loss: 0.7200 - val_accuracy: 0.8333\n",
      "Epoch 554/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1559 - accuracy: 0.8438\n",
      "Epoch 00554: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.9212 - val_loss: 0.7831 - val_accuracy: 0.8235\n",
      "Epoch 555/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2359 - accuracy: 0.9375\n",
      "Epoch 00555: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9335 - val_loss: 0.8364 - val_accuracy: 0.8039\n",
      "Epoch 556/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1599 - accuracy: 0.9062\n",
      "Epoch 00556: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9236 - val_loss: 0.7526 - val_accuracy: 0.8431\n",
      "Epoch 557/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0986 - accuracy: 0.9688\n",
      "Epoch 00557: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1740 - accuracy: 0.9236 - val_loss: 0.6647 - val_accuracy: 0.7941\n",
      "Epoch 558/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3398 - accuracy: 0.8438\n",
      "Epoch 00558: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9187 - val_loss: 0.7197 - val_accuracy: 0.8137\n",
      "Epoch 559/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0848 - accuracy: 0.9688\n",
      "Epoch 00559: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1940 - accuracy: 0.9236 - val_loss: 0.8711 - val_accuracy: 0.8039\n",
      "Epoch 560/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1087 - accuracy: 0.9375\n",
      "Epoch 00560: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9286 - val_loss: 0.9675 - val_accuracy: 0.7843\n",
      "Epoch 561/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2143 - accuracy: 0.8125\n",
      "Epoch 00561: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.9113 - val_loss: 0.8555 - val_accuracy: 0.8137\n",
      "Epoch 562/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0762 - accuracy: 1.0000\n",
      "Epoch 00562: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9335 - val_loss: 0.9522 - val_accuracy: 0.8039\n",
      "Epoch 563/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0687 - accuracy: 0.9688\n",
      "Epoch 00563: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9384 - val_loss: 0.9416 - val_accuracy: 0.8235\n",
      "Epoch 564/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 00564: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9310 - val_loss: 0.9498 - val_accuracy: 0.8137\n",
      "Epoch 565/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2679 - accuracy: 0.8438\n",
      "Epoch 00565: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9039 - val_loss: 0.9337 - val_accuracy: 0.7843\n",
      "Epoch 566/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2339 - accuracy: 0.8750\n",
      "Epoch 00566: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9236 - val_loss: 0.9697 - val_accuracy: 0.7941\n",
      "Epoch 567/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1853 - accuracy: 0.9062\n",
      "Epoch 00567: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9113 - val_loss: 0.8538 - val_accuracy: 0.8039\n",
      "Epoch 568/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1594 - accuracy: 0.8750\n",
      "Epoch 00568: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.8990 - val_loss: 0.8002 - val_accuracy: 0.8137\n",
      "Epoch 569/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1441 - accuracy: 0.9062\n",
      "Epoch 00569: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9286 - val_loss: 0.9640 - val_accuracy: 0.7745\n",
      "Epoch 570/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1212 - accuracy: 0.9688\n",
      "Epoch 00570: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9409 - val_loss: 1.0157 - val_accuracy: 0.8039\n",
      "Epoch 571/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1120 - accuracy: 0.9062\n",
      "Epoch 00571: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1324 - accuracy: 0.9261 - val_loss: 1.0472 - val_accuracy: 0.8235\n",
      "Epoch 572/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1693 - accuracy: 0.9375\n",
      "Epoch 00572: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9433 - val_loss: 1.1517 - val_accuracy: 0.8137\n",
      "Epoch 573/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1731 - accuracy: 0.8750\n",
      "Epoch 00573: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9163 - val_loss: 1.1013 - val_accuracy: 0.7941\n",
      "Epoch 574/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1182 - accuracy: 0.9688\n",
      "Epoch 00574: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9261 - val_loss: 1.0065 - val_accuracy: 0.7843\n",
      "Epoch 575/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0917 - accuracy: 0.9375\n",
      "Epoch 00575: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9212 - val_loss: 1.0588 - val_accuracy: 0.8137\n",
      "Epoch 576/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1581 - accuracy: 0.8438\n",
      "Epoch 00576: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1442 - accuracy: 0.9089 - val_loss: 1.0630 - val_accuracy: 0.8137\n",
      "Epoch 577/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0733 - accuracy: 0.9688\n",
      "Epoch 00577: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.9384 - val_loss: 0.9899 - val_accuracy: 0.8235\n",
      "Epoch 578/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1157 - accuracy: 0.9688\n",
      "Epoch 00578: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9212 - val_loss: 0.8956 - val_accuracy: 0.8137\n",
      "Epoch 579/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1089 - accuracy: 0.9688\n",
      "Epoch 00579: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9384 - val_loss: 0.7861 - val_accuracy: 0.8137\n",
      "Epoch 580/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1131 - accuracy: 0.9375\n",
      "Epoch 00580: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9212 - val_loss: 0.7524 - val_accuracy: 0.8235\n",
      "Epoch 581/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1757 - accuracy: 0.9062\n",
      "Epoch 00581: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1526 - accuracy: 0.9236 - val_loss: 0.8445 - val_accuracy: 0.7941\n",
      "Epoch 582/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2951 - accuracy: 0.8438\n",
      "Epoch 00582: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.9113 - val_loss: 0.8412 - val_accuracy: 0.8039\n",
      "Epoch 583/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2107 - accuracy: 0.8750\n",
      "Epoch 00583: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1753 - accuracy: 0.9138 - val_loss: 0.8524 - val_accuracy: 0.8137\n",
      "Epoch 584/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1759 - accuracy: 0.9062\n",
      "Epoch 00584: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9286 - val_loss: 0.7667 - val_accuracy: 0.8235\n",
      "Epoch 585/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1007 - accuracy: 0.9688\n",
      "Epoch 00585: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1630 - accuracy: 0.9310 - val_loss: 0.9140 - val_accuracy: 0.8039\n",
      "Epoch 586/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1083 - accuracy: 0.9375\n",
      "Epoch 00586: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9286 - val_loss: 0.9421 - val_accuracy: 0.8235\n",
      "Epoch 587/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1945 - accuracy: 0.9062\n",
      "Epoch 00587: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.9163 - val_loss: 0.9775 - val_accuracy: 0.8235\n",
      "Epoch 588/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2084 - accuracy: 0.9062\n",
      "Epoch 00588: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.9163 - val_loss: 0.8864 - val_accuracy: 0.8235\n",
      "Epoch 589/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0865 - accuracy: 1.0000\n",
      "Epoch 00589: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.9261 - val_loss: 0.9462 - val_accuracy: 0.8137\n",
      "Epoch 590/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1470 - accuracy: 0.9688\n",
      "Epoch 00590: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1273 - accuracy: 0.9483 - val_loss: 0.8559 - val_accuracy: 0.8137\n",
      "Epoch 591/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9062\n",
      "Epoch 00591: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1714 - accuracy: 0.9163 - val_loss: 0.8973 - val_accuracy: 0.7843\n",
      "Epoch 592/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0986 - accuracy: 0.9375\n",
      "Epoch 00592: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9286 - val_loss: 0.8837 - val_accuracy: 0.8039\n",
      "Epoch 593/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1684 - accuracy: 0.8750\n",
      "Epoch 00593: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 0.9212 - val_loss: 0.7492 - val_accuracy: 0.8333\n",
      "Epoch 594/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0932 - accuracy: 0.9688\n",
      "Epoch 00594: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9163 - val_loss: 0.7491 - val_accuracy: 0.8137\n",
      "Epoch 595/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 00595: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1613 - accuracy: 0.9212 - val_loss: 0.8097 - val_accuracy: 0.8333\n",
      "Epoch 596/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1234 - accuracy: 0.9375\n",
      "Epoch 00596: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.9212 - val_loss: 0.9173 - val_accuracy: 0.8039\n",
      "Epoch 597/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0998 - accuracy: 1.0000\n",
      "Epoch 00597: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9187 - val_loss: 0.8977 - val_accuracy: 0.8039\n",
      "Epoch 598/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1921 - accuracy: 0.9062\n",
      "Epoch 00598: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9310 - val_loss: 0.8131 - val_accuracy: 0.7941\n",
      "Epoch 599/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2483 - accuracy: 0.8750\n",
      "Epoch 00599: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.9310 - val_loss: 0.9066 - val_accuracy: 0.7941\n",
      "Epoch 600/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1553 - accuracy: 0.9062\n",
      "Epoch 00600: val_loss did not improve from 0.45224\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9286 - val_loss: 0.8822 - val_accuracy: 0.8235\n",
      "Training completed in time:  0:00:24.767989\n"
     ]
    }
   ],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 600\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.35294222831726\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3424139e+02,  1.1067206e+02, -6.6630416e+00,  4.1293573e+00,\n",
       "       -1.1308483e+01, -1.0980946e+01, -1.5250362e+01,  1.9236928e+00,\n",
       "       -1.4809805e+01,  2.3802032e-01, -7.7702599e+00,  3.6280439e+00,\n",
       "       -5.6303148e+00, -6.6174512e+00, -7.5932426e+00, -5.3561540e+00,\n",
       "       -2.5885587e+00, -5.4182224e+00, -5.7990346e+00, -5.9867935e+00,\n",
       "       -1.1707730e+00, -2.6819625e+00, -3.1655645e-01,  2.5990316e-01,\n",
       "       -9.8999280e-01,  5.7512655e+00, -1.5841848e+00,  4.2109928e-01,\n",
       "        1.8964884e+00,  3.1722341e+00,  2.6149845e+00,  5.7462988e+00,\n",
       "        2.8583210e+00,  1.4889371e+00,  3.4892721e+00, -8.1730686e-02,\n",
       "       -3.9798820e-01,  8.0889344e-01, -1.4317317e+00, -6.2499964e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Some Test Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.7407809e+02  9.4333420e+01 -1.8600550e+01  3.6770454e+01\n",
      " -8.4028921e+00 -1.6089230e+01 -1.5080973e+01 -2.5797531e+01\n",
      " -8.6793633e+00 -1.1694519e+01 -1.3656996e+01 -1.4920211e+01\n",
      " -1.0464228e+01 -6.5972204e+00 -8.0304260e+00 -5.9242659e+00\n",
      " -3.3941271e+00 -2.4859211e+00 -8.9847240e+00 -4.2518182e+00\n",
      " -2.0214176e+00 -5.9973407e+00  3.7480056e-01 -2.4540784e+00\n",
      " -4.6795554e+00 -1.7582238e+00 -4.5259929e+00 -2.8423886e+00\n",
      " -6.7155737e-01  1.0814966e-01  2.2921562e+00  2.6345947e+00\n",
      "  2.1705348e+00 -5.2751881e-01 -6.9644945e-03  1.1093524e+00\n",
      " -3.1303546e+00 -2.4162803e+00 -5.6212240e-01 -2.0515184e+00]\n",
      "[[-3.7407809e+02  9.4333420e+01 -1.8600550e+01  3.6770454e+01\n",
      "  -8.4028921e+00 -1.6089230e+01 -1.5080973e+01 -2.5797531e+01\n",
      "  -8.6793633e+00 -1.1694519e+01 -1.3656996e+01 -1.4920211e+01\n",
      "  -1.0464228e+01 -6.5972204e+00 -8.0304260e+00 -5.9242659e+00\n",
      "  -3.3941271e+00 -2.4859211e+00 -8.9847240e+00 -4.2518182e+00\n",
      "  -2.0214176e+00 -5.9973407e+00  3.7480056e-01 -2.4540784e+00\n",
      "  -4.6795554e+00 -1.7582238e+00 -4.5259929e+00 -2.8423886e+00\n",
      "  -6.7155737e-01  1.0814966e-01  2.2921562e+00  2.6345947e+00\n",
      "   2.1705348e+00 -5.2751881e-01 -6.9644945e-03  1.1093524e+00\n",
      "  -3.1303546e+00 -2.4162803e+00 -5.6212240e-01 -2.0515184e+00]]\n",
      "(1, 40)\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"audio/test/S15_P11_F.wav\"\n",
    "# S53_P16_F.wav\n",
    "# S15_P12_F\n",
    "# S47_P5_M\n",
    "# S66_P13_F\n",
    "# S80_P9_F\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "prediction_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your pronunciation need to be improve\n"
     ]
    }
   ],
   "source": [
    "# if (prediction_class == 1):\n",
    "#     print(\"Well Done.You have pronunced words correctly.\")\n",
    "# else:\n",
    "#     print(\"Your pronunciation need to be improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with some text data to give feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on...\n",
      "new serials transition from FTP to mmp demonstrate the benefits and drawbacks of each system in the 1970 Newzealand Grove disillusioned with the two-party system if PP did not provide water with another viable option however the late in third party received a considerable 16% of the word in 1978 but again only one of the two seats in Parliament three years later they would she was up to 21% but they gain only to seeds a royal Commission subsequent to recommended the shift to m m p and in 1993 straight right refund amount was held that was in favour of the reform\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "if (prediction_class == 1):\n",
    "    print(\"Well Done.You have pronunced words correctly.\")\n",
    "else:\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(filename) as source:\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print('working on...')\n",
    "            print(text)\n",
    "        except:\n",
    "            print('Sorry...run again...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Use of [] in place of [s]..Have pronunciation errors when speaking cell, sell, seat like words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "\n",
    "txt = text\n",
    "\n",
    "#Check if \"ain\" is present at the end of a WORD:\n",
    "\n",
    "# re.findall(r\"\\band\\b|\\bor\\b|\\bnot\\b\", \"These are oranges and apples and pears, but not pinapples or ..\")\n",
    "\n",
    "# torsional mode of vibration is a consequence of\n",
    "set1 = re.findall(r\"\\bbowling\\b|\\bball\\b|\\browing\\b|\\brow\\b|\\bhall\\b|\\bholes\\b\", txt)\n",
    "set2 = re.findall(r\"\\btelephone\\b|\\bphones\\b\", txt)\n",
    "set3 = re.findall(r\"\\bfruit\\b|\\bfly\\b|\\bfraction\\b|\\bfan\\b\", txt)\n",
    "set4 = re.findall(r\"\\bpool\\b|\\bpenalties\\b|\\btransportation\\b|\\btransported\\b\", txt)\n",
    "set5 = re.findall(r\"\\bseat\\b|\\bseats\\b|\\bcent\\b\", txt)\n",
    "set6 = re.findall(r\"\\bzones\\b|\\blizard\\b|\\bzoo\\b\", txt)\n",
    "set7 = re.findall(r\"\\bwaist\\b|\\bvibration\\b|\\bwind\\b|\\bweight\\b\", txt)\n",
    "\n",
    "# print(x)\n",
    "\n",
    "\n",
    "if set1:\n",
    "  print(\"Error: Confusing /o/ and / /..Have pronunciation errors when speaking bowling, rowers, hall like words.\")\n",
    "if set2:\n",
    "  print(\"Error: Overuse of / / rather than /o/..Have pronunciation errors when speaking phone, yoghut like words.\")\n",
    "if set3:\n",
    "  print(\"Error: Confusing /p/ and /f/..Have pronunciation errors when speaking fan, pan, profit like words.\")\n",
    "if set4:\n",
    "  print(\"Error: Overuse of /f /..Have pronunciation errors when speaking Airport, pool like words.\")\n",
    "if set5:\n",
    "  print(\"Error: Use of [] in place of [s]..Have pronunciation errors when speaking cell, sell, seat like words.\")\n",
    "if set6:\n",
    "  print(\"Error: Use of [s] in place of [z]..Have pronunciation errors when speaking zoo, zip like words.\")\n",
    "if set7:\n",
    "  print(\"Error: Use of [wr] and [ar]..Have pronunciation errors when speaking winter, win, west, vest, vantage like words.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
