{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with CNN*** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:159: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_of_audio</th>\n",
       "      <th>feature</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E2_P1.wav</td>\n",
       "      <td>[-3.6422656e+02  1.3304326e+02 -2.4707306e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3_P1.wav</td>\n",
       "      <td>[-3.7986017e+02  1.1947180e+02 -3.1631770e+00 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E2_P10.wav</td>\n",
       "      <td>[-3.5354391e+02  1.4500945e+02 -3.1722630e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E2_P11.wav</td>\n",
       "      <td>[-3.5276407e+02  1.3659334e+02 -3.1263487e+01 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E1_P12.wav</td>\n",
       "      <td>[-4.27301239e+02  1.29095963e+02 -3.96940780e+...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_of_audio                                            feature  lable\n",
       "0     E2_P1.wav  [-3.6422656e+02  1.3304326e+02 -2.4707306e+01 ...      1\n",
       "1     E3_P1.wav  [-3.7986017e+02  1.1947180e+02 -3.1631770e+00 ...      1\n",
       "2    E2_P10.wav  [-3.5354391e+02  1.4500945e+02 -3.1722630e+01 ...      1\n",
       "3    E2_P11.wav  [-3.5276407e+02  1.3659334e+02 -3.1263487e+01 ...      1\n",
       "4    E1_P12.wav  [-4.27301239e+02  1.29095963e+02 -3.96940780e+...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata=pd.read_csv('audio/metadata_new/final_metadata/combined_csv_new_updated - Copy.csv')\n",
    "df = pd.read_csv('audio/metadata_new/final_metadata/combined_csv_new_updated - Copy.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio('audio/rec/fold1/S1_P15_F.wav')\n",
    "data, sampling_rate = librosa.load('audio/rec - Copy/S1_P1_F.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.00143572,\n",
       "       -0.00139209, -0.00141668], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x20093ad1b20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x20093ff9f40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAEGCAYAAACjGskNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAB+xklEQVR4nO2dd5gW1fXHv3cLvffO0ruArCBiRVAEFTVq1EQx0VgS48+YGLEGO5aoidEk9t5iRREQERUUkN6kI733zrK79/fHzuzOO++dmXtn7pR393yeh4e3zDtzd8q95557zvcwzjkIgiAIgiAIgpAnK+4GEARBEARBEESmQUY0QRAEQRAEQShCRjRBEARBEARBKEJGNEEQBEEQBEEoQkY0QRAEQRAEQSiSE3cD/NCgQQOel5cXdzMIgiAIgiCIcszs2bN3cM4bir7LSCM6Ly8Ps2bNirsZBEEQBEEQRDmGMbbW6TsK5yAIgiAIgiAIRciIJgiCIAiCIAhFyIgmCIIgCIIgCEXIiCYIgiAIgiAIRciIJgiCIAiCIAhFtBjRjLGXGWPbGGOLHL5njLF/MsZWMsYWMMaOt3w3gjG2wvg3Qkd7CIIgCIIgCCJMdHmiXwUwxOX7cwB0MP5dB+DfAMAYqwfgbwD6AegL4G+Msbqa2kQQBEEQBEEQoaDFiOacfwdgl8smwwG8zkuYDqAOY6wpgLMBTOSc7+Kc7wYwEe7GOEEQBEEQBEHETlQx0c0BrLe832B85vR5Goyx6xhjsxhjs7Zv3x5aQwmCIAhCFz/vOIhrXp0ZdzMIggiBjEks5Jw/zznP55znN2worL5IEARBEIli6sodmLR0W9zNIAgiBKIyojcCaGl538L4zOlzgiAIgiAIgkgsURnRYwBcZah0nAhgL+d8M4AJAM5ijNU1EgrPMj4jCIIgiFhYvnW/tn0dKyzWti+CIJKFLom7dwBMA9CJMbaBMXYNY+wGxtgNxiZfAFgNYCWAFwD8HgA457sAPABgpvHvfuOz2Pjx513Yc6ggziYQBEEQMbF13xGc9dR32vb37OSV2vYVJU9MWIbHxi+NuxkEkWhydOyEc365x/ccwB8cvnsZwMs62qGDS/87DQPa18db154Yd1MIgiCIiCnQ7DnenaFOmX8Zxv9fh3SOuSUEkVwyJrEwSr5fuTPuJhAEQRDlAMZY3E0gCCIkyIi2cORYUdxNIAiCIMoRZEITRPmFjGgLBUXyy3icc2zcc1j5GP/6egWOFpKxThAEkUS+Wa63DgHXujeCIJIEGdEWshSW3aau3IEBo79WPsYTXy7Hiq0HlH9HEARBhM89nywCABwqKIy5JQRBJB0yoi2oLLvtO0wdLEEQRHmFa3IhFxVnpi+aQrkJwhsyoi2odHVvzVgbWjsIgiAIgiCIZENGtE827z0CADimEEdNEAQh4sY3Z2PlNgrzShKZ6T+On4/mbMCLU1bH3QyCiAQyogPix4jWtUxIEET5YNyiLfh47oa4m0EQgXn4i6V4cOySuJtBEJFARrRPDh4tXzHRExZvUZb4O3C0EDsPHA2pRQRRsXh28qq4m0BY4BXc2+E3JJpiqYmKBBnRFlQ6zW37jxq/8XGcBC4UXv/GbHw8d6PSb3732iz0efCrkFpEEARBZBrb95Njhag4kBEdAzsPJLMM7GZF3etpq6myI0EQ5ZPkuTqiJUNFRQgiUsiIjoEvf9oadxMIgiAIgiCIAJARTRAEETKFRcUZqxdMEARBiCEjOiD+hsVkDqbzN+yNuwlECBwuKMKBcpYIm2kM+ccU3Pjm7LibQShQwfMKCYKQgIzoWEhm+vK3y7fH3QQiBEa8/CNOe2xy3M2o0KzcdgBz1++JuxkEQRCERsiIJggNHCpIrqd31fYD2HkwmcmsFQnybGYWP+84GHcTCIJIOFqMaMbYEMbYMsbYSsbYSMH3TzHG5hn/ljPG9li+K7J8N0ZHe6JkdzkwTnTroT45cTnW7qw4A9DanQfR9d4JcTfDEbLdCEKdLUZVWkIveSPHYvNeNSUogkgqOUF3wBjLBvAsgMEANgCYyRgbwzn/ydyGc/4ny/Z/BNDbsovDnPNeQdsRF0s270PLetXibkYg9muOl/3npBUAgFsHd9S636Sy73ByvdAAFY1IOks274u7CYSALWTohcamPUfQtHbVuJtBEIHR4YnuC2Al53w157wAwLsAhrtsfzmAdzQcVztRmRrv/LguoiMRBOHEo+OX4qa350R4RHEPs3Cj3oTeK1+agcWbKEk4KJ8t2Bx3E8oxNLEnygc6jOjmANZb3m8wPkuDMdYaQBsAX1s+rsIYm8UYm84Yu8DpIIyx64ztZm3fTglwSWfsgk3Kvykq5pi8dJvUtnkjx2LbflpulaGgsDjuJiSSD2ZvwOfl0FCasmIHJQlrICuZ+d/lggNHi+JuAkFoIerEwssAfMA5tz5BrTnn+QCuAPA0Y6yd6Iec8+c55/mc8/yGDRtG0VYiAKu2q8dEz1u/G795dab09lv3UnlZGQ4W0ICVBJyiasKw1XYltCpqJsFY+bOiZ67ZlYjwrqPHqE8iygc6jOiNAFpa3rcwPhNxGWyhHJzzjcb/qwF8g9R46cQTf3cUnEUJ0YdW7dsLi5PhYeXGXVBMxTSIhPDi1J/jbkLGUx490Zf8Zxq27ItvBW/v4WOxHZsgwkCHET0TQAfGWBvGWCWUGMppKhuMsc4A6gKYZvmsLmOssvG6AYABAH6y/zYqEjBBj4WjCVnuN0+/l6dk5bb9AIDChBmtSXdcHSaPdArb9ydjJaM8ejyJ5BLnOFdYVDLWJKvnJqKgvFZsDWxEc84LAdwEYAKAJQDe55wvZozdzxg737LpZQDe5akWUhcAsxhj8wFMBjDaquqRRG59b16KPE95MLx5wro0L+Nm0JPfRdQSOVhCi+fY+WD2eu+NiNBwesoO09I2EQFJqlp6+4cL4m4CETHt7vyiXGqvB5a4AwDO+RcAvrB9dq/t/SjB734A0ENHG6Lio7kb0b9d/bibUS4xTdGnvlqBRy4S3xZbLUuRSZnAJG0S4sRHczfiyv55cTeDsLGuAmmqZxLTV++KuwlaSUIstMmeQxTWURHZfagAbVA97mZohSoWWpBdVU1OV1S+MM+rmwTgIUtIwsw1yRjkzLEp6atVCxIS+15RcTJiKJyDiILlW0vC4JJcXZUgMg0yoi34mahPWrJVf0MiZv+RYJ3q/PV79DREAqsh8uKU1ZEd143Jy0pk+f78/rx4G0JkJGRCE1Fw0JCVm7JiR2xt2EOJhRWaj+c4aU5kLmRE+8FibK/afiCyw27YfSiU/X4U8MaOq+LabmNJ8MUpq/HkxOWxtAEoi+H+ZJ66NjZRcXD0OJMVTURAseGAiPN2W7p5f+nrgwmK0SaiQTSJOlpYlNHKVmRE+2Dplv3eG2k/5j6c/OjkyI+bCTw4dklpqfEoOVpYhNd+WOMrvGfuut2RdxzlNTs607EaFjq58LnvkTdybCj7VuXg0UJ8vzI+DyhRRlLCh5KmrhQF4xdtqdDhNKI7r9Pd42N1ggWFjGgfHDhaNpuKqhvYui88Oa6gfWoxLzHQ8kaOxfpd4XjLk8jCDXvxtzGLff32wud+wDNfr9TcIiITCau64Nx1e0LZrx9e/WENfvXijLibUaFJnMmauAZ5U1TMsePAUWzee9jXBPWGN2fj47nlL6RBFidbY+W26Fb0dUNGtA/imEGb+ppJ5M6PF5YuFZ7yWLje8iT2u37nIHEWPagIHC0souIOCSFJyhAVFfMahO2IbttQTn0hU1SNrLw0dTXyH/wK4xdt8b2PuMIfk0B57AbIiPaBdSYZ1U1xrCi8A32zLBxPWBjsS6BRtGnPYe+NBJBhocayLfuxV0Eaa+SHC9Hzvi9DbJEaFfl6z1tPyjAq9Lr/S3w6T6/H0rz9wr4N3Wx0q+GciY/DNg0rwvsOV9xwDqdLvutQQaTt0AkZ0RZkDbQ4Hv6JP5WogCQ1AD+qcxJnZrkTkz0mIVNWbBd6RDNxEImTs5/+Dnd+slB6+7WG/vKWveTxj5uvDBWj3Qczd7CMkj2HjmmX8DT7mzhDoq0Oh7i6v2H/nIIdB/wZw9+tCO5wqsjd/mfzNwm9+D/+nAy5Wj+QEW1hfUjqFzrYbczUChIa1rFGQ8EIGaMyE2OnrnzpR/z321Vpn2ficmbcHD0mf/+biZQ3vDk7rOYQirw1Y23cTcgYPtWs9hNVb+Mmmfr10m0RtcKZxZv2+R5HNhsT8iDzkG8ScA6i5oilKuushNR30AUZ0VZ89DJRzerNziepRuSRiEoXj5mfmTJya3emT9ASuqgQORMWb1FI0pE/aWbM+bwIdczdMHV6KzLWYkmEO0H1++0w2/9hsW1/qpf36a+WY9zCzVqP0fmecXh7hrgo1+JNe7HTp6dZls0BVrf2V0Bpv92WcI3Fm8pXTDgZ0RnGsYR6onWEJviZkOw7khomMWHxlsgMehXGCgaRYornAJDZS3kqJHUVKUrenE6e6LhYZlQsjLrXefqrFaVKRDMsz3oQY/7IsWLMW79b+N2wf07F7R8uCLB3F4yTtyQGmdvywrTVO5W2X7fzUGkthiRCRrSFTDBpJi3xXgq64Nnv8Ye35kTQGv/88Z25+Mk2I/VjU977yaKU99e/MRvjFgXzeuSNHIvPF3h7vIOuQjCqskFUMPZp9q5mMuc9MxVz14kNwTBYsGEPAODz+Wr9o9+E2CPHijBhcWr8q3VXYY63Xon4fnte04v8XUiylEQ6pz4+GVe8MB0nP/p13E0RQkZ0QKJ2Jq7Y5j0Dnrd+j9DzGSaqp+Gz+Ztw4XPfBz6uqEpgloYYm3d/XO+5TZiKKTppWrtK6WsVdYuoULla5LzPfI4WRr9SlDdybGIKz5gs3LhX2Sung7W71PJX1vnU/p+weAuuf6MkHyGMVRi3vsBLe/2QptXKJBewStKqddB+e8W2A9iw258KVtiQEW3Bz4WO+hFKQmKGLo4WOj/kQYq26KjIJbOLoGVrvY5xqKBQi8FRu2pu6Wt7+EsSUHmGJpWj+58gdEz4ZfErcVfg0k+7YU0CFuXybNsfn2rO4Zhj88P+239YtQMd7hoX6jGIEsiIDsj8hCQtxc02DYVDrOPJiFd+VP69X71mKyoDRthe0VMenYzrXterLJGdlbwQEq/z+Pq0Nch/8KtoGhMzSfZsEcEQ5WqU55WVv3rEJW/YFZ9ncZWmBP0DPh0pW/eGG+MrSmSPE11zxZ93BFcB040WI5oxNoQxtowxtpIxNlLw/dWMse2MsXnGv2st341gjK0w/o3Q0Z7yTFJDCH5YpXdZ0s/gYnZoQYpamMuOMrv4TCJu2o0PZm9w/X7nwQKs2Bo8gSXphpmX1N+9ny72resaBTqriS7cSEVJksTiTfquh4o8Yxj47QWS2nsEadfXy/SsaI0as1h620MFZQb3ef+aquX4Trw4ZXWo+1dF12TxjCe+0bMjjQQ2ohlj2QCeBXAOgK4ALmeMdRVs+h7nvJfx70Xjt/UA/A1APwB9AfyNMVY3aJv8skyDwZLpZHJVNdPTE8RoNCfMVsPus/mb0PHu9KWxXZoLRyzdsi8tblPH1bAWG7F25EnBTLCMe4nVL+3vGpe4eFs3OOdaVo4qAiu26pMUfe6blWmfRRkf7rfYSgYPCY7oCqOxVi/2Ikpfhl8PeVjEocCUN3IsvhFMlnSHNOrwRPcFsJJzvppzXgDgXQDDJX97NoCJnPNdnPPdACYCGKKhTb548stlrt9/vzJ51fLKK36WbUzj+cUpP/s+7urtJccttjiN7v/8J2GYh47YayvXvDor7TMdcn1WXdLxi7ZEqghgxclINk9jl3vHR9ga/eiYoEQhe/jN8u3o+/Ck0I/jRCap0ug0cv/7Xbp38OmvVmjbvyzmLfb4hKV4QdCmikAcd6D9mGFKsW7VUJ5cJ3EkEwPA1a/MTHk/ftEWHDfqS63H0GFENwdglTLYYHxm5xeMsQWMsQ8YYy0VfxsJXkaRzjgjv8kaSSUJHmxT5P+nzf7F3A8ahpA1az6IRuW6gPfMbs1qGk98uRwXPveDr9/uOVTgu2zz5KXbHI3ksAa0qAcSHZ6mTxU8W36JW54rkyp1OhX0yGTMs//s5FV45mtvIz6s3Mcw7gLrOLQmYfGz9vP45U9b42lIDCTAPAAALAtB3zuqxMLPAORxzo9Dibf5NdUdMMauY4zNYozN2r49nEHAq69wysl6dPxS5WPd9sF85d/IEDQ+0+8ykN9n5NN5+oyGuev2lL7OGznWV1jHCoWEE5mxRWXpaKORGLly2/5Ehgf0un8iej8w0ddvN+1NpjxRUKwKLcu2BK/EdTCCkJb1RkJXkiSwksqRmOOYw0amh1wSwCkRNeMWlelSu41lUaqiOHHzO3PjbkKFI4yVPh1G9EYALS3vWxiflcI538k5N91CLwLoI/tbyz6e55znc87zGzZsqKHZ6XiV41zqMIv59zerlI/16bxNuPPjhY7f7z5YIL3cs27nodIYx6CJh3t8ej79liO3JtcF7dbsk5yP5rgn7omwFoDx0lQOqx9+Y1r5q+q2xaVMrvWO9buiMW/9nlgks0aPK5tAb98fPEY+Co/NV0tKPGBBZCQrCgmwtWJnyeZwcoXCOLXzjYIyADBlxQ485uDgiuO6BikVTqjhJD/73+9KbDWdK+c6jOiZADowxtowxioBuAzAGOsGjLGmlrfnA1hivJ4A4CzGWF0jofAs47NEMk2zAoXbUmHvByai8z1yMaKnPj65NMZRZza5CirqHNYllSkr9MWZ2xP97DrUMoa+VbKw5/1fui4JivrhTE2OC5udLmEg1tCm1T6XYC949vtYPDtvWMpY3+UyKU4iHMAPK3fgjo9CKpFMpBGv97/EcNhzqKCs/HpCltn9ILKD9hwsc3w8On4pnnNwcMVhRO87nDyN/qiI2vO/zSEEM4yVpcBGNOe8EMBNKDF+lwB4n3O+mDF2P2PsfGOzmxljixlj8wHcDOBq47e7ADyAEkN8JoD7jc9iJwkxvn4IWhkqij87rOXBT2yhIXYjetCT33qGeNjjqfe4dHyiGPq9GjpKrxWRMBi/aLMWOT0vRMewGqIfesj+uTF9dbxdh9tEQZYox5qV2w7gnk8X4R2J6pw6SUJiYXExjyVkSqb6X9j5MseKeGkfLNPXJNUbv+dQ+vO2WVJ1Jgn3IAAsryCKYFHnQUQpAqElJppz/gXnvCPnvB3n/CHjs3s552OM13dwzrtxzntyzs/gnC+1/PZlznl7498rOtqjgy2ChzGJEnjvzdSb+BJFh+kUq6b72MUaMr3clCxEEy17Z+FnUvLRHPc48QNHC7FIs57wDW/OwV0fL9K6TytmIZz/eRjJTp6jikKUc/fr35iNVduTlXwVFUl2kYSn6+6vgw1rSFhkWzU1V1HfnrEOkyUqk4qql2ZLNjYpE4Oznvou7iZUSHT2s1Sx0IGkzFS9+GZZapJl0A44ikHcKSP84NFgoRD2pRrR7Fd1hWG5i07s94IQligSkZ6euBznPqNfrP/HNf48ub9+cQZen7bGdRvzPtWhT1xQWJzolaLpq3cmKjH0T+/Nw5wIZQ1nr3U/VhLUOeLq3WUm9mHJHFoNR/sqnezvgvD4hNT45NW2Cdywf07F4YIi3PnxQt+hWbKyo7qLg4XB9yt34NTHJsfdDC3oLBIn0/d7PUMyK0KykBHtwM6DydJZdMJeye2TucGq6EWBk2yb7njBLxZu8d7IA7c++fyezdI+s5eBD8PjoTIAOqFqhH5ikV6zvp60ZCumrtyBez+Vq9w1c01wY67j3eMS7bG+3Sh3nBRD+uO5G/H5/M2RHKugsBi/+PcPrhUmw5CZ8ksSJ2NR+KG9qqVaKdRkAD07OfWZFRk65gTLb0hbEq+niWq//asXZ2g19oJyuKAIN78z11fe1YcKSf5TVmx3VRlbL1Eu/v1Z7iFqhcX6bA0yoh0Y9s9wy3Lqwm6UBBU1j8JLFJVG9obdwTsgNxu4ae0qaZ8VFgcP5/BCh2F+SDEB8pb35glfX/NaeoEYN0wZv6CEFb+dN3KsY2a3HackUp168rqIyvtrnjs3T5D9GYkS0xNstiDBNpd2/K5SiorE6ODzBekTu0xZAfbDlBV6pXm37D3iqnqkm2+Xb8eY+Ztw3euzlX+r4iC78qUfMdUlplmmL9t32L0Pp3AOgpAgbGO9d6s6aZ/ZPSFJWLoW4WSIf7EwfI9lHAomV740Q3pbWSP6xrfSBxMdcfhhoMub6IXovkpiqXkTr7PiJGsa1vEAYPV2faXGrYhiiIno0C3MMvipb3H20+HHVM9eWxLmN2Fxycqun3Aj1Z+4bS4z0Yoy5p2M6HJGgxqV426Cb3Tf+DrMhk/nOYfH1Kicm/aZPSYv0zwrv39rjvZ92vWbVSY3TsuzqiXXp6zYIR1GUCTZ49vzEQA96iwyFBQWK3mhTAWUqJa7v1ladm5I9lENvwWvkkLdaun9ohMqxah0s8CiKZ2J7D9SGHp/s+dQAX7x72kASsLCAH9GtOrYfsxljJDZV5SjLhnRIbF4017XuMCwqF45O/JjJhXRg6T6+FsHtIkSZVrtBV/clqX8Yh5i9fYDvr1WUZbE7vvQpJT3Kt75ebYY89J9+OjIn/tmpev3+40BfapG7fKweOLLZTjxkUneG9oI24Y28x0+W+A8+dTdhj+8NUd6BWXAo1/jz++XVYvdJBFetN1BczYsrvexXJ4kWtevJr3tyaO/DrEl6VhXmc7/1/ehHeeujxeWenBNglYTDkreyLH43OW5FKFLKUbVmeSlUOWFV6t1dkFkRGvC7m0Z9s+pyH/wK1/7si4Jr4s4vtItdolzHtpSYyYwetwSz23sBsJWDWoUTgx+6jvfEkmbLWW4/SZ0ylTUFHmdVYwonfGzbqsKAGAeKjyJMW+2S068n/cZpxr2X2Z6x3UWUfJi7MLN0tVJN+89khKbKuMJPeEhf/24CJl7Pw6d+LjYdyT1bw1jGd464ZYN1QrKWzPW4c3pqfKzL079OZJju/HIF+IKjk6YXaG1r/czCV4oIclqtXvcvN32Wg5xQ0a0JvYf1bes0vbOL0pfT1zi7f20EtTL41aV8YdVOzHw798mOgPaitUYMtt809tzUh7WF75bLb3Mb9fUlTkPr/6wpvT10cIiX+fOKc62qJj7NjK/XFx2X71maaMKMgl0VmNdJzrtXNNDZA7gQXYd1Aj4brl68tGcdbulQztkl2LzRo71da+u3Jb+LKmG3vjhqyXy8b7b9h8tTTqOuivTqQqQVGpXraT8G11jimgv1l1Hcbn3GqsxH8+V86buOliAvJFj8cOq8Ceeqond5qqhGQ8NqCely3Lq45OxdmfJGCuq02Gyec9hT+eU9Zov3BBuFWcyojWhOsOTRcXzW1QcPI3NbcD7WjExZeeBo1gfo0TPQcHDPmHxVvxtTJkc20NfLMG9n+orMuJmL3S6ezwG2JYvZQaPv3wwP+W9tSqlX/vEaty/O9NfxboPZodf6e7tGeJiQmPm65NyXGk8Y1sNQ/TTecGWEr343euzcNFz+paSL3ruB1z/plwIgIytYt6TfuyaLyVCnpJQ6uS0x7+J5bjvaqgOebigKDHyiSLsIW0y6JrMqO5HVPUwKJOXqY2Txz8wEQBw/2c/aW+LLl6cUuZFF8XsFxXzwOEqG3YfLq3LsMDF8J28bDv6PeweymZdcVglsKHciqipQka0D9o3qpH2mV85Na+km7ccjAgRV7wwPXBn5Pbzl4zlKNljXP3KTJyiIBYfVRKe/aGa8XN05aI32TyGMqED9viwTXv0hois3OYvROcFS8fqRND7UdabE4RdRrluM07c7/mQZeJPWzFn3R6t+7TrkzshM80Ocs1EYRz23JAkLWTd/O7cSEPm9mtIpHvXqFKrSy4yCZi3hO5aAZ7HDeFetEqAquCkBJOElV8vZ83lz0/HgEeDx7fPWO1dBEcmBGvnwbLJkeh67Digb/JERrQPTmpXX9u+3JYtVJm5ZldwSTWND+xuDbP8wwVF2hO9ZGJ5ZfBaOvS7Dy++9bHknyTCqsgWFHOg0NnBZiKm0asrMTrJeRSrtx9MKwRx88D2oR1vmUsFVFnM/tC+qmWHcx6KpzUMzD6hx6gvte/b2tsktOtxJdbJknG+3DzDQEm1Wx3J6jJx4zpkRHVOTMiIduHIsaLSZQGvcsV+CyzovJjFHMFXSiXiA2QPsWF38If/zelr8WsFjV8nrKdZZwnSoAReOUjOnyJEFD/npLgRBaLnzcwDyCwxwnRk4qJl7hdzudaUxcs0np28Eg9+Lr80bl95yM0Ob1hcsnkf/u/duYGSV616z/uPHMPug2JDeeJPW9Hr/om+jxMl9vuyfo1KOHi0EOc9o1b0TNWTHaWesF9+FKyUxpn8HDfzNcQ46zx9ZES7cM4/pmDg378BkBpYL0I2q97O7LX6YnMAoFOTmoF+P36Rt1SUqic3yAPvp8SoFzKZwn7JyVbrlZNajEUX4wT3U1iJKQA8Y/B/9WL6hGy1mTCqYUCNWg7tjWlrSl97rfzIeiXNx/WZr1dqUjOI1lJ5fMIyJSUEHStmKnw6bxPaWZLHg9Bj1Jfo/YDYUDa9hzqKTukep7yoV60Stuw7goUb9wqTVZ34wSUxXsS4Re7jehKwlgvv/8gkrN91CPco5vFYx+wlm/c5TrwqCjpVn8iIduHnHQdLlygWbyqTVdkpuAH9egRv+2CBvx86kNegutL293yS+jDKxIOqet3thlTLelWF29mLcgDAJ4Ysma4QDCuihAMVku4FTgKi66bbi2L1PnnF4IsGWZ3eqA0eS6+6Kxre82lZkqzX/ThXNm7asqMPZoul4wqLij2Ns8cnLBUa4TrPgMz59GrnnkPxFfzQxVsz0lcN/jW5RBf93GemBN7/L/79Q+B9uGF3JqywjEO/c9HNvu1/8z3HBrfV3jAdKmGwee8RnPLYZCx1kXkT/b1WQ/ycf0xxnHjZOXIsvBj1vYePRRoDbz0WhXPEgHXm8k05Kp9qX7KViWuaLhH8b8X+oDgtl3633Dn2WTRxCcpzk1dp36cqSTTEOefCJUQ/zF+fPkjpXkIN6v3VqR3r9aetC1GtxivWfOveI3LqHJbXViUbK+3vGoeOd49z3c+zk1dhwYa9pbJVpfvXeM8v2eKtGbvnsHvfIROjLxp0i4o5liREs/auj509k8s1xGGr4Ofyii6BGWb1846D6V8a/G/2hkDP1HLNZd3DQLRy53aO29yRvsohym/Ytv+Ip9Nsxs9qY70KPe/7Eg8ohF0FxbrqoLMPIiNaEqtAf1aCA6miMMpUC4jY23TUYXZbr7qzvui8gGoGOw6mdyJW42mNQ0fdVtGzXx6YtmonLv3vNC37OnRM4InUfI8eDrhKMdko373/SPiFGK55bWZo+/Y6r7ITUS+jUmUSzcHxyLhw5D8BufyGMPrEKSu247evzsQ5/wju5SXEyJa0Vik4Zfd4Z4LCyScaFIpEjobrXp+NQU9+G3jfQVB1yDkxbuFmjPOoWmpVUCtKmieaMTaEMbaMMbaSMTZS8P2tjLGfGGMLGGOTGGOtLd8VMcbmGf/G6GhPGFg9tBWpopQI1eV4+3KpU8dVzaVk+dHCYIaSaLC1dqiiWFkAaFZHHHpi/32c6F4Ss4cYiZZLZcMSsiOYcC7StCT7tUPBDpX9exUWsRbs8SuL6YSX8fv4hGVS9+wBj8nEZc9PL33tee8JDqfzlhhrKWOsq7DPUYkY4itf+jHjVXKSzq4I4nZ1xIuHjeiZ1WEDyiR4hz3CBVklsY5BN741Bze+Ncd1+20WG27f4WOeRrcsgY1oxlg2gGcBnAOgK4DLGWNdbZvNBZDPOT8OwAcAHrN8d5hz3sv4d37Q9hDhoxqU/5Vk1UU3D///ZsmV9VXB2hE5GfZuuq5+VT5ktDBV8DKeeraorbQ/+7kQrRzIyp8JZQA1dc2644tF5/FoYRHOVVAIULEPCzWrxExd6S0FKTP4quimexmcXwgSS0VtuOeTRVKTwa+XbsWdHy8sfW/VKn/Tp5qIvT1mLLEVUeGmJPLGtDWhV2gLA9E9YQ0D+njuBt+hM24SdzrCBMOO6xUVuUz9m0I0dZPhJxLy6Xw1D721wNhbM9Z5Gt2y6PBE9wWwknO+mnNeAOBdAMOtG3DOJ3POTbfLdAAtNByXECK+6xdu2Kstfk/VEy0bc+pmgMxbvyeQ3qxf55ebnM4zk1akfVa/emXPfVqTWXT0f177qFrJ2cMvw+odgvMueULdNssbORbvzZQvJmTHXJJzCg9SxU33e69k8lnQZNUgLNiwx3MbmdtNxSjwug3ekazQ98b0tVJhYr99dZZjFctnfeY4LNvqHRcbZNXg5PYNfP9WlXs+XYzz/qUmCyfLoYJoV2CtscB/em++79CZzZqKUzmNezLj4eQAeVTm3p0qAv7u9VmOvw1aRTDJvDMj/Iq5MugwopsDsP41G4zPnLgGgDUjpQpjbBZjbDpj7AKnHzHGrjO2m7V9Oy2jOTF1hdjLed6/pmqL31P1JMrqOh7fqq7jd4ePFeG3r+qNJw1qv4pkDRvW9Dais/3UxQ1A/RrebRJhejhEVTXX75JcOvdYu398wjLldpmYBu7nmpbl3Dz6hSJ3kIBFG+NLNJOZkMkk0al4yL204EUGhtVztvtgAf5qlLX/QuE6boo4ltXt8nt5AhPszFNCpyyYHdGYoipX54SoTLUfgjihfuMydg3p3kRqH9Y4Xmuv+pVDGBpQFlfudosmoSKiH35cE12lYTciTSxkjP0aQD6Axy0ft+ac5wO4AsDTjLF2ot9yzp/nnOdzzvMbNmwYQWszk2+Xh68cYu9Lz+/ZTMt+vQzQNQHK84psuYk/eYeZuNm7fivv6Tahvbyfdarm+trv5GXGvSRo8Nx1crqxonLU1tOmWh2wVb1qgv2lX4dHx6sns7l5lOwTHx3xty8paBnLIGPjyNyy78+S9/B4yYuJnjtrO699fRbeN0K1Vm93VmGw88d35kpv+5OGFbhJLiFp3glw7iddNElNIhlqa6Xcg0H+hH2SiY7KGI36ZX5L980sjVf9O9wcXyI1j4pCQWFx4BUWHUb0RgDWq9/C+CwFxtggAHcBOJ9zXurC45xvNP5fDeAbAL01tKlc4zbYx9HP6YoJC7PtMlXIVGNsRYNKlLqXJj9tCsf7acqbMYEVPTeAWorXYOzmGTm9U9kE2hwYROEc//5Gr3yhPQHJK/lOBt0VAYOUU7cawyrFR/ysqlhjXa1FPN6dKW+8qxT/8Ko2K4ObF1b0fKiwSVNCpM6+RxRXPXutnOfPz20YpoGeYkQHONCjAVbNZCh2GAFND7ifpnslOptwzpE3cqwWr/S1r83SoigSBde8NhNd750QaB86jOiZADowxtowxioBuAxAisoGY6w3gP+ixIDeZvm8LmOssvG6AYABAKITDpQkacsdcRhqVuznI8jgHRUyXclqgcyd218m+rPfmKZmGOlIsvOSXPR7edxCNoJUkvTq1930h1O8McbrKJb1+j48KeW9TokkXXwtE3fp0GyrEoLOVXvRaZoikQCZNFyfMY/72etW8aqyKctGj9AaFUTa2rpyD4KiqgtvneSIjMqlElrjgHhVTQfjjWrIQbqUvJFj8aFDYaSSnbv/3pzQpyXLK84Pi4o5vlqyFbe8N0/thzExZUXwviiwEc05LwRwE4AJAJYAeJ9zvpgxdj9jzFTbeBxADQD/s0nZdQEwizE2H8BkAKM554kzooNUWNvnou7gl7ELnGMHT+/YSPvx7JjxgY2M8Iste/UkbkQ9Wcm1legWXefGNaso7fNIQCk+PyyXSIxy4hqJOPM5gtCNIKE1Xka/m+qJddKhW+NV5farXilH+HmCJeQBOE/aKuWUDQVJcxokAbeFLFXdfDtuBS/s4TKiqq4mYU9Owry3Ve64Z75OT+iWRWQIX/zvYJr4usZ460qonzLrosmAecm84tnvNSqf2sdy1UuuY4UuZX8ZICesJSaac/4F57wj57wd5/wh47N7OedjjNeDOOeN7VJ2nPMfOOc9OOc9jf9f0tEe3QQZUo4b9aW2dpiIJJhMujevpf14duxqD3vCihULmc5NUs/V9W+kZzn/aXCHUNtwrDC4weI1yXv7R2cFjEkS3kvdeq3tG9XQsp8oNF5VV32cymQnBZF9nJvNfMdb+golkjjAxj2HXZ0FYdKtWXof+sSXyx23Dxoru3bnISzauFeoAGOXEOz70KS0bUy+XaYvH0YcoiJnUoVZlRMIVuxMpBQVdHKwY7+e/tEauy8qQuJn1dJcMfM7ylgn1zKIJC2DMNNjlVG2IE+YUMXCBLBHIQYRcC+DGkUBgAY1UisLBnVcReH5Eh1hoa2Ihsi7mp2l9ohUzVWTk1uxTd2LPKxH05T37ynEkiaByoodsxXrraK7cqhXEpwV2evmlXgXNaI/hzGWEpKl8jj6qRZZu5p3ouuA0V/jD2/r0XFVpUfz2krbe1Vd8zqd63YdwrnPTMV9n6WXWFfpG93GBVVEzwLnHAeOFqKgsNg1GdJPO8IcA7y6iaQsHi21lCD307WtEiTmPhqwWqhoQunGJBelEBXMBPI5Hh75xZv2xr5yRka0BGFfo0fH60tYeH3a2kBGigyLNSeymec36kchqNSc6OGVUSqxxuXJJn5YGWuTAgu7gqaK1FTeyLHIGzm29P2gLunhRYcCqBFYT3nd6v5UR2T27cWaHXLetijzBXq3quPrd1nMf8iaqqcKAL40YkCdCBI+pwPVo5tl4/3u0HR8fCRIxhIZRk5Ytw1DH/jGt+ag+98m4LYP5qP3A3pWWJvWLgmXUznnql2mdXPRcfxMBMPCvG5rBBORmWvcDUpRTsT/jJWxqAzNIkkpUC/M1YxnvnZedQdKKod6nZewISNagrDLO78boOBEHJzRKdUwCrp8F+bZdSvZnCNhRHPOUUfCc2aSJbHPf6QUaUlu/OmJbesB8KePasYJimQL33EJL1EhqCqCHaGCi8PlkR3Io3SS1KyS6xl+IhpMsxjznazs5wp4yRpaQ0Tmrd8TqCBPFIRZnnq3z337mZzLsnDjXhzRlGS4ee8RrN91COf+U6UqqNrf5uUs8Vt51sTNPrj/M7UUrzWGcs0XC9MnmoFW8CS3mydRsMkN3Q42L4qKObZoUN8JAhnREsg+ZDslSyHb4RwYv2iLFimmKNA9qTAH9jAMDtOIFu37qv6tPX9//+c/OYZo9GxZx1ebrINuknO4htrCRlRwywUI8jdbDXqd92FhUbFUoRwT2WE8yssrox4gak8WY57lu53ooVhSXgar8skFz36P2z9c6LK1N3PW7nH9/tSOqXUHVMOjvIy0IPeAjJZ91KjoectwymOTQ42jtubwhDG1eMuhgiYAvPy9txZ8rSplScqDnvwOgDihztr2HYoKJbKssiW5qvbV2yztCpJw2aJuVeltdYTMBUlSJyNaArfsaSv7PDJT3ZZUbnhzdpqMll/clpCXbfGv5FC2/8C7SOFg6fK+fpNj5EfOA3B3S+xj9cpiQ3m/yzU9r2cz1LD9rjQ0hXPPpIik8+DnS5S2t8f2HysqFnbCKiEOdvtkrsVQdJIG8+pURQlc7e8aJ2yXk6FezUGdw46fcA6/S68ySTaiXR84WugujxUxIkMnyEApUpex0qqe/IDtD//92k6fnujR4+Se3S5Na/raf9Jwu/dVKnD6wakMvSz1qlfy3gipd5FuZSKT1FXSYARxlrSsW006JElHbsyA0V8DKHFwua1eC48f+OgVgBemrJbazqvyVJBYUBVMz7lIDmnkRwsC79+8ybZpmg2bA2RYXtkrX5oh/Nx6vINHna+NiiFkFiRYveMgLvlPMOmkuClQXOI//YlvUt4P/ccUdBUkpqj0eX87r1vK+3OPK/OOr3WQ2ft0nrvQ//jF4gzyloJqiE7UqCJnRM/yMZEKEuPuWcjGwaD773dyfZzq8fwguj/C1CWvnKOWDGxHl86zGH8n+IUp3h5QN5KSbCdLz/ucV76+TKA334oood1v6IZIsShv5NhSOdq4aKngWQaAaat3CnMEwub2Dxfg3GfkQ4sAMqKlkJVa8oorlIndUZ0FuSFK0AlSac7EbflKhhqVUw2QSkYsalj+gikrdgiNB9lwACfPu8iA+HDORsfvghCW50Ene2we3hXbDqBJrSpp19uOiuSdjDpHgYfnycnDriJPJ9snuK1kOBGkqIXnPe3jvnTTQdZduhwQr5Z9uzw8DeROjdW8sXYNXy8jPUhf8JUmtQMn4k7iVGXfkWNp+v5e6Eh2i1Jlp7iY+04SdkqSFMWRO62+Wglyd/zn27KqsWd0Ts2jctM7N5GVz7Su3Hk5Mr3wE9ZGRrRGhj/7vev32RIuOC8vmhWvB/urBM3Az/z7N6Wv7afhUMie6JKdCz6yfFbTxbMYZmazbGa41aMpE8utwnGWuFa7fGFQ5q3fkxbfZ/eWuJ1fznmKV8YaL5vXoLrwN23qiz832X+0UNjZijyKTk2TXZ3ws5S8aNNeFBQWK5ehB2Q80eo8NNY5NODjELxFovLfb8/wXyLd6xqorgZPsKmLiDzZU1fswGfzN6ntOAaWbxWHKoaZmBiE40Z9iSqKMqLWZyJH0QA3CXe1IZW563dj+mp/oYAql81t9VUHExaVPSf2/nKahOKTtBFted3l3vFSv3Hcl9FOmfaZkBFt4dqT26TFYOrATDiU8UqJOi+nwdTLa33Pp+m6o3FhlV+yD/TmOQ/TWP3JpjAx4uUfcev780vfu12bLk3ltTLLlDzk/hYzGcwMA3HCWk48aHEHO9bTrtsx9dw3q9I++2TephQPmFfirvVb67Nw0XPuk1ZVVFQWZI3oP/9vvuv3IuWTTXsOo/M944S6wW40q+1dXVOm2fYuKIw+0Q1RtTZRKNw/BfGb1uTuDbtLDJ/VHrrFqrf8pCWpzolqldKNuj+9Pw9/fGeu8v43JWTFKaj8Z5jIqCpZSTGyFPpyK1v3uYcuWmU903+rJhhwuEDsDZV6dh0+95uEfffHi3z9Dih57o4Wih1kMrHXg7s2ljqOrnwOq/1x+QvTpX9HRrSF+jUqh1La9M3pJeEP5/3LO9ZGlI1dxHlKtmp9IxEhoc4CT+wGiJNOtM7EvHYNU0MG7EVpqgsGQpM61cTeWXvHlJPFcFK7+gDUPTm/8Cg9O8tiWHwyL9XDNeq8rkrHsmP9O+yGpCgJTwcbd5cZC26Z+W/OWJfSuVlfOxn8B456tznos6NrsvG3MemG8jfLtqOYA69NU/O+btp7xDOOXWYwtVfytN9vYXLjm7OFnx8qKEozMJ+cmF5F0Frs4RNJL/kCRVkvezGR24d0Vvq9G166uFHhVubcDTdjUhdO97jTymwHS7iYX03hIHrvqmovQYoM6S5A9WPAMfh7oxS9vb+UUXiRzdOY8XNqG/2s4AHAqu0HfK3AkBFtJwRn6FNfOZeMtSPyRNi1Qv1mbCcFexzeDsN7ZF/m152Y5+bAcHt4rIZbuvRQ2e8KizmenbzK9qk7QYX+s1jwW3bRRudls7Ckp6ynu3kd56STldsOpHiqZ0h06t94Fb9QwOnc6lo0Ob5V3bTPgqw0eE16ZNqtqguuM1503CLnQiwXPfeD5++t99VByfjIN6evK/Vay2DvK0T5CqZaRI9RE6T3C+jTUA+KjDFhXzmMSo3IKUG/8z3pS/k1KuegiWWFxk+iLwBMXekdkz/XQwVGliClrEUFV4D4pFTNWOyNCs+XyQ6/ksG+flWiWe4HMqIjQjaBQ7TVjgMFwofAy3sZJn5u8EMFheh497i04H2z/LY1EUE3nMM1lk612ETeyLH4bvn20uUq+3duV7uSxc1jGt1+KebAfS6C/q9K6JS6IbN6EhRZpQsA2HfYOyTq/Vnenh8/gwrnHJ8vML2yekallgJ5NbtnRQVRaIGVES//6HvfTvjVmFblSGGR64QLKEkiNuFcfiXl5EcnS7ejrm1lSqTeZDoE9h8pjL0sMaAeKifj0TSfxYNHC7F0y75Q4uNFKIdzaDj9z0t4RS987gfsF2gj67r8MqtITka0LNec3AbrHFSPgvDdivASg3Vhz3WQLXRERnRESC8HCTabunK7+AsJ3LxETh2rTOZs/oNfKbflk7mbhBI89YxBqa6kXqYfvDogN0PAfprM8/b+rA2+ql2pSscFYZRixayosJ5T3THeMvPVXYfkOkirVun4RVtw09tzUVTMtYVz6F5+9WrWChfNe6/M9l0HC4TOgDBKTIvYc+iYq0pN5yY1UzyNY+ZtxLcr9K1KmFx5Ympir12Vxo5fD5dOVA05mbvyiOFAeHzCMgx5ekpgvWRZhJVFJXh0/FJ8GnJoUg9BkakjAkdL1MhKRL409Wec+vhkvPbDGi3HXbBBn9qYLH4nrccKU3932uNyE+sKa0Rv2XsExcUc4xdtieTh3y05aDeqla7nWFjsvzabaInLZMNu8YCkkpmqwnYHXem7PylJXqhdVb68tiqc+9c+tZ99kXqAKm5qIE6IJiAA0LdNPV9tcLoeQZC1Ca2TyjiMjB9/9r7HF23cm1IA6ca3SmIVJy3Zisa1vJP4TA666D4nSWJsxCslXuoLezcXfn/8AxNx+fPpCTdm35aEpLhNlntp094jWuTN7KgqPDj1s7Lc+fFC3PPJokD3iupqgcxz/JEh5/mqJoNLloMF8rKRRwuLUGict38LkpxN1u86hLyRY0ORbBzvEqIky5j5m6T8aE6rAapVP+cHLP9tohLKqgune72Wx5h7tLAoZSXBq3ieSbk0or9dvh1XvjQjZUZSUFiM/UeOYfrqnbjkPz/gxEcmoe2dX+CGN2fjzo8X4mhhkVRCkgyimdDMn+XipUTlLif+tBX2sUBH5UEnwtIkXuQwGzZjvMOczARJDLF2XoVFxbj3U++M5W81xuWaTF8tNvx+9Ln8f8JD6qsJXsieZq8qckHxmqR8sdB7YHMS3R+/aAv6t6vvmoxq5YUpq0tXW+ys2i5XDVUWp4mWDD/+vAub9hwWDsSmxJco0ejl79cgb+RYnGRU/ZLh/BDChJYK+sQGNfQXmRAZmM9/F14o2tsz1uGN6Wtx/RuzfO/DLffCnPz0e/gr9H3oKxwtLJJaIXl0/FLXCWJYvPL9GultjxVxXOAhPQuUxVk/8Ln+lTt7Iqofbn5nLq4MIRTLCXOC5EQUCaR+carH4TU0fTJvk686GuXOiC4q5hjx8o+YsmIH2tzxBY4VFeOcf0xBx7vHoceoL3HZ89OFGbqd7h4fOD7VZJrA2HlswlKp34r0Ieeu24NCmxWtMhtX5bHxy0LZryiJyiTsh7I4gCt6jyXcoKCoOCWEw2mXuQoVp2SXn65y6URVk8HixiovGAciFRwrA1wMwo/mbsSO/UelQ3k27D7s2IF7VcsTxVgecnn2b3lvXkrMvSpOhvApjzkvbfqZ/Ea1zJuTpW+IM/MmRHr/D3+x1HcilCxBCq/89QPnSrUnjf4ay7bsx9Z9R7Ft/1F0unu8dJjRGU9847tNSeLsp78rfR2WIlFQZFYiqipqaAehoLAYH8zeYMkTSS6ccylPvqysnhUtPQxjbAhjbBljbCVjbKTg+8qMsfeM72cwxvIs391hfL6MMXZ20LbYlxs73DVOycDwEeKawpa9R3DFC+llpqtXUl++d0PF02n3Tplaqn/x0LDVzaPj5SYSYTBn3R7f4vI/WMJb+j40yWXLMpw8jybWDlFGM9PEKQHye0H2eFSxqmGwbd8R37FtXst2Mnitxnwyb6N0bHtJJUTx39KmgXup8VFj0j1jTuoEQIk3WaPdGCpReLOenawuGedULc7ML3GKyc1/8Ctc8Oz32CMZuhclXy3Z6lo51GpEAsCNb4mlBu1sCyEkLAxWK6z49LzfuYR40unWzJ8Oth863j0Of/nffNz09txS2d0kIMrt2LLviFRIrJdzRUTg7pYxlg3gWQDnAOgK4HLGmF249hoAuznn7QE8BeBR47ddAVwGoBuAIQCeM/bnm6C6hkERZWpXrZSdVuzDiZsHthd+vuNAasesYnh1vHtcynszacBNBUCktZiputRAOMt0gPPk1kvr02oIPf2V/LU8+6nvhJ8/KKgqt1iy4lMSOecfU6RCQ/JGjsVlJ7RM+Uw2li0IF/Zujiq5wa1VLzm+D+foKSQgSxjhD260ru8+iQiKSthQ6aTN4b4zk8Z2HnQ2HOet34Ne90+UPqYqvVrW8f3b83o2ky6cszYEhYY4Gfj3bwP9XrU0fFzMEhQqigK3rvo3r0QXhgKUlIa30/+Rr0NTyWFBd8wY6w9gFOf8bOP9HQDAOX/Ess0EY5tpjLEcAFsANAQw0rqtdTu3Y+bn5/NZs8TxYUmO1UkSPVvWKa2W57lti9r41YmtXZcECUI3z1/ZB9e9IecRi4PKOVmRybsR0TDnnsE4/oHwjGAiMxh5TmeMHleycvrABd2xbudBvPbD2kiVlYj4efaK43Fy+waoU73yHM6L+4i20WFEXwxgCOf8WuP9lQD6cc5vsmyzyNhmg/F+FYB+AEYBmM45f9P4/CUA4zjnHwiOcx2A6wAgu1bDPi1ufCVQuwmCIAiCIAjCjc2v3YKjm1cI13H0BuqGCOf8eQDPA0DP3sfzD289TbjdoCeDLdsQBEHIkJvNfOmEE8mlf9v6wsRwomJzXIvaWLp5P3miKyC3Du6IP/1zu6P2oQ4jeiMAa1BiC+Mz0TYbjHCO2gB2Sv42jdzsLLRvVCNIm0NjcNfGacHp1StnSye13TKog1KMrB9+fWIrPHhBD9fQlyX3D0FVm4RXmzvGxlY+lPBmzehhAEr0LivnZOP7lTvwqxfTk1wzhdUPD0XbO7/w3O6yE1qmaXerPHN+uHtYFzzx5TJpI7putVzsFmT9d2tWyzN23byuJjsOHHUtdsSY/ypprepVC63UexKO58bqh4ciK4vhwme/x1yHULc1o4fhwc9/wosh6AnL0LVpLen8GjuDujTC10u3aSsUVJ6xP3O/f2t2LIVD4mTN6GHS4bH1qlfCLskKf2Fz97AuwhyhLCZXiMuKeR/83+G9jglkOvK4ZwLowBhrwxirhJJEwTG2bcYAGGG8vhjA17wkjmQMgMsM9Y42ADoACBSFXrdaeAU7ZLj3XHtOJXDwaBHaNKgu9XsnA9qe/fqbAXnSbVow6qyU99ef2g4A0MSlYITdgAb0lS+Ngz8P7hjp8Uae09n1+1xLwYbL+7Z02TIV+7U0sSbXmbJpfouwJIHP/3iyVCLrmtHD8KVt0lopO0t7JUA73y7fjiPHgnulrurf2vX7Uzo0SPvM6zmsrCCvaCdqgzbs4/XNk38GssysO4dbZ/odZwIAmnmUHh9/yynSx1TFrwENACu3HaiwBvSrvzkh0O9ldOWTwOmdGsZyXLfe1j4hCZt+beqnffbx709CNc0KaSaBjWjOeSGAmwBMALAEwPuc88WMsfsZY+cbm70EoD5jbCWAW1GWULgYwPsAfgIwHsAfOOeB3EdTbx+Y8t7J6HBCsRhVGi3qVsVfh3RK+zxHNi3aAbtNMLRHU+nf1qqSOrFoWa8kI/7Na/sGapMqQ3s0ifR4Vs7p0QTVK/sTfunStCwz+8c7z0z5zul+aVTTXeWgikXPc9T53aTbYr+WJpfkt0j7zG953CTQrVktMElD2O4BiWLJtXfLOtJKBye2rQenYcbLiHzikp5pn1V2UQVpUquKlB5qEvj5kaGhH+PaU9oo/8apwmD9GiWODLtmv8kHN/THmtHD0LlJdDJjstSonIM1Loobz1+ZmjP1xc3hTQTi4PROjaS3nfinU0NsSbh4qf3o5Os/n4bOTWrihtPalRZMSwLVBON8zxZ1pMpEDOoSk0405/wLznlHznk7zvlDxmf3cs7HGK+PcM4v4Zy355z35Zyvtvz2IeN3nTjn45yOIUv1yjm46PiSsrVLHxiCWlVyserhofh+5EBMuOVURwm5ufcMxq/6tQp6eDDGcONp7dI+v6yv3L5b1RNLPmXbRmwdoupOhR6uPVl94AlyPKBkKTVMshjzbVzk1S9bRcixGaZOq/lOVZNEeBXcMHnxqnzH7/q0ziyvs9c9JmtA++WEPOfCP4C7gTesR1O0b1xTuFoj4jiXDvxwgbvBLyov7jSRAoB3rzsRRwIohrzzuxOFn3/yhwGOv1Hx9loJ+xoDQK2q6iuT2x20j01HSKHAnXtpfgvk+zwPsjT38IC78e1tpzt+9+GN/XFWtzIHx8y7BklXeLU7FTKVcf9XNmnokCFydnHTtmENjL/lVM9V1ySQlcWkiq1NWaE+CclcV5ULT17aC2tGDyv19mVnMTSvUxWdmtTErWd1wprRw/Dhjf2x7MEhuLhPCwzu2hh1q1dCi7p6NEtFg8NZkpVwags6/dvO7pRWJatr0/C8Hd2b1w5lv26DQFYWQ5/W7oZNEIIs71t/mpvNcNvZ6SsNdvzMaL04sV36MhUANK3tHJbjxpS/nhGkOUJkT/OVHmEMQdnvoRV9fq/mrt8zxvCvK3oLvzutY0Ms3rRXOub6d6e0xS6HAhwt6vo3jETUCFBopm61XPRvVx83np7uBHDTJ374oh5YM3oYXnCZ5NlZcv8QP010pXOTdONnk0fRHBXMfl1kXz52cfqKgS5qVs5B6/rVMPFW/x7SKi5OF3MCvmb0MKwZPQwNa1aWCt3r16YeGrmEBIZFycqOPO9f399zm0pGCJRdc14HDT1WJWW46Yz2ePvafhpaI8eQbu6rxksf0P/86qJBdfH59hqahnRr4muiWi6NaBn6tK6HyjnZeOKSnkqdv19EHiURy7buT/ssRzCLypJcS/7xLmdPgVObegYQ9HejjkO8+pOXlgxAYVZbCuCIBrOd/OtPbev5G/vKgR0vI0+EU8WxzXvlvd5WWjqsegRBNm7eek6rS3p0dXJy+/RYYzvnHtcsZWn3zqElHpfzejbD8i3pz6kTboOorDc7CibcUvK3/vubVY7fi4xfM0xKpWRuWH+3NXekam6253PoB1EhKjeCxKgDwML7zsa3t50RKKZTdeWSS/SWzxiTzF/m6zc83ThBweNfo3IOqhn3mtuKSdsG1THv3sF4+MIegdtnZ5hC6KUTfzm7k5Qn9dzjxMf6o8MKvBMdG7sLN7hNyqzoWN1XxSls06sIV+2quWjhY0yssEZ01ATpy/Pz6qUZcrI0qulsvFdy6NxlkiC/+cvpym25uE963C6A0gSteZLFX/zgdf7cro/du2qGdDiF3ngRxsDuxKkd40k08cJ6Tv10XEFpXEvOO1TfUsHvtwPa4MLezVG1Ura2UIQizZleQVrl5VXs1EQcwlItNxql1LrVctHDZZVs6Zb96GzJX7iqf+tQQiw+mZcqIOVloB7XIpyVPRVUb1eZ29LsU/80uCOev7IPzuwsH3cchAKf4Urv39DfMS+HMYY61SpJO6ec+Oymk9M+q1stupLYTmPL2R6eZZPfDsjDU7/siVsG6UnEP7NLNPeEFb99c25O6u/cQqCskBEdEfZYWhUa1aws7AQ/vNF7mSos8iTVRqzUrpqLNaOHpXlmTKP9jwM7aGmbCMbcDZZqkp4xcw9rRg/DX4d0EnqHVz881NWYsbbj6pPypI7rRE4WEyrCmLz+22DJo8/96vhAv5ehUHMioMw59RPek5Odhad+2cv4vfLPhYhWJIKEanlVUAwjhCcnaDa2JBzAwo3uMmMXWMJ0sowwPhk++v1J0u2wJ+j9oo93aFDcOLXByUsuU4StgZFo2aR2FZzVrQkul8z7CYooJt2N1HA8f+PwpYLEbTuPXNQDPQQTJl2XX8aRFjQk8uXv1+DC3i0CTyZMzDa3a6huL0TNSe1SVydb15drMxnRdkLo7wYqzNBrCWIam9hiXhuUesfi75xVMDty+2y5df0ST6TdIP3HZb20Ht/NwHDroKwDUHpSV1mHnpvNcO5xTZU6IHMg8kthMQ9s0LmF0ZzYVhyHHRRr4tKq7Qddt7Uq2wyW8Gx01JgY5HRqdRlGY+ZvSvusZT3/cdJey6wyBq+sHKdJdYcwI928NMJbpsxqXuVKPhgntq2H41vJGx/2pLszO6eHsZghPPPuHSy9X0B/jLwXTn2ilCfa9gxE5XV0WkGdc0/6uT5wtBB7D5fps7dv6K++xLnHNfPcJqpJhBsX9hZP6OKax5lhQX0FsnNe5EY0OTdp5/PeICPawuFjRaFoId8/vETC7JnLxYlKVi4QJDxlMZYiu7S7NEkpQ7SsbNi9gOZb+4N+fk/vjkuWNTvdDbX9R53jpY4UiBPI7Ib3sSKOA8Z+ZLPbTW4f4p7hbI2r7W8zbEd99pPSsexkpUwSUg2isHTXrfH4bmExo87rmjIpkTFeZYzQoM+5ru79nnO7pH3Wq2WJQTfIh1HitaIi482yJ+Od0z06aUonTdlsQeKxKNbUGi97jmQs6kXHe3sZrdg1+x/6Ir2wg0kdxaX8N6+JLnnMDdX+Cyh5NqPQBK7mMFGsV118ruevL1u9OKtbE1/hdEES01VyBQDgP7/u472RA7pXPYJM6AGgt9GX2Vslk2z5J8naDnYvt1+nUtuG1aVWYOyQEW3hn5NWhCJGb6p+9GxRx3Nb0TNgf+jNcID2Dd09bqoPb1TY/0bz70k3rvV1CPZZ5prRw0oTGgH3uMZ5G/ZIH2dt6VKvXNtNOSWRIoIVqyKI7hhL62m2e3mCXoMLeqVPhIZ0a5LiMfXyOFi/td4jTkvwMoaiyEhQmTDIDsR/Oct9ILAvIQIlKhM/3nkm/ikx6bbSTEKlReZy2r2TxyIudVxTsBonUi16VhBqZPWidzHCYrxCOlTvcLu3b5tAzvLmMzvgpjPaK+/fXJWLm8IEl7MvUjR0rI/qqu0HfOUhiO4/K26Thy6K4VlO/ZDMs+tn8uPGv3/l36BvVrsK6hoTmyybpXmNhIzu7DW7pY5jFo8z8R0TbQn1EdX6cIKMaI04aayaHHMQ6bcyQEI1wKS2x6D/wPDu0vsKm5UPneP4XXUj6zzMJSfRMrb1eIeP6SkRrTqTtXu1nLBKjP33u9XOG/rAWs52xwG9ovlDezRNC9OxG0luXh7GWIpRZzVe1zkUj9i811vWTOSxbVI73dhyaprsvepnibB9oxpoVKuKshrDpr1HPNvl5xG7Y2i6t9zEq6iQH0TVRYMoHHh551XNDrs6hMgou/LE1iWKCopEHT/tdP38eOSiYMpfz1Duq62n9JDDqqIX7Rv5W+r3Q69WddJWG2Vxumwix4LM2OMUOiODNRTQ3sfLrDJ72Tcm1l3PvGuQXOMc91Wys9+fLq9mQka0BF5yLyYiD4oVmZtWpbKSFyKZIh0xd6JSxF5YEyvtyVTm5CKs4cPJiyCreOLkdBSNd6Yahu5BSGecb5TsP1JYGuJiYj9vGxS0fK2/dfK6eA2U953fTWis/OJ49wQxKye3l1M98bOy5adAiInnPe3jIWvrEiP98tXByimLOF6QHBVkVc3LLl2zwz3Uy85ZNqWDQ5om4CLCVryoHzAnI2pa1quGY4X++1a//XKUUpSVc7KxdIu/8u4qcophVxm8x5LwvmLrgZTvmkkk/B4nWa/C2pcH1eT2EwpCRrQE954rV5bZy+PhtSSkC/OeaiKQrHry0l6B92+WHNflhcox1nrCcsLMvWew0LhICWNwydpWiYc7p3vJuclrUB1/O89ZNaM8Yi+Xu3DUWZi5ZlfadvZx7LCL0fvm9LUp7z9fsLn0dYMa4vtvmINWqskgB4Nsm0OlOhGyai79FAtDAOHqZjsZ2W7Fgdy8o1E5Tnu3quP7t152074jx9w38MArETPIOfLrmfZbgClTGX+Lc5nyMzQ6psLALhwAALsP+bsnRYnFSx8Ygp0H5fs2K0Eeb2suy7TVO5V+m1e/Gi4NofCNF3cO7aIsaEBGtATdm8vFNHllqbt1iJf3bYn3rnMPB5HFjO0RHa9vm+C6qbpljsuS2fSPyNed2tZRLWPbvrKOpcBH3Oe0VTtxwKFqXW52Fn4zwDvuK8ncd77c5NHEXi63ZpVcoQGhMilZsS3Vg2GdiFo1ga04GdcmohjZRfedLWyXk9Fpzfh3w0+hjSBymJ7hHILva1TOkarCKdxfCM+syOit6VLq3Iu2HvJaspUnnQhzHlE519+9MPX2gVLbLdksLhqUzGAOZzo3cR6jg1TxlMFv6IWJvRqxE9atVIrnVMnNlpYFDCq5mkKAm6hRTflwNh1x4P+7oUQuuGPjmhjuUc3WDhnREsgu5chWJRTxyEXHoZ8mObGwnUO6B07T2A/Dq2WWWxft+4kvl3n+/uELezj+tZOWbE37TPVvSICErCMrbQasCmb1OxH2JBMVhlpiY3XehzUq52CPQ2luEbIdd5CsflVkCuuIWlPMue+qesu2+lt2luXec7tiQPtg/aJIgs7Kx3NTi6cMV1QF8r4V/N8DOhWKdKE7+VHVeSRTTdHK0WPhJsbeLVDYMZFR2NloCWd7zdD1FxnJVqdYR0GJex3YNeqDdF+1qvqfvPwoWMF0QocUnkoVTDtkREsQhrfFilMlv6QyZ11q1myzOsGWDsM8uyr6ryIq5WRhk8+y2k6keniTa0W/YYRSyBassNLJ6ORFpVbNkBc/hHm2DorCSgIeMMpJ0pFjRb6KSRRz7juB6IgPA8XrnFjj/397chu8da2eFTpZqiiG03iduyC3gKiYU9y0rKvPiG5auwr6ta2PBaPO0rZPO2Gry7jZB6pydSe1K5kwivKOditM8v3iJ/zMyhmdyibyUSXJeq08hg0Z0Qng9x7yZiqc3qmhZ9WyoNiXWXKCuBZh0YkOtBcxboVPwjhPE39K907bSU1uUV+KUineo4MhCjrBKx86B6seHlr6fqwlhtkkSLymtV9W8RzLoBKmJJvAEvYE3MqPP3t7b0QDWzH3X4petXocAPy6X2vX73Uncal6LlWxJxqm4XFqhxi/FyVPuhVCsmMNW9EdcgcAf7+kJ7697XQ8e8XxmHaHXLiIF5sNB4VKc1VX7702FxU404VqaJY5Ce4lyAHwSujvIFAQMccK2fMbdOXMbz/ihFd+yxOX9BTKhEYJGdEShD2haqsog5Xnspx25YnuA5QOqtji9IKenyhmrKIj9LBl/wqX0Nz2KWj3zgNqCRwtXQqNOPH10m0p738zIE95H2GRk53l2ZHqMmmOadayFQ3OTremjOY7kP6sxI3wz+E85e9UeRyLfRjRMklOb/+uX2yhDJsV1GIA75hYr9NZo0oOnvplT9wpkBJUmYQ1qF42sQvap4qeheqVc9C6fnXUrpaLpgI5SJOaIXvPdSsfJTH+28+feHqn9HCuBy4okbn1ez+s3KamXCNS1gnCpfnuyYWndWyo3XBXJVk9fEIJcom+utU5NtQv57sEvturjYXBTpuWsFMFqaSzentqzO87gti8l6b+rLRP1b5Px3JtjkcncoVLOVo3VRITv/GyTizZ7D+O1joWBFwAkSJX8SD2QhxR6/56IWpOQRFPSW5SabG9iqBcG7yPcFK7BsrFZnTx3YodaZ+5JV56SZt60bJuNVzYu4VQ5SM3J/VcualQDO0RdmVJud7NjyGl8pyo9rFWg1RkcAW1yWX1jL24yNJ3dBbEPPtZ1VIdm+3nYr+ics0lffQqanTxiP0OKmmng0DDEGOsHmNsImNshfF/2tPDGOvFGJvGGFvMGFvAGPul5btXGWM/M8bmGf96BWlPWASZ6bRvpD8BwC384yNbkkwYmP2dKQnWSlOiSdQGhz0GViRbtkRRrzOOGgUdAtxjy12K4JicLFhGDGI4BMmmtg4kefXdVReU9y24/ZxuyaOFYkWHuL0iXjgNxEcsoU1JM/yTgFsFPz+rSVZES/cm9tA5NxWKE9vpSUwHxCEwSam/crmLU8ALkTrV4xcfF6Q50gWzvLCGHp5pkZyUlcYVCRuYlyzXo1+6zJCTa1Az9W9RveR1NE0oTBoFEGuIiqC+nJEAJnHOOwCYZLy3cwjAVZzzbgCGAHiaMVbH8v1tnPNexr95AdsTCkkbVOIeqJN2PnQhKiqh+pde1d87nMbaMemImfUjayaDmTAlauOpHeSKjYjwKty5/EFnw14UdqC7DLoIuwa2U65SnIaGaDlXFutAHXb30jOC66Ub17hqj2vu9fzp0ttvXU/fpFJmhUonKrecahltr5j4cySrYYZRnRMAzu7mrCAFyMlpzrlnsFBStTT0xeMEP3JRDwDpIY2qoTNmTPcvPcIwkkLjWsGvadC15OEATjdevwbgGwC3WzfgnC+3vN7EGNsGoCGAPQGPXWFxy8CPwry1P+yVcvSEc4TZdhkdaFESCGPM0TISrfQH0bP1S1BPmBOjzitRERENQh0a1wAWhnJYV7WDnzaVrQyYxr3IU9OgRmXsUIxPd6NRzVSPiGyxFTespdx1IJXg6/CQWcOKKmVn4ViRnHay16qC6PHp0bxO6et2Datj1faSuMuzFKoSqhh5Op5JP5UnZdH1/OpMyBR5bPtr9HTHRRAH0KO/OA6/eXWmxtaUYPZjWQ4Ppyi0w069gN5wxhjm/+2stPPjx8mzZvSwQG2Jkgm3nOq7FLxJ0OlmY865mX6/BYBrL8gY6wugEoBVlo8fMsI8nmKMOU4LGGPXMcZmMcZmbd++PWCzyy/dJUtlBsH+WH02f5OW/R4sSJdDsxJEn1Q01stoeBa5jJ5RagC74WWM+e0kLu/r7E2QLdoj8jpabT2VogEAMGvtbu+NAMy6e5DSfgH3ogdFthtIh+Hz9C97Bd6HFZnbUWabXyskJ3s9A17Jmq9c3bc0Ec2e6OvGRIVcEz9x23bOcPHyexfzcD9HSZSxExmbbkpHqb/1czz138iia3WoVlgVh42/fdZasbqOjjoKbsbwCiOkL6qKykmiTrVKUiXI3fA0ohljXzHGFgn+Dbdux0v8/o63K2OsKYA3APyGc266Be8A0BnACQDqwebFtu3/ec55Puc8v2FD/8uW5Z3hvcQZ7X+/pCee+9XxWo6h+jC7qYlYmSpI6LFy//Duagf2IKgRLEq4kykhHKZXS8Tmvf6STc3OW3Se2gvklER4/ak3BpB3NJt1mkSREbn9BR/JZe/1MJCxcWT+QhWPplfJa1HomfV+alW/GhbedzYAYKiHnJWVViGtvjjhdk68wuuSMdUOTtROA11hWjqKcQDBwpBGndfV8bvxi7YAQOmKjBNO95lbSIJZRdnt0rlqyyf45o065MgJz1ZwzgdxzrsL/n0KYKthHJtG8jbRPhhjtQCMBXAX53y6Zd+beQlHAbwCoK+OP6piI77rf9GnRUq1tyCo6kIHnekBQL829QIZS37tVreSwX85Kz1jX0YdpUXdsvMRxbjk5k2XQbicqGEiMPeewfjDGe19/94c1HVlaLtdC9nl0p6aQzRUkDEsZSYKXmovVrxug9Mln9lmtatIFfUxjRHR3yGTjyCiY2PvCaE9nEeFqSvdnQM6ObVjQzxxSc9Q9h2mx1zkKW1oKaJx7nFN8dKIfF/7bm1JQA7S3TppPstMLkYEKKdt7t3J2P3epcR7lQxVzpLhHpeJSZQENeXHABhhvB4B4FP7BoyxSgA+BvA65/wD23emAc4AXABgUcD2EBGgmtgou/TvNiB7ia77QcaAdRvYvZdxxZztVZxBEa+/Y+YauRAIJ0Sdd51qwTPS61avFChJVneCrSico3JOFv5xWS/pfcQpYHDucd66yjJnrIekBjbg7eW75uT0ZCfR/frDHWdKDfhXD2iTEnN5tcU48auRbzeCRMZ4Xc2qA2Hx+m/7ZlwFXEB8TzSxFGX61xXHpyhWKO3b5TvVcDIRMtU+g6xyiXxW1r2pFnRRIcGOaOXkRauqy3k9m2mrrxD07I8GMJgxtgLAIOM9GGP5jLEXjW0uBXAqgKsFUnZvMcYWoiRFqQGABwO2JxLCFpNPOiqeKiDdQ+YkCXTkmHPsbv3qwbyNXk0ec9MA4efb9zsnp0VZjc6NypoSO03+8+vUsJ/qgvtdtky0KPFM93mTDS3xQjTZY4xhuIsuux2vbHZruEeQGH8RXuP0dae2lZo4emWs3z+8rGy9570nOJ5OBRPrhLRDYz1yog0lygjfNbSLluRSQkyDGsFjVUXY73/dkmxhIOovdaxgyiQshk2TABJ21jHoz4M74s+DO7pub1VXaVG3Kv52XjeXreUJZA1yzncCOFPw+SwA1xqv3wTwpsPv9dQOjYBfHN8CH87ZACCYzm3YRBEeoLqsZ29T7Wq52HkwvWTzNheDtVOTYIZSY8FyrNULcZyD923plv2BjpuJDOrSGHcPS6+e5gfRUqfue7RuQK/4CXkliWdN64SvSXr/8O646uUfAeiXivRaVm7boLrUBMZrP1f1z8O9ny6WahMDwxX9WuHtGeuktldFRq88jD7xd6e2xa9PbB1pqEZFYseBAukiTx/c0F96v/b7X8eKWtic0dk7Ad4LUdLg3y/tia37jgTedxD65NXF2AWbvTf04I9ndvDcxpq/oXMRMxmR2RmA1fuqu7RlnOT7+FtUH+q0QdlhDtLbJZ40DK9EENH+8kxOdhauPaWtln2dkJfu3dU9Bw1qRIuKFPjF608b0D69cI0uvIzfetUrySl4WF47eXfm3TsYP4x094F0b14LHRrXSMtl0GnUyqxC1Kzs7m2UmcyItqlaKRuDFWT5wuRql5jbqFUX/Fxe0SUwdc/dnDYNalRCpwAe1d4uhW6SQtPa6f2T2zmeeVe6MpFo/OzWrDYGdna/f3uEqOk++S+nY7ShTx0FZ3Qqs1t0JsmSEe2C9eG1dtaijHS/18RrCUIVt/ADER/ceFLKe5kMZJkEICv2WLbVO8RZyO0apg+IprSUvXKXDmRjtZ3Q8RwmRCUvNEQzft2xzFblhHcFpdu9jq3TqG/gEXak+283K40B3veS7IBo3c/l/cQTTRlpqM//eAoa1Kicdn51ngGZOGovtZEalTM/LONvgiSrX59Ycu0m/+X0wPu3hvCEgd1D3L1ZrdKJyyu/OcHxd7PuHuypA+42SVIdy+LA/vy8/tu+rsnUou+suQsPX9jDMXzRTr0QPfVtGlSPtK6CtTy7zj6IjGgXvv7zaZh2R4m3xcv76jdOWnfC3BxJHV0nLuztHf+pqpcZJKv7hICGrggzGz+Myotu5YFFJCWuOixEijCqMfUqnNjWvSDEolFnp33WoIYxUGgwplvWi3ZQfsTiyanuMdFsWluubaaX5qLezdFAIj44aQzu2ljJw6hL3UWFJfcP0bKf70cOxNd/Pk3Yl5n62EELcQAlITxRsnnfkdLcGdFqlhOqMnRXZMBqpLXY1ZrRw3Bqx4b4+6W9lPZhdUJd0a+VY/hiGuV0eNKZjFmxM+Q8sNZt91o29LukpNOQy2IIfNPLeMpkD9GiblVs2O1Po9jkot4tUFDoXW3QC+tpTlJxg6CXP+mebFGoRZ+8+MKhRF7Js7uXJKclN9PBGWv/0UoiUVHmfqlseHevP82/hncq3OWdfl64Sk0KrUXd1PMWZvua1KqC6XempREp0btVHcxdtweAuyf1/J7N0bVpZpRYt9+XOw8UoE61SsrV71RjnJPefwLAyYIQsCSNYVHTqXFNLNsaLFeJYqJj5meHcAQ/6PASmLSsVy24Z1OrUR98X01qV8Etg/SGvKjqXDsRV//b38PbmnRyNZ1/3ZjLprIJTeUVM2O+gybVE7sHPGl2y3k2acAnJy4P7Vj92gZfWTNjOyfc4l61MTuLBYoXjhLznpjy1zNC2zeQPrwlWCOglDDygVTxKnyjazVniIT8azUN4VdanZfa9lSBmCKorCeSAZPBK+nj/J7e2q8mY246Obhn0+W7UzqUzIhlj/HKb07AR78/yXtDAx6RL7CJLVGjkyZpLEB9DuJWatrEnvypu1P1G4oko0cb9H4UeWF0Y4ZBmM+iKDY/6bRzKQpkRU6dw387RMvpOkqkh8Xnfzw51OQpOzLPuxcjjNAKmSIxmYLuwklxcs+5/oqAOElLulYUjAivycakP5+GefcODnycX0iMKapOmEd/kZ68qLMITfxXp5zwgOaS1CZdmtaS3rZ21dzAXh43ndtfKxYzaNewBo5vlb50r0PgXgYnLWNrHOnQHk1w97l65NwA987mh5EDsfzBc1I+y5KwWN76Xb+U9zoSoawaodeckl4QQwY3RQA3VOyIPw4UVzQcqEH2ycR8xvIalBh7Vzgk0+lizehhysvUbtw/vBseu1iuSp2UOoexkR97b0h37xyPJCyhv6gY8qGLqwJUrjOpXS0Xa0YPCyWnIy50/Smq+wlDJ3qYYmXgiX8qWVEQVcBNChcd754rVatKrha5wGaGzKibhvWQ7k3wsYdzzlqbQjSJ1+mcISNaE3U1hmUsvq8s+cn0/kZFr5bO8aqDujTGG9f0Ddx5H3YpqqIT67K82eblD56Tsmrw3K/64JQOcqWJ7d4+mfNgVU9oVqeqdJESK/aCFlY73e9ypNVb8jufcnYyxUKCJqY5TTJq+awWKcK8JmZ19CCJj0GXh/0UQLiqf15pEpkXsiFWfo20XgKZyvSJuX7jT6WfbFizshbtXT9E5UCIk92Hjin/xrzXgj4/ojvL7TYOYyJirnSaEn1edGhcE2tGD8MlihX4/FDJp1fbWnCqUUirBYvuOxvdmpWsCtkLtFmpWz0XvQXOOSvWy3pSu/S+QedlJyNaE/ZA/6d+2RPj/u8UX/uyGnndm0ebGFKrqrNxkp3FpA3OJKDaoXpxRT9vT7x9/2GqG7x8dT5eudpZ/skNayy+31AkGXkiv/s20ZkA4mloGQN4cYxxkuYg4sVFEio6IsL2XZr3VZTV0JrXqZqmRe2GNdbbS/4OAL78k3vsMWEnWYHGVkNZ5zK+G12a1korS52EugT3KK66mpPuoLlbPSTsGNlkyfzW3nkFUa7RVNwUz5C5sLd3bE8Y6FCyKM8EebiuOdk77MFugA3u2hj/mrwywFGd8RLKd8Ntpq+bj35/Ei567ofS9yrJr05GpR8P0h/OEIeGmJgTyCjisIMyang3jPARGhB2BIAZ13meQi5HUL73KPpiZeGos1JiTFtLPAcdI86ZeOOavrjypR+1HTNqtu6Tr1Uw9Xb9iYRuWHOQwiz0IXKgRV30xs7cewYrt0FXxVnVfKdzengnGAaBdKIJRzbvjbeMZxCSmCnt5j0oLE6fsNiXrsMwWsI8T37jnN2wx8VnZ8ufFCfPkVvsvhP9PDTHTcO8Sq5ct5gnCGepoTHMxI1aVXLR06XCp51LjISdqOJoL8kvcyLYQ5jiDOWtXikn5Z5KYlxxVN7SsFAZg+zyglFyWQI8w1FSt3olqRwcK3Wq5mKUUcjH7D/9KICpdtdu8dUy+4rSlCAjuryRvDEhNnRkNbv1OdNX7Uz7zD4oJ7WYipPtMOr8cCuTAfFonKrE+FaWNGI++UN61a8kZNKLiMq4Fw1wUVYlU8XrjpBVPdFJWKEwUefXEKno7ho+u+lkfP7Hk/Xu1EZWFsPVA0pWYH8zIA+Av6qrOstsy3i1o3TIJbPHTwDPX9kn7iZIkbYsHzgxI5lGnx90ZF67nU5RAo29o0ygowsAUCVHzdt1hiVJ5maLYsYd53RW2o+uaoVhxZqvfOgc6SVPJ29JzYgMVhWieq7NGOOk9iPm88hs753QPSmSOSthecf9eriv6q+myiSLKI49KpnTOJBN/pWlR4vakeZM9TFike8epk/NSsTNZ3ZIk3VVxWucocTCCJAuixkz3ZqlSuB5SdEkgdIyyzb8zHDd0JHt7DajfX/W+rTPOjVOvR5JDFEB5KT1rLzym76lr2+1SDGZVe1kw0DO1VDmfsadZ+K2IeHIQekoB2sqnyx7UE9p56C0rFcVJ2oo8iFDldxsPHBB99KSzSK6NJGX7dRN3OEbMt1BFC08u5t8PoWulaNTbUZzrRBihOO+vm6Iqre6cV8Eq4IqNKxZGWtGD8M5ihJ+ADC8l3yOxK2DO7quXrlV6TTxlinVd58kz2VCKNGpSU2MW7Sl9L1MtrkbUfRBvxnQBo9PWJb2eVAPXk4WQ6Els09U2EC1k21mK8xipW+bemmFd3RWoHTixtPbIT+E0tn2CZksMprH3ZrVwuJN+7QMnI1rOV+TJHBpfktcfHwL5YlKWEz5q3zinQ6u9NCT1z1Z9oPZS0RtdMn86WE1yTqhD1M1yInXf9sXeSPHlr5vbJNK++flvVE1NxtXntjadxiNbK5EVJPKIIw4Kc9X8nAS0bk6J+Po8Fp1kZFnlSWQ24UxVo8xNpExtsL4XziyM8aKGGPzjH9jLJ+3YYzNYIytZIy9xxgL3wKRpL7AWxpHfJwX/3dmB637i8Jz6jSTDHps+9KryIhRTUgb4BJHKBOa4GdA9NIXbVanaopupw5+f3o73HZ2eGL/ZhKcVwVOv9JtScOvAR2lTffEJT1RN4RiE5lAXGa8TGxoeJMMfx1sWENC/3b1U96f37MZGGN44ILupXG4boiW/OOUp/RCNGF77bd9BVsSYaOjcqhJ0LXLkQAmcc47AJhkvBdxmHPey/h3vuXzRwE8xTlvD2A3gGsCtkcboli4JCYN2R/MoAH8UQziLevpLVttYo+xs6sCvDQi33MW28Tm5aym6NmvrqGaYF796Cdrfx3SGad3Cq8AhXlb5eele4AusZR6/a2EjKAT5aEMcpThP71b1cG953WNVNMZSEbca1YWw5L7ow+3aSaxFG0vrhQG5mqOTPGNsO7JoPttLegnZVcC4wizEzlwVPTNCXlERZ+sJCkmejiA14zXrwG4QPaHrMT6GwjgAz+/jwPdg01fF8mtL/90KmbfPUhqP29f2680S9fvknxQ3MIe7PSxiKV7yY6p0Na2UlDJJqV2ZhfvOMCBXcoMyS9uPgWdXeI3Rf2wDiWC4qQGUgfAreiKNcRDpcy9lYcv7IEnL+3l67dBsCoePHvF8ZEfPwgMJXr242+pmMVEgoa++aGyj4ql+ijpDxvWrIzfn94uAe0JhsgQalSrLETkqv6tE6X5nunShUGIekhr6SGdqLM9QZ+gxpzzzcbrLQCcrJQqjLFZjLHpjLELjM/qA9jDOS803m8AEOtarlfcjn35ycSP4dq2QXW8d92Jjt93bFwT9SXj1k5q36A0SzeoF8NvEsmgrv4Kf1i9n0Hva/vv/SRAWENNunpc17A6hnO6B0++SxqyMWh+l7Kv6Ncq8uqeAPDPy3qXvtZRTCGKlSDT+9VEYeJL6CEpiW+lK3ISzWlRN5yVwzCw6k5fmt8Sb17bT7hdHG6KOFYYk0LU57u2Q5iaGf6qM1/F04hmjH3FGFsk+Dfcuh0vWatwOletOef5AK4A8DRjrJ1qQxlj1xmG+Kzt27er/lwOjytd5FAMcOzN6uW9P7lpQCgdqj2EQRW/snB+/5IbT1e+FRyxlvNdM3oYavnwCgeV1rEjW7gDKKv21r9dfalkvaj5+s+n+S6BHIc2dBTUtSwf92pVJ/D+ojCxzJClapXK5zXRSVGSg2wVEQ03Mveb24ppEMI4s1dYCqi4hV/6KdYUFHsYk70seHkmGVNHvXrVJp69KOfcMaaAMbaVMdaUc76ZMdYUwDaHfWw0/l/NGPsGQG8AHwKowxjLMbzRLQBsdGnH8wCeB4D8/PxQngCvZXSdSTh+DDzCnTYNSmb6jWv5zzw3PaEyxrTMTdi+kXwIUE5WeqevS1fZ5OqT8tDBZ+xw24b+Y47PO64ZekYsG9mgRiXsOFAQ6TGDcrElPjwsTu7QAO8J5BmjIqka0iKuDEkjOQ7M4c08+4O6NEbDmt4xxGHZm2HcBdbuspNL+GU8MdGp7+81KgFWBBKyAIMmtfWr0gQN5xgDYITxegSAT+0bMMbqMsYqG68bABgA4CfDcz0ZwMVuv48SL23Bs7uFW889k9HtVTcNYj9t+NOgjr6Pa8bkWj36oxw6O93eDFNf2ErDmnof+gt6N8ev+kVvGGRlMeQ5XFPzNH572+lajxm1lJoOb7tstcQgDO3RVFhtMSqSkFgoi05v/eWCMtPXn9ZW2/5lMbvqF0fk45GLjov8+CZh3AWy41Acd6D9mG55IkEJ4kgKg7gmzvbCMJfmt8TYm/VWeQx6FUcDeJ8xdg2AtQAuBQDGWD6AGzjn1wLoAuC/jLFilBjtoznnPxm/vx3Au4yxBwHMBfBSwPYE4rSOjfDClJ8dv0+K7muYJCVmzw+mkkaQhKEy/diyz67qn4dLT0hfepPJbFdhSPcmaWEcOq5G24bVsXr7QQBAI81GuQ7MFSBRtn0mMPX2M6QKACSF7Czmmb1OlKBTT/ZPg9LlSOtUjU7V1W/XnsFDgiO6krd1Ozl0kbTVHqd8sjARhUQyxtCtmd7cmUBWAOd8J+f8TM55B875IM75LuPzWYYBDc75D5zzHpzznsb/L1l+v5pz3pdz3p5zfgnn/GiwP6f8k5udrIfD5LgWem9MP3+ladQGmQiY4RPW2KmsLCb0SAWtiCizdO9UWloF3ca+brwu1+V9kx072KJuNW2Tz06No5WbI9zRmSOhQ7knCOYdqmo/hjXiBN1vkN/3bqnnun6hkA9V3eLcecsh4VEXN8SwwuFGjia75eWr87XsRyfJHl0zgJ4Re3SSNsM0CRIva2Lt3B+6sIfy753CBVRQkSGqlBPutZh2x0DH7HK/JDFRyivZ45GLjktkomUYxCG7Rjijc2VOdG3Lo5fXxL6UbkdU0CwqdFV8ValQa72XdK5wiJDRI48SXZGPAzv7UwELE0rPtpAJHdrxrevE3QRtuCmJBFn+0RGrLLOLKiEXRWhaW09HWGCRlUmikaby2FFxAqI8EWWCmzm+qY5zfsdFa6iDKNwpzvCtuPWxW3joGAdlcNfG+H7kwFCPQZRAnuiARG13H99KbgYddSfh5zw8paE4hkhMX0e826kdvUX6KyvI14mIagA146EBoIGk9niUqJyGpIYzEfLEEV606uGhWPZg9BUKvYi6WiSgrmfezucq44D2DTDCUDfRoaFux82495ps65LcjDp5WRbGWEblaXhhryScJMgTbcHP4xC191qmAlMmLH2Lg/7V9/PwhT1w6uOTS9+P6N8aAwJWqfrxrjPRUMLYDGoEZ5JKAUHogLF4kpezsxiys5K1ChN1P20W+7hCoBLiht/r1aBGZdw3vDtem7ZWvF9fe9WD3563RuUcHDhaiH5t6mHGz7u0tqk8E+SRf++6E1G7Wq5r9eA4IU+0lWROKlNwqsQTNzrGRT9GaStbbNl9w7ujUc1gs9ZGNatoH+hFBQuSGt8eNX49XUTm8btTkpXwVJE4Ia+kD4raezq4a2Nc0LsZAODEtmX9YFAXglNIxD8u64U7h7rHY/vFHBY6UgKwEtaxTlW+tl/b+ok1oAHyRAcmqiX541vVwZx1e9CyXrixVH4JoxKQiJPbN8DUlTsiOZZOThAksiR0JTByrujXCmd2aeS9IQCVmW796pWxdd9R9IihHLgIncWaMpUqMceiEuFrJFfJycKRwrI8jBeu0q+osHDUWY4a3sN7Ndd+PDvtGmamHGdc1KhSdq0GSff1mQH1aBb8eDCjWpA31S+qJ7RUr46ZuYwdHlYJ2jC5+cwOuLhPukxbJiSyRkXjEGLezJLr9w3vpn3fhD+CykJWJH59olrYhRdR9TfN6zrH4ua31tN/16ySG8ij7jdeuKYRSx1k3D+vZ7MAv85MrDHoOlS0kgQZ0Rb8CKdHZQf1b1uiVpHcRIbg+5Dx6vduVSf4gTTjlRx06+COwiWsqLz35YX//LoP7vKQzbJi6vLKJuMS4WEmtCZ1JS2J5IWkXhFHyWuTzk3jD4NYM3qY7/twSPemgY9fkXv9oT2aCGPyOzbO3JA+MqJ9cFHvsuWiqOygOiEuBWeSbFjS9C8BoI/PggyZXB0yDoZ0b6IUT/ePy3rhm7+cHl6DFKnI1/vqk6IvNZ/JLH1gCH47oI3WffqVuNPaBosJmYlPQzUNEqFu0q7lnZysLGE/qBonnSQq7tUMQBze4DA7vqD7vvfcrqVe1XevOzHUdiSx4/Xr2Ilbq7S8U6dapXK3dEhUDKrkZiNL8zgTVSLzKoukphuZuBL3hzPa44ubT8GJbf3XMegVcYG2JOF0S2dykn0yA2wTTq7F+Inq0idR39ekWqVsZGcxfHvb6bEK6EdNh8Y1MbCzvySJu4Z2wVXknSMAdG9eC4s27ou7GaFy4fEtYKn5Q8SBMVjFGc6RQgbaTVUrZaNrsxKliHn3Dlb+/dUn5SkkUZc/nG69ejFWrwwKGdE+OK55bbwd8TF7NK+Nd37n38vrhq5ONS4D+lf9WmHngYLIj1u7ai5evvoE3PXxQuXf/u7U6KW+khpPX9EZ0L5BKEb0yofOweFjRdr364fmdari/wZ1iLsZFRrz6ddR0VUHGeiITqFONXXDb9T5FTvJWXTrTbtjIOr6OJdJgYxoH1iXoepH5CFmjAUqhe3GwM6N8O3y7b5/75aNHQUPXdgj1uObcXIVeZmOCEBINk1OdhZqxlAdkEgm5rgVpwnd2qLrX6sKST5WNHIEjpymtZOX56QC9bAWpGfGlu1+cXz4mpRh07JesJv4lA7RJSZakxKuPy0ZhRuG9ijJ2A4SD06Uf5w8gMnwCxLlHdOA6R2jWk2T2skt30yEz2WK1TIzATKiLciucjGXd4R/zDPpZtTnZped7zM7Nw65RXKYYRJJTxRsEfOKQUWnIqtzEPHTvUVJ0aFGPqRcdUFPQMUmJ7v83QHJHvUTSJNaVdCTlu1DwZzDfHjDSY7bWEu9JsUmyZTM4mtO1iuZReghjEIzRHCsJarLA5mohkGUL3RIBCaNQEY0Y6weY2wiY2yF8X/aOhFj7AzG2DzLvyOMsQuM715ljP1s+a5XkPZEwfQ7z0ypzlce+qWkGYF1q7snGbwYQhnZIPAMWZA/q2uTuJtQoXF6ympXpdhQInyq5SbHgHn+yj5xN4GImGl3DETnJrXiboZ2gnqiRwKYxDnvAGCS8T4FzvlkznkvznkvAAMBHALwpWWT28zvOefzArYnEOXBIPZDUv5usxm5HslQg7qWhHEkTW0iIUnvjlA8Yip+KpSGQVLUEojyjW7daV9tSMpgQ0ROpicQOhHUiB4O4DXj9WsALvDY/mIA4zjnhwIeNzGUhy6hZ4s6cTfBF7lZyYhGMj35SRikCAIAhnSjVYegFJfDuc3dw7rEGj7ktcpIEJlGUCukMed8s/F6CwCvTK/LALxj++whxtgCxthTjDFH1xBj7DrG2CzG2Kzt2/3LsRHpZIcQ7F+jsrp6YtdmtXCdgn5ymKXQCUI3Tk64MGy141rWDmGvFYvyuEJw7SltE7GC57XaSBCZguedzBj7ijG2SPBvuHU7XtLjOPY6jLGmAHoAmGD5+A4AnQGcAKAegNudfs85f55zns85z2/YMDpJNcIfv/WRxFatUg7uHNpFatvVDw9Fy3rVvDckUI+8P0JOyKuLTpb8hvJCpeysjF1dShLl0ROdFGpVpRIVRPnA807mnA9y+o4xtpUx1pRzvtkwkre57OpSAB9zzo9Z9m16sY8yxl4B8BfJdodCVPPzi/u0iOhI5RcKnSCC8q/Lj4/4iOJ7trXmyeDyh87Rur+KysntG8TdhHIM9d9E+SDomsoYACOM1yMAfOqy7eWwhXIYhjdYiYDqBQAWBWxP5OQ1UC91nbSlrOqV9HoF+retj9M7VZzVgsq5ybqehBxZWSwRk7F+bcOpREoEo0vT8qckkBRodYwoLwS1nkYDeJ8xdg2AtSjxNoMxlg/gBs75tcb7PAAtAXxr+/1bjLGGKJmWzgNwQ8D2RE6zOpmfcao7Ru6dCla5r2Pjmpj059PiboYj8ZuJBJF5UMhBOCx9YAiqJEhujyCCEKiX4JzvBHCm4PNZAK61vF8DIK0+Nud8YJDjE0RSaNewRtxNcKR21VzsPFgQdzMqPKTulVl0a0bJmWFABjRRnqCpdiwkM2OlR3MaNMoj713fH0cLi+JuRoWmWqXscrFqRRAEQZRBRnRA/DmXkumSOqMCxTFXJJJSVKQi8/3tA5ETgpQkER60ckAQhBdkRBMEQYQMFZkgCIIof5CsQAz0a1Mv7iYQBEEQBEEQASAjOgbaNlSXxYuC+jXUlv2bU4wnQRBEuSQB6o8EkXgonMMCUwiCq1e9EnYdLCg3cXMj+rfG0B5NlX7z0e9Pwv4jx7w3JAiCyDDKSdceOXWq5WLPIRoXiIoBGdE+qV01F7t8yoaxBHbP9w3vrvybxrWqoHGtKiG0hiAqHpfmUyXTJKHiVCmP+NWQysmiBW6i4kBGdEB0FyohCKLiUbdaLn55Qsu4m0EQgbmib0ts3nsk7mYQRCSQEe2TapVKBOMr56gLx1dwBwdBEDbm3ntW3E0gbFA37Y9bz+oUdxMIIjJo3cWCSqd53altQ2sHQRAEQRAEkWzIiLagEgNWlUqXEgRBlFt0rRhm6sojT2ZhXYJIFGREWyhW6DXO7NIY71/fX/kYfVrXRav61ZR/RxAEQYTPiP6tAQBVfITqicjKVCuaIAhPyIi2kKOQJJidxdDXR9GUD288CbWq5Cr/jiAIggifa08pCdXL0pQ0TiY0QZRfyIi2UK0S5VkSBEEQ+qCoCIIov5ARLSCpFQUJgiCIzIJTcDFBlFvI9WrjrqFdcEbnhnE3gyAIgogB3SHMOVlZKCgq1rvTCBjSrYlSnhBBVEQCeaIZY5cwxhYzxooZY/ku2w1hjC1jjK1kjI20fN6GMTbD+Pw9xlilIO3Rwe9ObYv2jWrG3QyCIAgiBprXqYrnr+yjbX+3DO6gbV9R8p8r++D5qxyHdYIgEDycYxGAiwB857QBYywbwLMAzgHQFcDljLGuxtePAniKc94ewG4A1wRsD0EQBEH4hjGGs7o10ba/mpRIThDllkBGNOd8Ced8mcdmfQGs5Jyv5pwXAHgXwHDGGAMwEMAHxnavAbggSHsIgiAIgiAIIgqiSCxsDmC95f0G47P6APZwzgttnwthjF3HGJvFGJu1ffv20BpLEARBELo4rnltVK9ExbkIojzimVjIGPsKgGht6y7O+af6mySGc/48gOcBID8/n7IdCIIgiMTTs2UdLL5/SNzNIAgiBDyNaM75oIDH2AigpeV9C+OznQDqMMZyDG+0+TlBEARBEARBJJoowjlmAuhgKHFUAnAZgDG8RDxzMoCLje1GAIjMs00QBEEQBEEQfgkqcXchY2wDgP4AxjLGJhifN2OMfQEAhpf5JgATACwB8D7nfLGxi9sB3MoYW4mSGOmXgrSHIAiCIAiCIKKAZWI1pfz8fD5r1qy4m0EQBEEQBEGUYxhjsznnQtF0KvtNEARBEARBEIqQEU0QBEEQBEEQipARTRAEQRAEQRCKkBFNEARBEARBEIpkZGIhY2w/AK9y40Q8NACwI+5GEELo2iQXujbJha5NcqFrk1zK07VpzTlvKPrCs9hKQlnmlClJxAtjbBZdm2RC1ya50LVJLnRtkgtdm+RSUa4NhXMQBEEQBEEQhCJkRBMEQRAEQRCEIplqRD8fdwMIR+jaJBe6NsmFrk1yoWuTXOjaJJcKcW0yMrGQIAiCIAiCIOIkUz3RBEEQBEEQBBEbZEQTBEEQBEEQhCIZZUQzxoYwxpYxxlYyxkbG3R6iDMbYGsbYQsbYPMbYrLjbU9FhjL3MGNvGGFtk+aweY2wiY2yF8X/dONtYUXG4NqMYYxuN52ceY2xonG2siDDGWjLGJjPGfmKMLWaM/Z/xOT03MeNybei5iRnGWBXG2I+MsfnGtbnP+LwNY2yGYa+9xxirFHdbwyBjYqIZY9kAlgMYDGADgJkALuec/xRrwwgAJUY0gHzOeXkRV89oGGOnAjgA4HXOeXfjs8cA7OKcjzYmoXU557fH2c6KiMO1GQXgAOf8iTjbVpFhjDUF0JRzPocxVhPAbAAXALga9NzEisu1uRT03MQKY4wBqM45P8AYywUwFcD/AbgVwEec83cZY/8BMJ9z/u842xoGmeSJ7gtgJed8Nee8AMC7AIbH3CaCSCSc8+8A7LJ9PBzAa8br11AyCBER43BtiJjhnG/mnM8xXu8HsARAc9BzEzsu14aIGV7CAeNtrvGPAxgI4APj83L73GSSEd0cwHrL+w2ghyhJcABfMsZmM8aui7sxhJDGnPPNxustABrH2RgijZsYYwuMcA8KGYgRxlgegN4AZoCem0RhuzYAPTexwxjLZozNA7ANwEQAqwDs4ZwXGpuUW3stk4xoItmczDk/HsA5AP5gLFkTCYWXxHFlRixXxeDfANoB6AVgM4C/x9qaCgxjrAaADwHcwjnfZ/2Onpt4EVwbem4SAOe8iHPeC0ALlEQNdI63RdGRSUb0RgAtLe9bGJ8RCYBzvtH4fxuAj1HyIBHJYqsRW2jGGG6LuT2EAed8qzEQFQN4AfT8xIIR0/khgLc45x8ZH9NzkwBE14aem2TBOd8DYDKA/gDqMMZyjK/Krb2WSUb0TAAdjIzPSgAuAzAm5jYRABhj1Y1kDzDGqgM4C8Ai918RMTAGwAjj9QgAn8bYFsKCaaQZXAh6fiLHSJB6CcASzvmTlq/ouYkZp2tDz038MMYaMsbqGK+rokT8YQlKjOmLjc3K7XOTMeocAGDI1zwNIBvAy5zzh+JtEQEAjLG2KPE+A0AOgLfp2sQLY+wdAKcDaABgK4C/AfgEwPsAWgFYC+BSzjkluEWMw7U5HSVL0hzAGgDXW+JwiQhgjJ0MYAqAhQCKjY/vREnsLT03MeJybS4HPTexwhg7DiWJg9koccy+zzm/37AL3gVQD8BcAL/mnB+Nr6XhkFFGNEEQBEEQBEEkgUwK5yAIgiAIgiCIREBGNEEQBEEQBEEoQkY0QRAEQRAEQShCRjRBEARBEARBKEJGNEEQBEEQBEEoQkY0QRBEBsIYq88Ym2f828IY22i8PsAYey7u9hEEQZR3SOKOIAgiw2GMjQJwgHP+RNxtIQiCqCiQJ5ogCKIcwRg7nTH2ufF6FGPsNcbYFMbYWsbYRYyxxxhjCxlj441SymCM9WGMfcsYm80Ym2CrBEcQBEEIICOaIAiifNMOwEAA5wN4E8BkznkPAIcBDDMM6WcAXMw57wPgZQBUcZQgCMKDnLgbQBAEQYTKOM75McbYQpSU5h1vfL4QQB6ATgC6A5jIGIOxDZVOJgiC8ICMaIIgiPLNUQDgnBczxo7xskSYYpSMAQzAYs55/7gaSBAEkYlQOAdBEETFZhmAhoyx/gDAGMtljHWLuU0EQRCJh4xogiCICgznvADAxQAeZYzNBzAPwEmxNoogCCIDIIk7giAIgiAIglCEPNEEQRAEQRAEoQgZ0QRBEARBEAShCBnRBEEQBEEQBKEIGdEEQRAEQRAEoQgZ0QRBEARBEAShCBnRBEEQBEEQBKEIGdEEQRAEQRAEocj/A8MrPoHEKulcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = random.choice(df.index)\n",
    "\n",
    "print('lable:', df['lable'][index])\n",
    "data, sampling_rate = librosa.load('audio/rec - Copy/'+str(df['name_of_audio'][index]))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='lable', ylabel='count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.countplot(df['lable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "508it [07:45,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "### Now we iterate through every audio file and extract features \n",
    "### using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join('audio/rec - Copy/', str(row.name_of_audio))\n",
    "    final_class_labels=row[\"lable\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-364.22656, 133.04326, -24.707306, 8.948652, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-379.86017, 119.4718, -3.163177, 25.166592, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-353.5439, 145.00945, -31.72263, 8.87748, 17....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-352.76407, 136.59334, -31.263487, 8.083315, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-427.30124, 129.09596, -3.9694078, 8.760965, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  class\n",
       "0  [-364.22656, 133.04326, -24.707306, 8.948652, ...      1\n",
       "1  [-379.86017, 119.4718, -3.163177, 25.166592, -...      1\n",
       "2  [-353.5439, 145.00945, -31.72263, 8.87748, 17....      1\n",
       "3  [-352.76407, 136.59334, -31.263487, 8.083315, ...      1\n",
       "4  [-427.30124, 129.09596, -3.9694078, 8.760965, ...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "extracted_features_df.to_csv('audio/metadata_new/final_metadata/extracted_features_df.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(508, 40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Label Encoding\n",
    "###y=np.array(pd.get_dummies(y))\n",
    "### Label Encoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-384.79132  ,  126.83416  ,   22.044748 , ...,    3.6588137,\n",
       "           4.157282 ,    4.989248 ],\n",
       "       [-332.5388   ,   92.562675 ,   -3.9492354, ...,    5.115012 ,\n",
       "           4.4112234,    2.8012795],\n",
       "       [-315.8667   ,  120.47542  ,   20.410995 , ...,   -2.1051085,\n",
       "          -1.2823454,   -1.5356203],\n",
       "       ...,\n",
       "       [-392.28574  ,   88.3984   ,   -4.571233 , ...,   -1.4070433,\n",
       "          -2.783642 ,   -1.0058346],\n",
       "       [-393.10715  ,  157.4563   ,  -13.183333 , ...,    4.2225337,\n",
       "           3.0959253,    2.988069 ],\n",
       "       [-339.63928  ,  100.59851  ,   -1.3436158, ...,   -1.5343856,\n",
       "          -0.6541061,   -2.4956307]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 40)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### -------- Model Creation -------- ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "###first layer\n",
    "model.add(Dense(100,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "###third layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "###final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               4100      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 30.4512 - accuracy: 0.5938\n",
      "Epoch 00001: val_loss improved from inf to 10.50683, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 23.6793 - accuracy: 0.5739 - val_loss: 10.5068 - val_accuracy: 0.7157\n",
      "Epoch 2/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 20.9837 - accuracy: 0.6250\n",
      "Epoch 00002: val_loss improved from 10.50683 to 2.98369, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 18.3883 - accuracy: 0.6527 - val_loss: 2.9837 - val_accuracy: 0.7157\n",
      "Epoch 3/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 10.4431 - accuracy: 0.5625\n",
      "Epoch 00003: val_loss improved from 2.98369 to 1.34091, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 11.8652 - accuracy: 0.5985 - val_loss: 1.3409 - val_accuracy: 0.7451\n",
      "Epoch 4/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 5.0535 - accuracy: 0.6250\n",
      "Epoch 00004: val_loss did not improve from 1.34091\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 8.9514 - accuracy: 0.6182 - val_loss: 3.0204 - val_accuracy: 0.7157\n",
      "Epoch 5/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 7.3028 - accuracy: 0.6562\n",
      "Epoch 00005: val_loss did not improve from 1.34091\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 8.5055 - accuracy: 0.6108 - val_loss: 1.7315 - val_accuracy: 0.7157\n",
      "Epoch 6/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 5.9378 - accuracy: 0.7188\n",
      "Epoch 00006: val_loss improved from 1.34091 to 0.98213, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.6061 - accuracy: 0.6256 - val_loss: 0.9821 - val_accuracy: 0.7157\n",
      "Epoch 7/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 5.0160 - accuracy: 0.6250\n",
      "Epoch 00007: val_loss improved from 0.98213 to 0.84464, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.3107 - accuracy: 0.5739 - val_loss: 0.8446 - val_accuracy: 0.7157\n",
      "Epoch 8/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 6.2611 - accuracy: 0.5938\n",
      "Epoch 00008: val_loss improved from 0.84464 to 0.59119, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.1447 - accuracy: 0.6404 - val_loss: 0.5912 - val_accuracy: 0.7255\n",
      "Epoch 9/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 3.3299 - accuracy: 0.6875\n",
      "Epoch 00009: val_loss did not improve from 0.59119\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.2570 - accuracy: 0.6256 - val_loss: 0.6852 - val_accuracy: 0.7157\n",
      "Epoch 10/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.6221 - accuracy: 0.6562\n",
      "Epoch 00010: val_loss did not improve from 0.59119\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.4764 - accuracy: 0.6453 - val_loss: 0.6616 - val_accuracy: 0.7157\n",
      "Epoch 11/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.4798 - accuracy: 0.6875\n",
      "Epoch 00011: val_loss did not improve from 0.59119\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1042 - accuracy: 0.6379 - val_loss: 0.5939 - val_accuracy: 0.7157\n",
      "Epoch 12/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.4435 - accuracy: 0.5312\n",
      "Epoch 00012: val_loss did not improve from 0.59119\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7581 - accuracy: 0.6034 - val_loss: 0.6327 - val_accuracy: 0.7157\n",
      "Epoch 13/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 4.1087 - accuracy: 0.6562\n",
      "Epoch 00013: val_loss improved from 0.59119 to 0.55714, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8570 - accuracy: 0.6379 - val_loss: 0.5571 - val_accuracy: 0.7157\n",
      "Epoch 14/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.3690 - accuracy: 0.7188\n",
      "Epoch 00014: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.1840 - accuracy: 0.6429 - val_loss: 0.5696 - val_accuracy: 0.7157\n",
      "Epoch 15/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.9045 - accuracy: 0.5625\n",
      "Epoch 00015: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.6806 - accuracy: 0.6675 - val_loss: 0.5685 - val_accuracy: 0.7157\n",
      "Epoch 16/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.2903 - accuracy: 0.5625\n",
      "Epoch 00016: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5828 - accuracy: 0.6626 - val_loss: 0.5765 - val_accuracy: 0.7157\n",
      "Epoch 17/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.6569 - accuracy: 0.5625\n",
      "Epoch 00017: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.8766 - accuracy: 0.6232 - val_loss: 0.6117 - val_accuracy: 0.7157\n",
      "Epoch 18/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.0846 - accuracy: 0.6250\n",
      "Epoch 00018: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.6026 - accuracy: 0.6330 - val_loss: 0.6375 - val_accuracy: 0.7157\n",
      "Epoch 19/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.4539 - accuracy: 0.6562\n",
      "Epoch 00019: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4677 - accuracy: 0.6626 - val_loss: 0.6584 - val_accuracy: 0.7059\n",
      "Epoch 20/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.6137 - accuracy: 0.6562\n",
      "Epoch 00020: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5805 - accuracy: 0.6158 - val_loss: 0.6512 - val_accuracy: 0.7157\n",
      "Epoch 21/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5578 - accuracy: 0.7188\n",
      "Epoch 00021: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.1644 - accuracy: 0.6576 - val_loss: 0.6217 - val_accuracy: 0.7157\n",
      "Epoch 22/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.2324 - accuracy: 0.5312\n",
      "Epoch 00022: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.1417 - accuracy: 0.6305 - val_loss: 0.6336 - val_accuracy: 0.7157\n",
      "Epoch 23/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.9755 - accuracy: 0.6250\n",
      "Epoch 00023: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.1506 - accuracy: 0.6330 - val_loss: 0.6445 - val_accuracy: 0.7157\n",
      "Epoch 24/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.9369 - accuracy: 0.6875\n",
      "Epoch 00024: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.1540 - accuracy: 0.6601 - val_loss: 0.6145 - val_accuracy: 0.7157\n",
      "Epoch 25/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.0115 - accuracy: 0.7812\n",
      "Epoch 00025: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.9660 - accuracy: 0.6724 - val_loss: 0.6440 - val_accuracy: 0.7157\n",
      "Epoch 26/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6500 - accuracy: 0.6875\n",
      "Epoch 00026: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7812 - accuracy: 0.6749 - val_loss: 0.6241 - val_accuracy: 0.7157\n",
      "Epoch 27/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.0133 - accuracy: 0.6250\n",
      "Epoch 00027: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.9089 - accuracy: 0.6798 - val_loss: 0.6231 - val_accuracy: 0.7157\n",
      "Epoch 28/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7570 - accuracy: 0.6875\n",
      "Epoch 00028: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.9168 - accuracy: 0.6453 - val_loss: 0.6346 - val_accuracy: 0.7157\n",
      "Epoch 29/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7487 - accuracy: 0.6875\n",
      "Epoch 00029: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7916 - accuracy: 0.6897 - val_loss: 0.6281 - val_accuracy: 0.7157\n",
      "Epoch 30/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7850 - accuracy: 0.6875\n",
      "Epoch 00030: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7810 - accuracy: 0.7020 - val_loss: 0.6279 - val_accuracy: 0.7157\n",
      "Epoch 31/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.1504 - accuracy: 0.5000\n",
      "Epoch 00031: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.8768 - accuracy: 0.6675 - val_loss: 0.6227 - val_accuracy: 0.7157\n",
      "Epoch 32/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.9134 - accuracy: 0.5312\n",
      "Epoch 00032: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.8832 - accuracy: 0.6626 - val_loss: 0.6215 - val_accuracy: 0.7157\n",
      "Epoch 33/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.9571 - accuracy: 0.7188\n",
      "Epoch 00033: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7436 - accuracy: 0.6921 - val_loss: 0.6232 - val_accuracy: 0.7157\n",
      "Epoch 34/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6183 - accuracy: 0.6875\n",
      "Epoch 00034: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.7458 - accuracy: 0.6921 - val_loss: 0.6280 - val_accuracy: 0.7157\n",
      "Epoch 35/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6554 - accuracy: 0.6562\n",
      "Epoch 00035: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7757 - accuracy: 0.6921 - val_loss: 0.6150 - val_accuracy: 0.7157\n",
      "Epoch 36/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4738 - accuracy: 0.8750\n",
      "Epoch 00036: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7593 - accuracy: 0.7315 - val_loss: 0.6112 - val_accuracy: 0.7157\n",
      "Epoch 37/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.1331 - accuracy: 0.5938\n",
      "Epoch 00037: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7479 - accuracy: 0.7020 - val_loss: 0.6310 - val_accuracy: 0.7157\n",
      "Epoch 38/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6367 - accuracy: 0.6562\n",
      "Epoch 00038: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.7330 - accuracy: 0.7044 - val_loss: 0.6244 - val_accuracy: 0.7157\n",
      "Epoch 39/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5516 - accuracy: 0.7500\n",
      "Epoch 00039: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.7266 - val_loss: 0.6199 - val_accuracy: 0.7157\n",
      "Epoch 40/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6054 - accuracy: 0.7188\n",
      "Epoch 00040: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.7020 - val_loss: 0.6323 - val_accuracy: 0.7157\n",
      "Epoch 41/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5405 - accuracy: 0.7812\n",
      "Epoch 00041: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6359 - accuracy: 0.7266 - val_loss: 0.6221 - val_accuracy: 0.7157\n",
      "Epoch 42/600\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6757 - accuracy: 0.7266\n",
      "Epoch 00042: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.6757 - accuracy: 0.7266 - val_loss: 0.6220 - val_accuracy: 0.7157\n",
      "Epoch 43/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7280 - accuracy: 0.6562\n",
      "Epoch 00043: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.6396 - accuracy: 0.7266 - val_loss: 0.6147 - val_accuracy: 0.7157\n",
      "Epoch 44/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6049 - accuracy: 0.6875\n",
      "Epoch 00044: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.6529 - accuracy: 0.7167 - val_loss: 0.6170 - val_accuracy: 0.7157\n",
      "Epoch 45/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4305 - accuracy: 0.9062\n",
      "Epoch 00045: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.6412 - accuracy: 0.7118 - val_loss: 0.6213 - val_accuracy: 0.7157\n",
      "Epoch 46/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5746 - accuracy: 0.6875\n",
      "Epoch 00046: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.7167 - val_loss: 0.6088 - val_accuracy: 0.7157\n",
      "Epoch 47/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.7500\n",
      "Epoch 00047: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6196 - accuracy: 0.7241 - val_loss: 0.6129 - val_accuracy: 0.7157\n",
      "Epoch 48/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5905 - accuracy: 0.7812\n",
      "Epoch 00048: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6268 - accuracy: 0.7118 - val_loss: 0.6173 - val_accuracy: 0.7157\n",
      "Epoch 49/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5502 - accuracy: 0.7188\n",
      "Epoch 00049: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6250 - accuracy: 0.7365 - val_loss: 0.6089 - val_accuracy: 0.7157\n",
      "Epoch 50/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7880 - accuracy: 0.6562\n",
      "Epoch 00050: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.7291 - val_loss: 0.6124 - val_accuracy: 0.7157\n",
      "Epoch 51/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5734 - accuracy: 0.7188\n",
      "Epoch 00051: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.5739 - accuracy: 0.7365 - val_loss: 0.6124 - val_accuracy: 0.7157\n",
      "Epoch 52/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.8541 - accuracy: 0.5625\n",
      "Epoch 00052: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6362 - accuracy: 0.7143 - val_loss: 0.6082 - val_accuracy: 0.7157\n",
      "Epoch 53/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5443 - accuracy: 0.8750\n",
      "Epoch 00053: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7365 - val_loss: 0.6074 - val_accuracy: 0.7157\n",
      "Epoch 54/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.9372 - accuracy: 0.6562\n",
      "Epoch 00054: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6148 - accuracy: 0.7315 - val_loss: 0.6118 - val_accuracy: 0.7157\n",
      "Epoch 55/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4411 - accuracy: 0.7812\n",
      "Epoch 00055: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6024 - accuracy: 0.7217 - val_loss: 0.6044 - val_accuracy: 0.7157\n",
      "Epoch 56/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5865 - accuracy: 0.7812\n",
      "Epoch 00056: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.6349 - accuracy: 0.7340 - val_loss: 0.6040 - val_accuracy: 0.7157\n",
      "Epoch 57/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5026 - accuracy: 0.7812\n",
      "Epoch 00057: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5949 - accuracy: 0.7241 - val_loss: 0.6087 - val_accuracy: 0.7157\n",
      "Epoch 58/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4761 - accuracy: 0.8125\n",
      "Epoch 00058: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6053 - accuracy: 0.7438 - val_loss: 0.6023 - val_accuracy: 0.7157\n",
      "Epoch 59/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5765 - accuracy: 0.7500\n",
      "Epoch 00059: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5971 - accuracy: 0.7389 - val_loss: 0.6137 - val_accuracy: 0.7157\n",
      "Epoch 60/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5692 - accuracy: 0.6875\n",
      "Epoch 00060: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.7167 - val_loss: 0.6033 - val_accuracy: 0.7157\n",
      "Epoch 61/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6238 - accuracy: 0.7812\n",
      "Epoch 00061: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6365 - accuracy: 0.7389 - val_loss: 0.5962 - val_accuracy: 0.7157\n",
      "Epoch 62/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5390 - accuracy: 0.7812\n",
      "Epoch 00062: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5983 - accuracy: 0.7266 - val_loss: 0.5949 - val_accuracy: 0.7157\n",
      "Epoch 63/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5764 - accuracy: 0.7812\n",
      "Epoch 00063: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5898 - accuracy: 0.7365 - val_loss: 0.6055 - val_accuracy: 0.7157\n",
      "Epoch 64/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5200 - accuracy: 0.7812\n",
      "Epoch 00064: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5992 - accuracy: 0.7192 - val_loss: 0.5986 - val_accuracy: 0.7157\n",
      "Epoch 65/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4358 - accuracy: 0.8125\n",
      "Epoch 00065: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7389 - val_loss: 0.5923 - val_accuracy: 0.7157\n",
      "Epoch 66/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4800 - accuracy: 0.8438\n",
      "Epoch 00066: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6157 - accuracy: 0.7340 - val_loss: 0.5962 - val_accuracy: 0.7157\n",
      "Epoch 67/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7103 - accuracy: 0.6562\n",
      "Epoch 00067: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6122 - accuracy: 0.7118 - val_loss: 0.5981 - val_accuracy: 0.7157\n",
      "Epoch 68/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6729 - accuracy: 0.7188\n",
      "Epoch 00068: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5904 - accuracy: 0.7291 - val_loss: 0.5899 - val_accuracy: 0.7157\n",
      "Epoch 69/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7908 - accuracy: 0.6562\n",
      "Epoch 00069: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5808 - accuracy: 0.7192 - val_loss: 0.5874 - val_accuracy: 0.7157\n",
      "Epoch 70/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5418 - accuracy: 0.7188\n",
      "Epoch 00070: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5826 - accuracy: 0.7241 - val_loss: 0.5762 - val_accuracy: 0.7157\n",
      "Epoch 71/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6281 - accuracy: 0.6875\n",
      "Epoch 00071: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5859 - accuracy: 0.7340 - val_loss: 0.5876 - val_accuracy: 0.7157\n",
      "Epoch 72/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5956 - accuracy: 0.7812\n",
      "Epoch 00072: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6065 - accuracy: 0.7192 - val_loss: 0.5842 - val_accuracy: 0.7157\n",
      "Epoch 73/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5477 - accuracy: 0.7812\n",
      "Epoch 00073: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5479 - accuracy: 0.7438 - val_loss: 0.5647 - val_accuracy: 0.7157\n",
      "Epoch 74/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7895 - accuracy: 0.6875\n",
      "Epoch 00074: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5978 - accuracy: 0.7192 - val_loss: 0.5835 - val_accuracy: 0.7157\n",
      "Epoch 75/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5892 - accuracy: 0.7812\n",
      "Epoch 00075: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5558 - accuracy: 0.7291 - val_loss: 0.5673 - val_accuracy: 0.7157\n",
      "Epoch 76/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5174 - accuracy: 0.7188\n",
      "Epoch 00076: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5779 - accuracy: 0.7414 - val_loss: 0.5712 - val_accuracy: 0.7157\n",
      "Epoch 77/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7228 - accuracy: 0.7812\n",
      "Epoch 00077: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5887 - accuracy: 0.7340 - val_loss: 0.5910 - val_accuracy: 0.7157\n",
      "Epoch 78/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5448 - accuracy: 0.8438\n",
      "Epoch 00078: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5641 - accuracy: 0.7389 - val_loss: 0.5768 - val_accuracy: 0.7157\n",
      "Epoch 79/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4465 - accuracy: 0.8125\n",
      "Epoch 00079: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6050 - accuracy: 0.7315 - val_loss: 0.5811 - val_accuracy: 0.7157\n",
      "Epoch 80/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5682 - accuracy: 0.8125\n",
      "Epoch 00080: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5657 - accuracy: 0.7217 - val_loss: 0.5812 - val_accuracy: 0.7157\n",
      "Epoch 81/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4365 - accuracy: 0.8438\n",
      "Epoch 00081: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5852 - accuracy: 0.7365 - val_loss: 0.5730 - val_accuracy: 0.7157\n",
      "Epoch 82/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3735 - accuracy: 0.8750\n",
      "Epoch 00082: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5667 - accuracy: 0.7266 - val_loss: 0.5820 - val_accuracy: 0.7157\n",
      "Epoch 83/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3905 - accuracy: 0.9062\n",
      "Epoch 00083: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5711 - accuracy: 0.7389 - val_loss: 0.5776 - val_accuracy: 0.7157\n",
      "Epoch 84/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5849 - accuracy: 0.6875\n",
      "Epoch 00084: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5882 - accuracy: 0.7340 - val_loss: 0.5817 - val_accuracy: 0.7157\n",
      "Epoch 85/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5001 - accuracy: 0.8125\n",
      "Epoch 00085: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5668 - accuracy: 0.7438 - val_loss: 0.5763 - val_accuracy: 0.7157\n",
      "Epoch 86/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5966 - accuracy: 0.7500\n",
      "Epoch 00086: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5629 - accuracy: 0.7389 - val_loss: 0.5797 - val_accuracy: 0.7157\n",
      "Epoch 87/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6305 - accuracy: 0.6875\n",
      "Epoch 00087: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5489 - accuracy: 0.7414 - val_loss: 0.5758 - val_accuracy: 0.7157\n",
      "Epoch 88/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5025 - accuracy: 0.8125\n",
      "Epoch 00088: val_loss did not improve from 0.55714\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5411 - accuracy: 0.7389 - val_loss: 0.5620 - val_accuracy: 0.7157\n",
      "Epoch 89/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4533 - accuracy: 0.7812\n",
      "Epoch 00089: val_loss improved from 0.55714 to 0.54935, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5630 - accuracy: 0.7463 - val_loss: 0.5493 - val_accuracy: 0.7157\n",
      "Epoch 90/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6534 - accuracy: 0.6250\n",
      "Epoch 00090: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5790 - accuracy: 0.7414 - val_loss: 0.5629 - val_accuracy: 0.7157\n",
      "Epoch 91/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 1.0013 - accuracy: 0.5938\n",
      "Epoch 00091: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5870 - accuracy: 0.7365 - val_loss: 0.5841 - val_accuracy: 0.7157\n",
      "Epoch 92/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4837 - accuracy: 0.8125\n",
      "Epoch 00092: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5618 - accuracy: 0.7266 - val_loss: 0.5747 - val_accuracy: 0.7157\n",
      "Epoch 93/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4609 - accuracy: 0.8125\n",
      "Epoch 00093: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.6019 - accuracy: 0.7266 - val_loss: 0.5711 - val_accuracy: 0.7157\n",
      "Epoch 94/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5989 - accuracy: 0.6562\n",
      "Epoch 00094: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5416 - accuracy: 0.7365 - val_loss: 0.5729 - val_accuracy: 0.7157\n",
      "Epoch 95/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5089 - accuracy: 0.8125\n",
      "Epoch 00095: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5395 - accuracy: 0.7365 - val_loss: 0.5576 - val_accuracy: 0.7157\n",
      "Epoch 96/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6319 - accuracy: 0.7188\n",
      "Epoch 00096: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5643 - accuracy: 0.7340 - val_loss: 0.5574 - val_accuracy: 0.7157\n",
      "Epoch 97/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4915 - accuracy: 0.7500\n",
      "Epoch 00097: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5455 - accuracy: 0.7389 - val_loss: 0.5557 - val_accuracy: 0.7157\n",
      "Epoch 98/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6163 - accuracy: 0.6875\n",
      "Epoch 00098: val_loss did not improve from 0.54935\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5346 - accuracy: 0.7463 - val_loss: 0.5521 - val_accuracy: 0.7157\n",
      "Epoch 99/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5034 - accuracy: 0.7188\n",
      "Epoch 00099: val_loss improved from 0.54935 to 0.54180, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5402 - accuracy: 0.7365 - val_loss: 0.5418 - val_accuracy: 0.7157\n",
      "Epoch 100/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4493 - accuracy: 0.7500\n",
      "Epoch 00100: val_loss did not improve from 0.54180\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5653 - accuracy: 0.7389 - val_loss: 0.5578 - val_accuracy: 0.7157\n",
      "Epoch 101/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5561 - accuracy: 0.6875\n",
      "Epoch 00101: val_loss did not improve from 0.54180\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5249 - accuracy: 0.7488 - val_loss: 0.5607 - val_accuracy: 0.7157\n",
      "Epoch 102/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4985 - accuracy: 0.7188\n",
      "Epoch 00102: val_loss did not improve from 0.54180\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5471 - accuracy: 0.7315 - val_loss: 0.5541 - val_accuracy: 0.7157\n",
      "Epoch 103/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4422 - accuracy: 0.8125\n",
      "Epoch 00103: val_loss improved from 0.54180 to 0.54093, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5275 - accuracy: 0.7315 - val_loss: 0.5409 - val_accuracy: 0.7157\n",
      "Epoch 104/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5598 - accuracy: 0.6875\n",
      "Epoch 00104: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5442 - accuracy: 0.7488 - val_loss: 0.5634 - val_accuracy: 0.7157\n",
      "Epoch 105/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6261 - accuracy: 0.6250\n",
      "Epoch 00105: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5576 - accuracy: 0.7389 - val_loss: 0.5567 - val_accuracy: 0.7157\n",
      "Epoch 106/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.8226 - accuracy: 0.7188\n",
      "Epoch 00106: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5693 - accuracy: 0.7389 - val_loss: 0.5510 - val_accuracy: 0.7157\n",
      "Epoch 107/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5292 - accuracy: 0.7812\n",
      "Epoch 00107: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5528 - accuracy: 0.7438 - val_loss: 0.5617 - val_accuracy: 0.7157\n",
      "Epoch 108/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4046 - accuracy: 0.8438\n",
      "Epoch 00108: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5317 - accuracy: 0.7291 - val_loss: 0.5564 - val_accuracy: 0.7157\n",
      "Epoch 109/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5599 - accuracy: 0.7188\n",
      "Epoch 00109: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.7438 - val_loss: 0.5440 - val_accuracy: 0.7157\n",
      "Epoch 110/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5894 - accuracy: 0.7188\n",
      "Epoch 00110: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5267 - accuracy: 0.7414 - val_loss: 0.5499 - val_accuracy: 0.7157\n",
      "Epoch 111/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5553 - accuracy: 0.7188\n",
      "Epoch 00111: val_loss did not improve from 0.54093\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5543 - accuracy: 0.7217 - val_loss: 0.5472 - val_accuracy: 0.7157\n",
      "Epoch 112/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4536 - accuracy: 0.7188\n",
      "Epoch 00112: val_loss improved from 0.54093 to 0.53939, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5299 - accuracy: 0.7438 - val_loss: 0.5394 - val_accuracy: 0.7157\n",
      "Epoch 113/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5097 - accuracy: 0.7812\n",
      "Epoch 00113: val_loss improved from 0.53939 to 0.53340, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5086 - accuracy: 0.7389 - val_loss: 0.5334 - val_accuracy: 0.7157\n",
      "Epoch 114/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.7301 - accuracy: 0.6875\n",
      "Epoch 00114: val_loss improved from 0.53340 to 0.53218, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5447 - accuracy: 0.7266 - val_loss: 0.5322 - val_accuracy: 0.7157\n",
      "Epoch 115/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4733 - accuracy: 0.7812\n",
      "Epoch 00115: val_loss did not improve from 0.53218\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5305 - accuracy: 0.7389 - val_loss: 0.5379 - val_accuracy: 0.7157\n",
      "Epoch 116/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5593 - accuracy: 0.6875\n",
      "Epoch 00116: val_loss did not improve from 0.53218\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5193 - accuracy: 0.7389 - val_loss: 0.5426 - val_accuracy: 0.7157\n",
      "Epoch 117/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4120 - accuracy: 0.8125\n",
      "Epoch 00117: val_loss did not improve from 0.53218\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5543 - accuracy: 0.7340 - val_loss: 0.5381 - val_accuracy: 0.7157\n",
      "Epoch 118/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3493 - accuracy: 0.8125\n",
      "Epoch 00118: val_loss improved from 0.53218 to 0.53192, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5413 - accuracy: 0.7118 - val_loss: 0.5319 - val_accuracy: 0.7157\n",
      "Epoch 119/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4475 - accuracy: 0.8125\n",
      "Epoch 00119: val_loss improved from 0.53192 to 0.52116, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5024 - accuracy: 0.7438 - val_loss: 0.5212 - val_accuracy: 0.7157\n",
      "Epoch 120/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5414 - accuracy: 0.6875\n",
      "Epoch 00120: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5232 - accuracy: 0.7340 - val_loss: 0.5342 - val_accuracy: 0.7157\n",
      "Epoch 121/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4712 - accuracy: 0.8125\n",
      "Epoch 00121: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4914 - accuracy: 0.7389 - val_loss: 0.5358 - val_accuracy: 0.7157\n",
      "Epoch 122/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6540 - accuracy: 0.6250\n",
      "Epoch 00122: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5295 - accuracy: 0.7463 - val_loss: 0.5418 - val_accuracy: 0.7157\n",
      "Epoch 123/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5713 - accuracy: 0.7500\n",
      "Epoch 00123: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5168 - accuracy: 0.7365 - val_loss: 0.5302 - val_accuracy: 0.7157\n",
      "Epoch 124/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4442 - accuracy: 0.8125\n",
      "Epoch 00124: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5280 - accuracy: 0.7365 - val_loss: 0.5318 - val_accuracy: 0.7157\n",
      "Epoch 125/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6437 - accuracy: 0.5938\n",
      "Epoch 00125: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5330 - accuracy: 0.7192 - val_loss: 0.5382 - val_accuracy: 0.7157\n",
      "Epoch 126/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4080 - accuracy: 0.8438\n",
      "Epoch 00126: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5334 - accuracy: 0.7537 - val_loss: 0.5392 - val_accuracy: 0.7157\n",
      "Epoch 127/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5685 - accuracy: 0.7500\n",
      "Epoch 00127: val_loss did not improve from 0.52116\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5173 - accuracy: 0.7389 - val_loss: 0.5273 - val_accuracy: 0.7157\n",
      "Epoch 128/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5544 - accuracy: 0.7500\n",
      "Epoch 00128: val_loss improved from 0.52116 to 0.50971, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4902 - accuracy: 0.7488 - val_loss: 0.5097 - val_accuracy: 0.7157\n",
      "Epoch 129/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6261 - accuracy: 0.6875\n",
      "Epoch 00129: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5092 - accuracy: 0.7315 - val_loss: 0.5143 - val_accuracy: 0.7157\n",
      "Epoch 130/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5333 - accuracy: 0.7188\n",
      "Epoch 00130: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5331 - accuracy: 0.7291 - val_loss: 0.5290 - val_accuracy: 0.7157\n",
      "Epoch 131/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3170 - accuracy: 0.9062\n",
      "Epoch 00131: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5085 - accuracy: 0.7266 - val_loss: 0.5249 - val_accuracy: 0.7157\n",
      "Epoch 132/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5807 - accuracy: 0.6562\n",
      "Epoch 00132: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5214 - accuracy: 0.7291 - val_loss: 0.5218 - val_accuracy: 0.7157\n",
      "Epoch 133/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4520 - accuracy: 0.7188\n",
      "Epoch 00133: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5008 - accuracy: 0.7389 - val_loss: 0.5150 - val_accuracy: 0.7157\n",
      "Epoch 134/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4835 - accuracy: 0.7500\n",
      "Epoch 00134: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4913 - accuracy: 0.7365 - val_loss: 0.5231 - val_accuracy: 0.7157\n",
      "Epoch 135/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3736 - accuracy: 0.8438\n",
      "Epoch 00135: val_loss did not improve from 0.50971\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5153 - accuracy: 0.7340 - val_loss: 0.5133 - val_accuracy: 0.7157\n",
      "Epoch 136/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5222 - accuracy: 0.7188\n",
      "Epoch 00136: val_loss improved from 0.50971 to 0.50839, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.7562 - val_loss: 0.5084 - val_accuracy: 0.7157\n",
      "Epoch 137/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5706 - accuracy: 0.7812\n",
      "Epoch 00137: val_loss did not improve from 0.50839\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.5114 - accuracy: 0.7241 - val_loss: 0.5097 - val_accuracy: 0.7157\n",
      "Epoch 138/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4251 - accuracy: 0.8438\n",
      "Epoch 00138: val_loss improved from 0.50839 to 0.50815, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.5028 - accuracy: 0.7488 - val_loss: 0.5081 - val_accuracy: 0.7157\n",
      "Epoch 139/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5504 - accuracy: 0.6875\n",
      "Epoch 00139: val_loss improved from 0.50815 to 0.50613, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.4840 - accuracy: 0.7414 - val_loss: 0.5061 - val_accuracy: 0.7157\n",
      "Epoch 140/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4345 - accuracy: 0.8125\n",
      "Epoch 00140: val_loss improved from 0.50613 to 0.49053, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4791 - accuracy: 0.7365 - val_loss: 0.4905 - val_accuracy: 0.7157\n",
      "Epoch 141/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4585 - accuracy: 0.7812\n",
      "Epoch 00141: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.7488 - val_loss: 0.4925 - val_accuracy: 0.7157\n",
      "Epoch 142/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4484 - accuracy: 0.8438\n",
      "Epoch 00142: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5126 - accuracy: 0.7438 - val_loss: 0.4948 - val_accuracy: 0.7157\n",
      "Epoch 143/600\n",
      "11/13 [========================>.....] - ETA: 0s - loss: 0.4697 - accuracy: 0.7386\n",
      "Epoch 00143: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.4768 - accuracy: 0.7389 - val_loss: 0.4930 - val_accuracy: 0.7157\n",
      "Epoch 144/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5852 - accuracy: 0.6562\n",
      "Epoch 00144: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4826 - accuracy: 0.7365 - val_loss: 0.4955 - val_accuracy: 0.7157\n",
      "Epoch 145/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3901 - accuracy: 0.7812\n",
      "Epoch 00145: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4914 - accuracy: 0.7217 - val_loss: 0.4958 - val_accuracy: 0.7157\n",
      "Epoch 146/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4084 - accuracy: 0.8125\n",
      "Epoch 00146: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.4589 - accuracy: 0.7315 - val_loss: 0.4935 - val_accuracy: 0.7157\n",
      "Epoch 147/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4643 - accuracy: 0.7500\n",
      "Epoch 00147: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4768 - accuracy: 0.7389 - val_loss: 0.4952 - val_accuracy: 0.7157\n",
      "Epoch 148/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4894 - accuracy: 0.7188\n",
      "Epoch 00148: val_loss did not improve from 0.49053\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4731 - accuracy: 0.7438 - val_loss: 0.4926 - val_accuracy: 0.7157\n",
      "Epoch 149/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3793 - accuracy: 0.8125\n",
      "Epoch 00149: val_loss improved from 0.49053 to 0.48561, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4656 - accuracy: 0.7192 - val_loss: 0.4856 - val_accuracy: 0.7157\n",
      "Epoch 150/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3826 - accuracy: 0.8438\n",
      "Epoch 00150: val_loss did not improve from 0.48561\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4645 - accuracy: 0.7537 - val_loss: 0.4960 - val_accuracy: 0.7157\n",
      "Epoch 151/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5788 - accuracy: 0.7500\n",
      "Epoch 00151: val_loss improved from 0.48561 to 0.48216, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4569 - accuracy: 0.7512 - val_loss: 0.4822 - val_accuracy: 0.7157\n",
      "Epoch 152/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4436 - accuracy: 0.7188\n",
      "Epoch 00152: val_loss improved from 0.48216 to 0.48096, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4561 - accuracy: 0.7389 - val_loss: 0.4810 - val_accuracy: 0.7157\n",
      "Epoch 153/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4925 - accuracy: 0.8125\n",
      "Epoch 00153: val_loss did not improve from 0.48096\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4608 - accuracy: 0.7266 - val_loss: 0.4896 - val_accuracy: 0.7157\n",
      "Epoch 154/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5426 - accuracy: 0.7188\n",
      "Epoch 00154: val_loss did not improve from 0.48096\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4549 - accuracy: 0.7635 - val_loss: 0.4945 - val_accuracy: 0.7157\n",
      "Epoch 155/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4747 - accuracy: 0.7188\n",
      "Epoch 00155: val_loss did not improve from 0.48096\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4569 - accuracy: 0.7389 - val_loss: 0.4853 - val_accuracy: 0.7157\n",
      "Epoch 156/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4307 - accuracy: 0.7812\n",
      "Epoch 00156: val_loss did not improve from 0.48096\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4665 - accuracy: 0.7414 - val_loss: 0.5003 - val_accuracy: 0.7157\n",
      "Epoch 157/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4657 - accuracy: 0.7188\n",
      "Epoch 00157: val_loss did not improve from 0.48096\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4233 - accuracy: 0.7709 - val_loss: 0.4907 - val_accuracy: 0.7157\n",
      "Epoch 158/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4069 - accuracy: 0.8750\n",
      "Epoch 00158: val_loss improved from 0.48096 to 0.48051, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4600 - accuracy: 0.7438 - val_loss: 0.4805 - val_accuracy: 0.7157\n",
      "Epoch 159/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5408 - accuracy: 0.6875\n",
      "Epoch 00159: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4689 - accuracy: 0.7241 - val_loss: 0.4899 - val_accuracy: 0.7157\n",
      "Epoch 160/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4229 - accuracy: 0.7500\n",
      "Epoch 00160: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4399 - accuracy: 0.7414 - val_loss: 0.4892 - val_accuracy: 0.7157\n",
      "Epoch 161/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4081 - accuracy: 0.8438\n",
      "Epoch 00161: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4674 - accuracy: 0.7365 - val_loss: 0.4869 - val_accuracy: 0.7157\n",
      "Epoch 162/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4847 - accuracy: 0.7188\n",
      "Epoch 00162: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4551 - accuracy: 0.7463 - val_loss: 0.4877 - val_accuracy: 0.7157\n",
      "Epoch 163/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4169 - accuracy: 0.7500\n",
      "Epoch 00163: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4368 - accuracy: 0.7315 - val_loss: 0.4806 - val_accuracy: 0.7157\n",
      "Epoch 164/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4972 - accuracy: 0.6562\n",
      "Epoch 00164: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4418 - accuracy: 0.7438 - val_loss: 0.4850 - val_accuracy: 0.7157\n",
      "Epoch 165/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4525 - accuracy: 0.8125\n",
      "Epoch 00165: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4624 - accuracy: 0.7365 - val_loss: 0.4848 - val_accuracy: 0.7157\n",
      "Epoch 166/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3357 - accuracy: 0.8438\n",
      "Epoch 00166: val_loss did not improve from 0.48051\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4251 - accuracy: 0.7488 - val_loss: 0.4847 - val_accuracy: 0.7157\n",
      "Epoch 167/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5031 - accuracy: 0.6875\n",
      "Epoch 00167: val_loss improved from 0.48051 to 0.48009, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7463 - val_loss: 0.4801 - val_accuracy: 0.7157\n",
      "Epoch 168/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4946 - accuracy: 0.6562\n",
      "Epoch 00168: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.7562 - val_loss: 0.4844 - val_accuracy: 0.7157\n",
      "Epoch 169/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4995 - accuracy: 0.7812\n",
      "Epoch 00169: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4179 - accuracy: 0.7537 - val_loss: 0.4828 - val_accuracy: 0.7157\n",
      "Epoch 170/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4681 - accuracy: 0.6562\n",
      "Epoch 00170: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4409 - accuracy: 0.7315 - val_loss: 0.4847 - val_accuracy: 0.7157\n",
      "Epoch 171/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4041 - accuracy: 0.7500\n",
      "Epoch 00171: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4196 - accuracy: 0.7414 - val_loss: 0.4865 - val_accuracy: 0.7157\n",
      "Epoch 172/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4781 - accuracy: 0.6875\n",
      "Epoch 00172: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.7512 - val_loss: 0.4900 - val_accuracy: 0.7157\n",
      "Epoch 173/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6521 - accuracy: 0.7188\n",
      "Epoch 00173: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4413 - accuracy: 0.7266 - val_loss: 0.4897 - val_accuracy: 0.7157\n",
      "Epoch 174/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5537 - accuracy: 0.7812\n",
      "Epoch 00174: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4443 - accuracy: 0.7217 - val_loss: 0.4899 - val_accuracy: 0.7157\n",
      "Epoch 175/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5402 - accuracy: 0.5938\n",
      "Epoch 00175: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4217 - accuracy: 0.7217 - val_loss: 0.4961 - val_accuracy: 0.7157\n",
      "Epoch 176/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5376 - accuracy: 0.6875\n",
      "Epoch 00176: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.7488 - val_loss: 0.4894 - val_accuracy: 0.7157\n",
      "Epoch 177/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4277 - accuracy: 0.7500\n",
      "Epoch 00177: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4427 - accuracy: 0.7562 - val_loss: 0.4907 - val_accuracy: 0.7157\n",
      "Epoch 178/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4798 - accuracy: 0.7188\n",
      "Epoch 00178: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4410 - accuracy: 0.7463 - val_loss: 0.4968 - val_accuracy: 0.7157\n",
      "Epoch 179/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3411 - accuracy: 0.7812\n",
      "Epoch 00179: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4043 - accuracy: 0.7389 - val_loss: 0.4999 - val_accuracy: 0.7157\n",
      "Epoch 180/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.8055 - accuracy: 0.5312\n",
      "Epoch 00180: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4257 - accuracy: 0.7438 - val_loss: 0.4923 - val_accuracy: 0.7157\n",
      "Epoch 181/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3676 - accuracy: 0.7812\n",
      "Epoch 00181: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4204 - accuracy: 0.7734 - val_loss: 0.4914 - val_accuracy: 0.7157\n",
      "Epoch 182/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3891 - accuracy: 0.7812\n",
      "Epoch 00182: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4073 - accuracy: 0.7783 - val_loss: 0.4907 - val_accuracy: 0.7451\n",
      "Epoch 183/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4526 - accuracy: 0.7500\n",
      "Epoch 00183: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4166 - accuracy: 0.7586 - val_loss: 0.4936 - val_accuracy: 0.7451\n",
      "Epoch 184/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3991 - accuracy: 0.7812\n",
      "Epoch 00184: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.7315 - val_loss: 0.4852 - val_accuracy: 0.7451\n",
      "Epoch 185/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2422 - accuracy: 0.8750\n",
      "Epoch 00185: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4134 - accuracy: 0.7586 - val_loss: 0.4855 - val_accuracy: 0.7451\n",
      "Epoch 186/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3594 - accuracy: 0.7500\n",
      "Epoch 00186: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4119 - accuracy: 0.7734 - val_loss: 0.4851 - val_accuracy: 0.7451\n",
      "Epoch 187/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4476 - accuracy: 0.7188\n",
      "Epoch 00187: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4079 - accuracy: 0.7635 - val_loss: 0.4897 - val_accuracy: 0.7353\n",
      "Epoch 188/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3835 - accuracy: 0.7812\n",
      "Epoch 00188: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4227 - accuracy: 0.7685 - val_loss: 0.4884 - val_accuracy: 0.7353\n",
      "Epoch 189/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3287 - accuracy: 0.7812\n",
      "Epoch 00189: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4136 - accuracy: 0.7635 - val_loss: 0.4959 - val_accuracy: 0.7549\n",
      "Epoch 190/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3218 - accuracy: 0.8438\n",
      "Epoch 00190: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4083 - accuracy: 0.7709 - val_loss: 0.4865 - val_accuracy: 0.7647\n",
      "Epoch 191/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3601 - accuracy: 0.7812\n",
      "Epoch 00191: val_loss did not improve from 0.48009\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3831 - accuracy: 0.7759 - val_loss: 0.4868 - val_accuracy: 0.7745\n",
      "Epoch 192/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6891 - accuracy: 0.8125\n",
      "Epoch 00192: val_loss improved from 0.48009 to 0.47994, saving model to saved_models\\audio_classification.hdf5\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4440 - accuracy: 0.7438 - val_loss: 0.4799 - val_accuracy: 0.7745\n",
      "Epoch 193/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3422 - accuracy: 0.8125\n",
      "Epoch 00193: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4045 - accuracy: 0.7562 - val_loss: 0.4871 - val_accuracy: 0.7255\n",
      "Epoch 194/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4574 - accuracy: 0.5938\n",
      "Epoch 00194: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4187 - accuracy: 0.7635 - val_loss: 0.4930 - val_accuracy: 0.7255\n",
      "Epoch 195/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5679 - accuracy: 0.6250\n",
      "Epoch 00195: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.4211 - accuracy: 0.7611 - val_loss: 0.4923 - val_accuracy: 0.7255\n",
      "Epoch 196/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3546 - accuracy: 0.7812\n",
      "Epoch 00196: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3810 - accuracy: 0.7660 - val_loss: 0.5000 - val_accuracy: 0.7353\n",
      "Epoch 197/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3893 - accuracy: 0.7812\n",
      "Epoch 00197: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4002 - accuracy: 0.7660 - val_loss: 0.5018 - val_accuracy: 0.7451\n",
      "Epoch 198/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4755 - accuracy: 0.7188\n",
      "Epoch 00198: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4099 - accuracy: 0.7734 - val_loss: 0.5069 - val_accuracy: 0.7353\n",
      "Epoch 199/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3158 - accuracy: 0.7812\n",
      "Epoch 00199: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.7660 - val_loss: 0.5006 - val_accuracy: 0.7745\n",
      "Epoch 200/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3479 - accuracy: 0.7812\n",
      "Epoch 00200: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.7635 - val_loss: 0.4964 - val_accuracy: 0.7843\n",
      "Epoch 201/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3660 - accuracy: 0.8438\n",
      "Epoch 00201: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3876 - accuracy: 0.7635 - val_loss: 0.4980 - val_accuracy: 0.7843\n",
      "Epoch 202/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3190 - accuracy: 0.8125\n",
      "Epoch 00202: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3924 - accuracy: 0.7931 - val_loss: 0.5047 - val_accuracy: 0.7843\n",
      "Epoch 203/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3505 - accuracy: 0.7812\n",
      "Epoch 00203: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3753 - accuracy: 0.7611 - val_loss: 0.5022 - val_accuracy: 0.7647\n",
      "Epoch 204/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3779 - accuracy: 0.7188\n",
      "Epoch 00204: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3842 - accuracy: 0.7611 - val_loss: 0.5003 - val_accuracy: 0.7647\n",
      "Epoch 205/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4443 - accuracy: 0.7188\n",
      "Epoch 00205: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3872 - accuracy: 0.7931 - val_loss: 0.4978 - val_accuracy: 0.7745\n",
      "Epoch 206/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2205 - accuracy: 0.9375\n",
      "Epoch 00206: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3763 - accuracy: 0.7808 - val_loss: 0.5094 - val_accuracy: 0.7745\n",
      "Epoch 207/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3944 - accuracy: 0.7812\n",
      "Epoch 00207: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3641 - accuracy: 0.8079 - val_loss: 0.4977 - val_accuracy: 0.7451\n",
      "Epoch 208/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4156 - accuracy: 0.8125\n",
      "Epoch 00208: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3982 - accuracy: 0.7808 - val_loss: 0.5086 - val_accuracy: 0.7549\n",
      "Epoch 209/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4135 - accuracy: 0.7500\n",
      "Epoch 00209: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3849 - accuracy: 0.7808 - val_loss: 0.4952 - val_accuracy: 0.7647\n",
      "Epoch 210/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2973 - accuracy: 0.8125\n",
      "Epoch 00210: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3821 - accuracy: 0.7537 - val_loss: 0.5033 - val_accuracy: 0.7549\n",
      "Epoch 211/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3123 - accuracy: 0.8750\n",
      "Epoch 00211: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3840 - accuracy: 0.7882 - val_loss: 0.5106 - val_accuracy: 0.7843\n",
      "Epoch 212/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2746 - accuracy: 0.7812\n",
      "Epoch 00212: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.7709 - val_loss: 0.5222 - val_accuracy: 0.7745\n",
      "Epoch 213/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3064 - accuracy: 0.7812\n",
      "Epoch 00213: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3896 - accuracy: 0.7685 - val_loss: 0.5098 - val_accuracy: 0.7745\n",
      "Epoch 214/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4017 - accuracy: 0.6875\n",
      "Epoch 00214: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3708 - accuracy: 0.7759 - val_loss: 0.5111 - val_accuracy: 0.7745\n",
      "Epoch 215/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4485 - accuracy: 0.7500\n",
      "Epoch 00215: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3689 - accuracy: 0.7635 - val_loss: 0.5182 - val_accuracy: 0.7745\n",
      "Epoch 216/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4076 - accuracy: 0.7188\n",
      "Epoch 00216: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.7611 - val_loss: 0.5164 - val_accuracy: 0.7745\n",
      "Epoch 217/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4541 - accuracy: 0.6875\n",
      "Epoch 00217: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.7734 - val_loss: 0.5068 - val_accuracy: 0.7647\n",
      "Epoch 218/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4291 - accuracy: 0.7812\n",
      "Epoch 00218: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3545 - accuracy: 0.8054 - val_loss: 0.5074 - val_accuracy: 0.7745\n",
      "Epoch 219/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4959 - accuracy: 0.6875\n",
      "Epoch 00219: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3769 - accuracy: 0.7906 - val_loss: 0.5069 - val_accuracy: 0.7941\n",
      "Epoch 220/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2719 - accuracy: 0.8750\n",
      "Epoch 00220: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3442 - accuracy: 0.7882 - val_loss: 0.5416 - val_accuracy: 0.7745\n",
      "Epoch 221/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3168 - accuracy: 0.7500\n",
      "Epoch 00221: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.7734 - val_loss: 0.5501 - val_accuracy: 0.7647\n",
      "Epoch 222/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2472 - accuracy: 0.8750\n",
      "Epoch 00222: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3668 - accuracy: 0.7833 - val_loss: 0.5217 - val_accuracy: 0.7549\n",
      "Epoch 223/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3410 - accuracy: 0.7812\n",
      "Epoch 00223: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3681 - accuracy: 0.7808 - val_loss: 0.5268 - val_accuracy: 0.7745\n",
      "Epoch 224/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4696 - accuracy: 0.7188\n",
      "Epoch 00224: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3787 - accuracy: 0.7635 - val_loss: 0.5080 - val_accuracy: 0.7941\n",
      "Epoch 225/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2432 - accuracy: 0.8750\n",
      "Epoch 00225: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.7734 - val_loss: 0.5184 - val_accuracy: 0.7745\n",
      "Epoch 226/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3495 - accuracy: 0.8125\n",
      "Epoch 00226: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.7980 - val_loss: 0.5344 - val_accuracy: 0.7843\n",
      "Epoch 227/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3151 - accuracy: 0.8125\n",
      "Epoch 00227: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3859 - accuracy: 0.7783 - val_loss: 0.5214 - val_accuracy: 0.8039\n",
      "Epoch 228/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2960 - accuracy: 0.9062\n",
      "Epoch 00228: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3471 - accuracy: 0.8030 - val_loss: 0.5315 - val_accuracy: 0.8039\n",
      "Epoch 229/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4058 - accuracy: 0.8438\n",
      "Epoch 00229: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3636 - accuracy: 0.7980 - val_loss: 0.5174 - val_accuracy: 0.7745\n",
      "Epoch 230/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4064 - accuracy: 0.7188\n",
      "Epoch 00230: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3975 - accuracy: 0.7685 - val_loss: 0.5098 - val_accuracy: 0.7941\n",
      "Epoch 231/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3056 - accuracy: 0.7812\n",
      "Epoch 00231: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3707 - accuracy: 0.7808 - val_loss: 0.5188 - val_accuracy: 0.8137\n",
      "Epoch 232/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3333 - accuracy: 0.8125\n",
      "Epoch 00232: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3449 - accuracy: 0.7833 - val_loss: 0.5177 - val_accuracy: 0.8039\n",
      "Epoch 233/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2810 - accuracy: 0.8125\n",
      "Epoch 00233: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.3695 - accuracy: 0.8030 - val_loss: 0.4885 - val_accuracy: 0.7941\n",
      "Epoch 234/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3111 - accuracy: 0.8125\n",
      "Epoch 00234: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3571 - accuracy: 0.8103 - val_loss: 0.4884 - val_accuracy: 0.7745\n",
      "Epoch 235/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2732 - accuracy: 0.7812\n",
      "Epoch 00235: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3683 - accuracy: 0.8030 - val_loss: 0.4813 - val_accuracy: 0.7745\n",
      "Epoch 236/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3330 - accuracy: 0.7500\n",
      "Epoch 00236: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3647 - accuracy: 0.7783 - val_loss: 0.4920 - val_accuracy: 0.8039\n",
      "Epoch 237/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3257 - accuracy: 0.7500\n",
      "Epoch 00237: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3567 - accuracy: 0.8005 - val_loss: 0.5160 - val_accuracy: 0.7843\n",
      "Epoch 238/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3398 - accuracy: 0.9062\n",
      "Epoch 00238: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3455 - accuracy: 0.7956 - val_loss: 0.5016 - val_accuracy: 0.7843\n",
      "Epoch 239/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2364 - accuracy: 0.8750\n",
      "Epoch 00239: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3697 - accuracy: 0.7759 - val_loss: 0.4800 - val_accuracy: 0.7843\n",
      "Epoch 240/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3164 - accuracy: 0.8125\n",
      "Epoch 00240: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3452 - accuracy: 0.8079 - val_loss: 0.5155 - val_accuracy: 0.8039\n",
      "Epoch 241/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2930 - accuracy: 0.7500\n",
      "Epoch 00241: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3586 - accuracy: 0.7882 - val_loss: 0.5104 - val_accuracy: 0.7647\n",
      "Epoch 242/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3023 - accuracy: 0.7812\n",
      "Epoch 00242: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3648 - accuracy: 0.7709 - val_loss: 0.5252 - val_accuracy: 0.7941\n",
      "Epoch 243/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4012 - accuracy: 0.7188\n",
      "Epoch 00243: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3372 - accuracy: 0.8030 - val_loss: 0.5405 - val_accuracy: 0.7843\n",
      "Epoch 244/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2815 - accuracy: 0.8125\n",
      "Epoch 00244: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3259 - accuracy: 0.8276 - val_loss: 0.5451 - val_accuracy: 0.7843\n",
      "Epoch 245/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5013 - accuracy: 0.7188\n",
      "Epoch 00245: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3602 - accuracy: 0.8251 - val_loss: 0.5461 - val_accuracy: 0.7745\n",
      "Epoch 246/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2918 - accuracy: 0.7812\n",
      "Epoch 00246: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3625 - accuracy: 0.8054 - val_loss: 0.5104 - val_accuracy: 0.7843\n",
      "Epoch 247/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3143 - accuracy: 0.8438\n",
      "Epoch 00247: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8128 - val_loss: 0.5007 - val_accuracy: 0.7843\n",
      "Epoch 248/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2225 - accuracy: 0.8438\n",
      "Epoch 00248: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3353 - accuracy: 0.8227 - val_loss: 0.5109 - val_accuracy: 0.7843\n",
      "Epoch 249/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3240 - accuracy: 0.7812\n",
      "Epoch 00249: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3320 - accuracy: 0.8103 - val_loss: 0.5089 - val_accuracy: 0.7941\n",
      "Epoch 250/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2842 - accuracy: 0.8750\n",
      "Epoch 00250: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3414 - accuracy: 0.8030 - val_loss: 0.4979 - val_accuracy: 0.8039\n",
      "Epoch 251/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3620 - accuracy: 0.8438\n",
      "Epoch 00251: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3296 - accuracy: 0.8350 - val_loss: 0.5080 - val_accuracy: 0.7843\n",
      "Epoch 252/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2645 - accuracy: 0.8438\n",
      "Epoch 00252: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3235 - accuracy: 0.8276 - val_loss: 0.5520 - val_accuracy: 0.7745\n",
      "Epoch 253/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3711 - accuracy: 0.7500\n",
      "Epoch 00253: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.8030 - val_loss: 0.5212 - val_accuracy: 0.7941\n",
      "Epoch 254/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3655 - accuracy: 0.8438\n",
      "Epoch 00254: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3460 - accuracy: 0.8227 - val_loss: 0.5082 - val_accuracy: 0.8039\n",
      "Epoch 255/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3384 - accuracy: 0.8438\n",
      "Epoch 00255: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3106 - accuracy: 0.8325 - val_loss: 0.5603 - val_accuracy: 0.7941\n",
      "Epoch 256/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4452 - accuracy: 0.7812\n",
      "Epoch 00256: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.8399 - val_loss: 0.4930 - val_accuracy: 0.8039\n",
      "Epoch 257/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2576 - accuracy: 0.8750\n",
      "Epoch 00257: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3388 - accuracy: 0.7808 - val_loss: 0.5084 - val_accuracy: 0.7843\n",
      "Epoch 258/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3714 - accuracy: 0.8750\n",
      "Epoch 00258: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3512 - accuracy: 0.8030 - val_loss: 0.4824 - val_accuracy: 0.7745\n",
      "Epoch 259/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2826 - accuracy: 0.8125\n",
      "Epoch 00259: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3491 - accuracy: 0.7833 - val_loss: 0.4858 - val_accuracy: 0.7941\n",
      "Epoch 260/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2736 - accuracy: 0.8750\n",
      "Epoch 00260: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.2896 - accuracy: 0.8399 - val_loss: 0.5588 - val_accuracy: 0.7647\n",
      "Epoch 261/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3303 - accuracy: 0.7500\n",
      "Epoch 00261: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.8128 - val_loss: 0.5425 - val_accuracy: 0.7745\n",
      "Epoch 262/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2680 - accuracy: 0.8438\n",
      "Epoch 00262: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8202 - val_loss: 0.5430 - val_accuracy: 0.7843\n",
      "Epoch 263/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3254 - accuracy: 0.7812\n",
      "Epoch 00263: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3221 - accuracy: 0.8054 - val_loss: 0.5511 - val_accuracy: 0.7647\n",
      "Epoch 264/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2958 - accuracy: 0.8750\n",
      "Epoch 00264: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8276 - val_loss: 0.5670 - val_accuracy: 0.7451\n",
      "Epoch 265/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2123 - accuracy: 0.9375\n",
      "Epoch 00265: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3219 - accuracy: 0.8177 - val_loss: 0.6018 - val_accuracy: 0.8235\n",
      "Epoch 266/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3225 - accuracy: 0.8438\n",
      "Epoch 00266: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8522 - val_loss: 0.5585 - val_accuracy: 0.8137\n",
      "Epoch 267/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2956 - accuracy: 0.8438\n",
      "Epoch 00267: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3011 - accuracy: 0.8325 - val_loss: 0.5915 - val_accuracy: 0.7647\n",
      "Epoch 268/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2667 - accuracy: 0.8438\n",
      "Epoch 00268: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3389 - accuracy: 0.8251 - val_loss: 0.5620 - val_accuracy: 0.7647\n",
      "Epoch 269/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3016 - accuracy: 0.8750\n",
      "Epoch 00269: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3363 - accuracy: 0.8276 - val_loss: 0.5194 - val_accuracy: 0.7745\n",
      "Epoch 270/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2919 - accuracy: 0.8438\n",
      "Epoch 00270: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.8177 - val_loss: 0.4971 - val_accuracy: 0.7843\n",
      "Epoch 271/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.6444 - accuracy: 0.7500\n",
      "Epoch 00271: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3388 - accuracy: 0.8374 - val_loss: 0.5218 - val_accuracy: 0.7745\n",
      "Epoch 272/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3814 - accuracy: 0.7500\n",
      "Epoch 00272: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.2987 - accuracy: 0.8547 - val_loss: 0.5369 - val_accuracy: 0.7745\n",
      "Epoch 273/600\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8424\n",
      "Epoch 00273: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.3302 - accuracy: 0.8424 - val_loss: 0.5213 - val_accuracy: 0.7451\n",
      "Epoch 274/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3503 - accuracy: 0.8750\n",
      "Epoch 00274: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8177 - val_loss: 0.5786 - val_accuracy: 0.7353\n",
      "Epoch 275/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1739 - accuracy: 0.8750\n",
      "Epoch 00275: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3092 - accuracy: 0.8350 - val_loss: 0.5622 - val_accuracy: 0.7647\n",
      "Epoch 276/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1745 - accuracy: 0.9688\n",
      "Epoch 00276: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3086 - accuracy: 0.8448 - val_loss: 0.5779 - val_accuracy: 0.7647\n",
      "Epoch 277/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3756 - accuracy: 0.7500\n",
      "Epoch 00277: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3068 - accuracy: 0.8424 - val_loss: 0.5321 - val_accuracy: 0.7647\n",
      "Epoch 278/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2796 - accuracy: 0.8125\n",
      "Epoch 00278: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3022 - accuracy: 0.8424 - val_loss: 0.5435 - val_accuracy: 0.7745\n",
      "Epoch 279/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3512 - accuracy: 0.7500\n",
      "Epoch 00279: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.3380 - accuracy: 0.8177 - val_loss: 0.5504 - val_accuracy: 0.7843\n",
      "Epoch 280/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2536 - accuracy: 0.8125\n",
      "Epoch 00280: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.8202 - val_loss: 0.5567 - val_accuracy: 0.7843\n",
      "Epoch 281/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3156 - accuracy: 0.9062\n",
      "Epoch 00281: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8202 - val_loss: 0.5076 - val_accuracy: 0.7647\n",
      "Epoch 282/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3329 - accuracy: 0.8125\n",
      "Epoch 00282: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2942 - accuracy: 0.8374 - val_loss: 0.5131 - val_accuracy: 0.7745\n",
      "Epoch 283/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2790 - accuracy: 0.8750\n",
      "Epoch 00283: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8547 - val_loss: 0.5213 - val_accuracy: 0.7941\n",
      "Epoch 284/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3725 - accuracy: 0.7188\n",
      "Epoch 00284: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8399 - val_loss: 0.5210 - val_accuracy: 0.8039\n",
      "Epoch 285/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3622 - accuracy: 0.8125\n",
      "Epoch 00285: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2968 - accuracy: 0.8399 - val_loss: 0.5303 - val_accuracy: 0.8039\n",
      "Epoch 286/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1435 - accuracy: 0.9375\n",
      "Epoch 00286: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2768 - accuracy: 0.8547 - val_loss: 0.5680 - val_accuracy: 0.8039\n",
      "Epoch 287/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1521 - accuracy: 0.9688\n",
      "Epoch 00287: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.8596 - val_loss: 0.5517 - val_accuracy: 0.8137\n",
      "Epoch 288/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2042 - accuracy: 0.8438\n",
      "Epoch 00288: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2542 - accuracy: 0.8645 - val_loss: 0.6062 - val_accuracy: 0.8333\n",
      "Epoch 289/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4435 - accuracy: 0.8125\n",
      "Epoch 00289: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3583 - accuracy: 0.8300 - val_loss: 0.5195 - val_accuracy: 0.7941\n",
      "Epoch 290/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2075 - accuracy: 0.8750\n",
      "Epoch 00290: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2884 - accuracy: 0.8424 - val_loss: 0.5734 - val_accuracy: 0.7941\n",
      "Epoch 291/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2708 - accuracy: 0.8438\n",
      "Epoch 00291: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8448 - val_loss: 0.5289 - val_accuracy: 0.7745\n",
      "Epoch 292/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2499 - accuracy: 0.8750\n",
      "Epoch 00292: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2983 - accuracy: 0.8498 - val_loss: 0.5449 - val_accuracy: 0.7941\n",
      "Epoch 293/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3140 - accuracy: 0.8438\n",
      "Epoch 00293: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2921 - accuracy: 0.8473 - val_loss: 0.5922 - val_accuracy: 0.7647\n",
      "Epoch 294/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5680 - accuracy: 0.8125\n",
      "Epoch 00294: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8768 - val_loss: 0.5360 - val_accuracy: 0.7843\n",
      "Epoch 295/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3293 - accuracy: 0.9062\n",
      "Epoch 00295: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.2780 - accuracy: 0.8547 - val_loss: 0.5610 - val_accuracy: 0.8039\n",
      "Epoch 296/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2335 - accuracy: 0.8750\n",
      "Epoch 00296: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2795 - accuracy: 0.8645 - val_loss: 0.5442 - val_accuracy: 0.7843\n",
      "Epoch 297/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2544 - accuracy: 0.8750\n",
      "Epoch 00297: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2884 - accuracy: 0.8621 - val_loss: 0.5616 - val_accuracy: 0.8039\n",
      "Epoch 298/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3207 - accuracy: 0.8750\n",
      "Epoch 00298: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2899 - accuracy: 0.8498 - val_loss: 0.5498 - val_accuracy: 0.7843\n",
      "Epoch 299/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3184 - accuracy: 0.8750\n",
      "Epoch 00299: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2686 - accuracy: 0.8670 - val_loss: 0.5524 - val_accuracy: 0.7549\n",
      "Epoch 300/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4541 - accuracy: 0.7812\n",
      "Epoch 00300: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2637 - accuracy: 0.8670 - val_loss: 0.5495 - val_accuracy: 0.7549\n",
      "Epoch 301/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4056 - accuracy: 0.7812\n",
      "Epoch 00301: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2603 - accuracy: 0.9015 - val_loss: 0.5515 - val_accuracy: 0.7941\n",
      "Epoch 302/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3182 - accuracy: 0.7812\n",
      "Epoch 00302: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2351 - accuracy: 0.8892 - val_loss: 0.6132 - val_accuracy: 0.7647\n",
      "Epoch 303/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1704 - accuracy: 0.9375\n",
      "Epoch 00303: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.8818 - val_loss: 0.5331 - val_accuracy: 0.7647\n",
      "Epoch 304/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2466 - accuracy: 0.8438\n",
      "Epoch 00304: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8695 - val_loss: 0.5983 - val_accuracy: 0.7843\n",
      "Epoch 305/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1944 - accuracy: 0.9062\n",
      "Epoch 00305: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8670 - val_loss: 0.5745 - val_accuracy: 0.7843\n",
      "Epoch 306/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2661 - accuracy: 0.9062\n",
      "Epoch 00306: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2483 - accuracy: 0.8744 - val_loss: 0.5645 - val_accuracy: 0.7843\n",
      "Epoch 307/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3597 - accuracy: 0.8438\n",
      "Epoch 00307: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2559 - accuracy: 0.8818 - val_loss: 0.5304 - val_accuracy: 0.8137\n",
      "Epoch 308/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2571 - accuracy: 0.8750\n",
      "Epoch 00308: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8695 - val_loss: 0.4906 - val_accuracy: 0.8137\n",
      "Epoch 309/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2024 - accuracy: 0.9062\n",
      "Epoch 00309: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.8768 - val_loss: 0.5094 - val_accuracy: 0.7941\n",
      "Epoch 310/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1564 - accuracy: 0.9375\n",
      "Epoch 00310: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.8645 - val_loss: 0.5057 - val_accuracy: 0.7941\n",
      "Epoch 311/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3048 - accuracy: 0.8438\n",
      "Epoch 00311: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.8695 - val_loss: 0.5745 - val_accuracy: 0.7647\n",
      "Epoch 312/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2426 - accuracy: 0.9375\n",
      "Epoch 00312: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.8498 - val_loss: 0.5492 - val_accuracy: 0.7745\n",
      "Epoch 313/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2103 - accuracy: 0.8750\n",
      "Epoch 00313: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.8596 - val_loss: 0.5388 - val_accuracy: 0.7843\n",
      "Epoch 314/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3542 - accuracy: 0.8750\n",
      "Epoch 00314: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.8498 - val_loss: 0.5783 - val_accuracy: 0.8235\n",
      "Epoch 315/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2310 - accuracy: 0.8438\n",
      "Epoch 00315: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2773 - accuracy: 0.8571 - val_loss: 0.5157 - val_accuracy: 0.8039\n",
      "Epoch 316/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2495 - accuracy: 0.8750\n",
      "Epoch 00316: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.8768 - val_loss: 0.6085 - val_accuracy: 0.8039\n",
      "Epoch 317/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2985 - accuracy: 0.8438\n",
      "Epoch 00317: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2646 - accuracy: 0.8670 - val_loss: 0.5674 - val_accuracy: 0.7843\n",
      "Epoch 318/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2292 - accuracy: 0.8750\n",
      "Epoch 00318: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2473 - accuracy: 0.8793 - val_loss: 0.6148 - val_accuracy: 0.8333\n",
      "Epoch 319/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1778 - accuracy: 0.9375\n",
      "Epoch 00319: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2981 - accuracy: 0.8522 - val_loss: 0.5020 - val_accuracy: 0.8137\n",
      "Epoch 320/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2836 - accuracy: 0.8125\n",
      "Epoch 00320: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2609 - accuracy: 0.8793 - val_loss: 0.5901 - val_accuracy: 0.8333\n",
      "Epoch 321/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2525 - accuracy: 0.9062\n",
      "Epoch 00321: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.8768 - val_loss: 0.5193 - val_accuracy: 0.8235\n",
      "Epoch 322/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3055 - accuracy: 0.8438\n",
      "Epoch 00322: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2750 - accuracy: 0.8670 - val_loss: 0.5041 - val_accuracy: 0.8333\n",
      "Epoch 323/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2542 - accuracy: 0.9062\n",
      "Epoch 00323: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2345 - accuracy: 0.8941 - val_loss: 0.4932 - val_accuracy: 0.8235\n",
      "Epoch 324/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0953 - accuracy: 0.9688\n",
      "Epoch 00324: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.8916 - val_loss: 0.5142 - val_accuracy: 0.8333\n",
      "Epoch 325/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1978 - accuracy: 0.9062\n",
      "Epoch 00325: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2551 - accuracy: 0.8596 - val_loss: 0.5488 - val_accuracy: 0.8431\n",
      "Epoch 326/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3999 - accuracy: 0.8750\n",
      "Epoch 00326: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2595 - accuracy: 0.8793 - val_loss: 0.5186 - val_accuracy: 0.8039\n",
      "Epoch 327/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3338 - accuracy: 0.8125\n",
      "Epoch 00327: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2622 - accuracy: 0.8645 - val_loss: 0.5668 - val_accuracy: 0.8137\n",
      "Epoch 328/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4147 - accuracy: 0.7188\n",
      "Epoch 00328: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.8424 - val_loss: 0.4912 - val_accuracy: 0.8137\n",
      "Epoch 329/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3602 - accuracy: 0.7812\n",
      "Epoch 00329: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2640 - accuracy: 0.8596 - val_loss: 0.5221 - val_accuracy: 0.8235\n",
      "Epoch 330/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2198 - accuracy: 0.8750\n",
      "Epoch 00330: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2460 - accuracy: 0.8793 - val_loss: 0.5903 - val_accuracy: 0.8039\n",
      "Epoch 331/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2290 - accuracy: 0.8438\n",
      "Epoch 00331: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2142 - accuracy: 0.8966 - val_loss: 0.5560 - val_accuracy: 0.8235\n",
      "Epoch 332/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2789 - accuracy: 0.8750\n",
      "Epoch 00332: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2500 - accuracy: 0.8670 - val_loss: 0.5396 - val_accuracy: 0.8235\n",
      "Epoch 333/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2525 - accuracy: 0.9062\n",
      "Epoch 00333: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9039 - val_loss: 0.5870 - val_accuracy: 0.8333\n",
      "Epoch 334/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2379 - accuracy: 0.8438\n",
      "Epoch 00334: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2397 - accuracy: 0.8719 - val_loss: 0.5742 - val_accuracy: 0.8333\n",
      "Epoch 335/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2429 - accuracy: 0.8438\n",
      "Epoch 00335: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2474 - accuracy: 0.8768 - val_loss: 0.6879 - val_accuracy: 0.8039\n",
      "Epoch 336/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1495 - accuracy: 0.9062\n",
      "Epoch 00336: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2355 - accuracy: 0.8695 - val_loss: 0.6573 - val_accuracy: 0.7745\n",
      "Epoch 337/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1824 - accuracy: 0.8750\n",
      "Epoch 00337: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2484 - accuracy: 0.8793 - val_loss: 0.5808 - val_accuracy: 0.8235\n",
      "Epoch 338/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1700 - accuracy: 0.9688\n",
      "Epoch 00338: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.8966 - val_loss: 0.5109 - val_accuracy: 0.8137\n",
      "Epoch 339/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1321 - accuracy: 0.9375\n",
      "Epoch 00339: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2116 - accuracy: 0.8941 - val_loss: 0.5830 - val_accuracy: 0.8529\n",
      "Epoch 340/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1018 - accuracy: 0.9375\n",
      "Epoch 00340: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2074 - accuracy: 0.8941 - val_loss: 0.6182 - val_accuracy: 0.8529\n",
      "Epoch 341/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1858 - accuracy: 0.9062\n",
      "Epoch 00341: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2318 - accuracy: 0.8793 - val_loss: 0.5399 - val_accuracy: 0.8039\n",
      "Epoch 342/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3845 - accuracy: 0.7812\n",
      "Epoch 00342: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.8768 - val_loss: 0.6193 - val_accuracy: 0.8627\n",
      "Epoch 343/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1886 - accuracy: 0.9062\n",
      "Epoch 00343: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2477 - accuracy: 0.8892 - val_loss: 0.5595 - val_accuracy: 0.8529\n",
      "Epoch 344/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2230 - accuracy: 0.9062\n",
      "Epoch 00344: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2362 - accuracy: 0.8842 - val_loss: 0.5366 - val_accuracy: 0.8431\n",
      "Epoch 345/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2813 - accuracy: 0.8438\n",
      "Epoch 00345: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8695 - val_loss: 0.6095 - val_accuracy: 0.8235\n",
      "Epoch 346/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1656 - accuracy: 0.9062\n",
      "Epoch 00346: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2020 - accuracy: 0.8966 - val_loss: 0.6221 - val_accuracy: 0.8235\n",
      "Epoch 347/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2291 - accuracy: 0.9062\n",
      "Epoch 00347: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.8990 - val_loss: 0.6109 - val_accuracy: 0.8431\n",
      "Epoch 348/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2757 - accuracy: 0.8438\n",
      "Epoch 00348: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2359 - accuracy: 0.9015 - val_loss: 0.5778 - val_accuracy: 0.8333\n",
      "Epoch 349/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3467 - accuracy: 0.7812\n",
      "Epoch 00349: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.8596 - val_loss: 0.5057 - val_accuracy: 0.8235\n",
      "Epoch 350/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2436 - accuracy: 0.9062\n",
      "Epoch 00350: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.8768 - val_loss: 0.5631 - val_accuracy: 0.8235\n",
      "Epoch 351/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1747 - accuracy: 0.9375\n",
      "Epoch 00351: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.8941 - val_loss: 0.5265 - val_accuracy: 0.8137\n",
      "Epoch 352/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1335 - accuracy: 0.9062\n",
      "Epoch 00352: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.8818 - val_loss: 0.5631 - val_accuracy: 0.8627\n",
      "Epoch 353/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1854 - accuracy: 0.9688\n",
      "Epoch 00353: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.8892 - val_loss: 0.6091 - val_accuracy: 0.8235\n",
      "Epoch 354/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2348 - accuracy: 0.8125\n",
      "Epoch 00354: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.8867 - val_loss: 0.5906 - val_accuracy: 0.8333\n",
      "Epoch 355/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4096 - accuracy: 0.8750\n",
      "Epoch 00355: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2610 - accuracy: 0.8966 - val_loss: 0.5653 - val_accuracy: 0.8333\n",
      "Epoch 356/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2023 - accuracy: 0.9062\n",
      "Epoch 00356: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2214 - accuracy: 0.8842 - val_loss: 0.6366 - val_accuracy: 0.8137\n",
      "Epoch 357/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0664 - accuracy: 1.0000\n",
      "Epoch 00357: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9187 - val_loss: 0.5851 - val_accuracy: 0.8529\n",
      "Epoch 358/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1217 - accuracy: 0.9062\n",
      "Epoch 00358: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2278 - accuracy: 0.8695 - val_loss: 0.5747 - val_accuracy: 0.8235\n",
      "Epoch 359/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1363 - accuracy: 0.9375\n",
      "Epoch 00359: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9163 - val_loss: 0.5893 - val_accuracy: 0.8529\n",
      "Epoch 360/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2387 - accuracy: 0.8438\n",
      "Epoch 00360: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9064 - val_loss: 0.7261 - val_accuracy: 0.8529\n",
      "Epoch 361/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2573 - accuracy: 0.8750\n",
      "Epoch 00361: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2081 - accuracy: 0.9039 - val_loss: 0.6247 - val_accuracy: 0.8333\n",
      "Epoch 362/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3808 - accuracy: 0.8438\n",
      "Epoch 00362: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.8818 - val_loss: 0.5996 - val_accuracy: 0.8627\n",
      "Epoch 363/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2853 - accuracy: 0.8438\n",
      "Epoch 00363: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2119 - accuracy: 0.9064 - val_loss: 0.6252 - val_accuracy: 0.8431\n",
      "Epoch 364/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0520 - accuracy: 1.0000\n",
      "Epoch 00364: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9212 - val_loss: 0.6343 - val_accuracy: 0.8431\n",
      "Epoch 365/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1750 - accuracy: 0.9375\n",
      "Epoch 00365: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2236 - accuracy: 0.8867 - val_loss: 0.5382 - val_accuracy: 0.8529\n",
      "Epoch 366/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1555 - accuracy: 1.0000\n",
      "Epoch 00366: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9089 - val_loss: 0.5707 - val_accuracy: 0.8529\n",
      "Epoch 367/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1387 - accuracy: 0.9375\n",
      "Epoch 00367: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.8892 - val_loss: 0.5487 - val_accuracy: 0.8431\n",
      "Epoch 368/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1468 - accuracy: 0.9375\n",
      "Epoch 00368: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9089 - val_loss: 0.5343 - val_accuracy: 0.8431\n",
      "Epoch 369/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0975 - accuracy: 0.9375\n",
      "Epoch 00369: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.8916 - val_loss: 0.5498 - val_accuracy: 0.8529\n",
      "Epoch 370/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1415 - accuracy: 0.9375\n",
      "Epoch 00370: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2261 - accuracy: 0.8990 - val_loss: 0.5182 - val_accuracy: 0.8431\n",
      "Epoch 371/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2285 - accuracy: 0.8438\n",
      "Epoch 00371: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8719 - val_loss: 0.5340 - val_accuracy: 0.8627\n",
      "Epoch 372/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1574 - accuracy: 0.9375\n",
      "Epoch 00372: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2117 - accuracy: 0.8941 - val_loss: 0.5231 - val_accuracy: 0.8725\n",
      "Epoch 373/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2051 - accuracy: 0.9375\n",
      "Epoch 00373: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9015 - val_loss: 0.5411 - val_accuracy: 0.8824\n",
      "Epoch 374/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2102 - accuracy: 0.9375\n",
      "Epoch 00374: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.8966 - val_loss: 0.5421 - val_accuracy: 0.8529\n",
      "Epoch 375/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2213 - accuracy: 0.8438\n",
      "Epoch 00375: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.8916 - val_loss: 0.6463 - val_accuracy: 0.8137\n",
      "Epoch 376/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2451 - accuracy: 0.8750\n",
      "Epoch 00376: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.8892 - val_loss: 0.6728 - val_accuracy: 0.8235\n",
      "Epoch 377/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2114 - accuracy: 0.8750\n",
      "Epoch 00377: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.8695 - val_loss: 0.5672 - val_accuracy: 0.8333\n",
      "Epoch 378/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1924 - accuracy: 0.9375\n",
      "Epoch 00378: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.8990 - val_loss: 0.5806 - val_accuracy: 0.8333\n",
      "Epoch 379/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1388 - accuracy: 0.9375\n",
      "Epoch 00379: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2241 - accuracy: 0.8892 - val_loss: 0.5549 - val_accuracy: 0.8431\n",
      "Epoch 380/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1386 - accuracy: 0.9375\n",
      "Epoch 00380: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.8892 - val_loss: 0.5538 - val_accuracy: 0.8333\n",
      "Epoch 381/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1219 - accuracy: 0.9375\n",
      "Epoch 00381: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2028 - accuracy: 0.9163 - val_loss: 0.5912 - val_accuracy: 0.8431\n",
      "Epoch 382/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2248 - accuracy: 0.9375\n",
      "Epoch 00382: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9261 - val_loss: 0.6169 - val_accuracy: 0.8333\n",
      "Epoch 383/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1205 - accuracy: 0.9688\n",
      "Epoch 00383: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1912 - accuracy: 0.9113 - val_loss: 0.6266 - val_accuracy: 0.8431\n",
      "Epoch 384/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0927 - accuracy: 1.0000\n",
      "Epoch 00384: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2020 - accuracy: 0.9015 - val_loss: 0.5893 - val_accuracy: 0.8333\n",
      "Epoch 385/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1277 - accuracy: 0.9688\n",
      "Epoch 00385: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9015 - val_loss: 0.5736 - val_accuracy: 0.8529\n",
      "Epoch 386/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1866 - accuracy: 0.9375\n",
      "Epoch 00386: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1902 - accuracy: 0.9039 - val_loss: 0.5993 - val_accuracy: 0.8824\n",
      "Epoch 387/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2093 - accuracy: 0.8750\n",
      "Epoch 00387: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9187 - val_loss: 0.6791 - val_accuracy: 0.8725\n",
      "Epoch 388/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1553 - accuracy: 0.9375\n",
      "Epoch 00388: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1871 - accuracy: 0.9212 - val_loss: 0.5319 - val_accuracy: 0.8627\n",
      "Epoch 389/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1342 - accuracy: 0.9375\n",
      "Epoch 00389: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2022 - accuracy: 0.9039 - val_loss: 0.4871 - val_accuracy: 0.8333\n",
      "Epoch 390/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1090 - accuracy: 0.9375\n",
      "Epoch 00390: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9310 - val_loss: 0.6177 - val_accuracy: 0.8725\n",
      "Epoch 391/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2542 - accuracy: 0.9062\n",
      "Epoch 00391: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9015 - val_loss: 0.5528 - val_accuracy: 0.8431\n",
      "Epoch 392/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2768 - accuracy: 0.9062\n",
      "Epoch 00392: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9212 - val_loss: 0.5916 - val_accuracy: 0.8431\n",
      "Epoch 393/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1606 - accuracy: 0.9062\n",
      "Epoch 00393: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2134 - accuracy: 0.9015 - val_loss: 0.5410 - val_accuracy: 0.8039\n",
      "Epoch 394/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1832 - accuracy: 0.9375\n",
      "Epoch 00394: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1971 - accuracy: 0.9064 - val_loss: 0.6143 - val_accuracy: 0.8137\n",
      "Epoch 395/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2386 - accuracy: 0.9062\n",
      "Epoch 00395: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2330 - accuracy: 0.8892 - val_loss: 0.5466 - val_accuracy: 0.8725\n",
      "Epoch 396/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1465 - accuracy: 0.9688\n",
      "Epoch 00396: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9113 - val_loss: 0.6448 - val_accuracy: 0.8431\n",
      "Epoch 397/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2629 - accuracy: 0.9375\n",
      "Epoch 00397: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.8990 - val_loss: 0.5637 - val_accuracy: 0.8235\n",
      "Epoch 398/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1900 - accuracy: 0.8750\n",
      "Epoch 00398: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.9064 - val_loss: 0.5124 - val_accuracy: 0.8431\n",
      "Epoch 399/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0904 - accuracy: 0.9688\n",
      "Epoch 00399: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.9064 - val_loss: 0.5831 - val_accuracy: 0.8529\n",
      "Epoch 400/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1464 - accuracy: 0.8750\n",
      "Epoch 00400: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2458 - accuracy: 0.8670 - val_loss: 0.4971 - val_accuracy: 0.8333\n",
      "Epoch 401/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2269 - accuracy: 0.8125\n",
      "Epoch 00401: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1934 - accuracy: 0.9089 - val_loss: 0.5521 - val_accuracy: 0.8137\n",
      "Epoch 402/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2671 - accuracy: 0.9062\n",
      "Epoch 00402: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.8818 - val_loss: 0.6016 - val_accuracy: 0.8039\n",
      "Epoch 403/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2114 - accuracy: 0.9062\n",
      "Epoch 00403: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9113 - val_loss: 0.6292 - val_accuracy: 0.8235\n",
      "Epoch 404/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2534 - accuracy: 0.8438\n",
      "Epoch 00404: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9089 - val_loss: 0.5754 - val_accuracy: 0.8137\n",
      "Epoch 405/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2299 - accuracy: 0.8750\n",
      "Epoch 00405: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.9113 - val_loss: 0.6476 - val_accuracy: 0.8137\n",
      "Epoch 406/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3832 - accuracy: 0.7812\n",
      "Epoch 00406: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1980 - accuracy: 0.8916 - val_loss: 0.6216 - val_accuracy: 0.8431\n",
      "Epoch 407/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1944 - accuracy: 0.9062\n",
      "Epoch 00407: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2072 - accuracy: 0.8842 - val_loss: 0.7593 - val_accuracy: 0.8431\n",
      "Epoch 408/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0605 - accuracy: 1.0000\n",
      "Epoch 00408: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1897 - accuracy: 0.8941 - val_loss: 0.7628 - val_accuracy: 0.8529\n",
      "Epoch 409/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3243 - accuracy: 0.8125\n",
      "Epoch 00409: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1944 - accuracy: 0.9039 - val_loss: 0.6333 - val_accuracy: 0.8627\n",
      "Epoch 410/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1011 - accuracy: 0.9688\n",
      "Epoch 00410: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9064 - val_loss: 0.7040 - val_accuracy: 0.8627\n",
      "Epoch 411/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0631 - accuracy: 0.9688\n",
      "Epoch 00411: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9236 - val_loss: 0.6730 - val_accuracy: 0.8333\n",
      "Epoch 412/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2069 - accuracy: 0.9062\n",
      "Epoch 00412: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2028 - accuracy: 0.9015 - val_loss: 0.6507 - val_accuracy: 0.8529\n",
      "Epoch 413/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2135 - accuracy: 0.9062\n",
      "Epoch 00413: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.9212 - val_loss: 0.7218 - val_accuracy: 0.8627\n",
      "Epoch 414/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2264 - accuracy: 0.9062\n",
      "Epoch 00414: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9163 - val_loss: 0.7514 - val_accuracy: 0.8627\n",
      "Epoch 415/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1245 - accuracy: 0.9062\n",
      "Epoch 00415: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1771 - accuracy: 0.9113 - val_loss: 0.6550 - val_accuracy: 0.8529\n",
      "Epoch 416/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2797 - accuracy: 0.8438\n",
      "Epoch 00416: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1777 - accuracy: 0.9138 - val_loss: 0.6388 - val_accuracy: 0.8627\n",
      "Epoch 417/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2278 - accuracy: 0.8438\n",
      "Epoch 00417: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.8966 - val_loss: 0.6205 - val_accuracy: 0.7941\n",
      "Epoch 418/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3388 - accuracy: 0.8125\n",
      "Epoch 00418: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1950 - accuracy: 0.9015 - val_loss: 0.6587 - val_accuracy: 0.8137\n",
      "Epoch 419/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2587 - accuracy: 0.8125\n",
      "Epoch 00419: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2045 - accuracy: 0.8966 - val_loss: 0.7000 - val_accuracy: 0.8333\n",
      "Epoch 420/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1681 - accuracy: 0.9375\n",
      "Epoch 00420: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9113 - val_loss: 0.7117 - val_accuracy: 0.8824\n",
      "Epoch 421/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1090 - accuracy: 0.9688\n",
      "Epoch 00421: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9360 - val_loss: 0.7472 - val_accuracy: 0.8529\n",
      "Epoch 422/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1983 - accuracy: 0.9062\n",
      "Epoch 00422: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9015 - val_loss: 0.7510 - val_accuracy: 0.8235\n",
      "Epoch 423/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2218 - accuracy: 0.8750\n",
      "Epoch 00423: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9384 - val_loss: 0.7433 - val_accuracy: 0.8431\n",
      "Epoch 424/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1647 - accuracy: 0.9062\n",
      "Epoch 00424: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9089 - val_loss: 0.7563 - val_accuracy: 0.8627\n",
      "Epoch 425/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1578 - accuracy: 0.8750\n",
      "Epoch 00425: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1927 - accuracy: 0.8842 - val_loss: 0.6852 - val_accuracy: 0.8627\n",
      "Epoch 426/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1314 - accuracy: 0.9375\n",
      "Epoch 00426: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.9163 - val_loss: 0.7031 - val_accuracy: 0.8529\n",
      "Epoch 427/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1343 - accuracy: 0.9375\n",
      "Epoch 00427: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1483 - accuracy: 0.9360 - val_loss: 0.7640 - val_accuracy: 0.8333\n",
      "Epoch 428/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1337 - accuracy: 0.9375\n",
      "Epoch 00428: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1724 - accuracy: 0.9261 - val_loss: 0.7317 - val_accuracy: 0.8333\n",
      "Epoch 429/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2418 - accuracy: 0.8438\n",
      "Epoch 00429: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9187 - val_loss: 0.7231 - val_accuracy: 0.8529\n",
      "Epoch 430/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2734 - accuracy: 0.8750\n",
      "Epoch 00430: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1778 - accuracy: 0.9187 - val_loss: 0.7283 - val_accuracy: 0.8529\n",
      "Epoch 431/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1232 - accuracy: 0.9688\n",
      "Epoch 00431: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1601 - accuracy: 0.9163 - val_loss: 0.7987 - val_accuracy: 0.8627\n",
      "Epoch 432/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1343 - accuracy: 0.9375\n",
      "Epoch 00432: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2055 - accuracy: 0.9163 - val_loss: 0.7849 - val_accuracy: 0.8333\n",
      "Epoch 433/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1790 - accuracy: 0.9375\n",
      "Epoch 00433: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1940 - accuracy: 0.8990 - val_loss: 0.6924 - val_accuracy: 0.8431\n",
      "Epoch 434/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1833 - accuracy: 0.8750\n",
      "Epoch 00434: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9187 - val_loss: 0.7073 - val_accuracy: 0.8235\n",
      "Epoch 435/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1555 - accuracy: 0.8750\n",
      "Epoch 00435: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9064 - val_loss: 0.6601 - val_accuracy: 0.8529\n",
      "Epoch 436/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2376 - accuracy: 0.9375\n",
      "Epoch 00436: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1949 - accuracy: 0.9187 - val_loss: 0.6955 - val_accuracy: 0.8431\n",
      "Epoch 437/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1297 - accuracy: 0.9375\n",
      "Epoch 00437: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9212 - val_loss: 0.6653 - val_accuracy: 0.8529\n",
      "Epoch 438/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1205 - accuracy: 0.9688\n",
      "Epoch 00438: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9310 - val_loss: 0.6600 - val_accuracy: 0.8235\n",
      "Epoch 439/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1634 - accuracy: 0.9375\n",
      "Epoch 00439: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1767 - accuracy: 0.9212 - val_loss: 0.6883 - val_accuracy: 0.8627\n",
      "Epoch 440/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9062\n",
      "Epoch 00440: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9212 - val_loss: 0.7335 - val_accuracy: 0.8529\n",
      "Epoch 441/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2022 - accuracy: 0.8750\n",
      "Epoch 00441: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9310 - val_loss: 0.7122 - val_accuracy: 0.8333\n",
      "Epoch 442/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0747 - accuracy: 0.9688\n",
      "Epoch 00442: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9261 - val_loss: 0.6856 - val_accuracy: 0.8529\n",
      "Epoch 443/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 00443: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.9335 - val_loss: 0.6642 - val_accuracy: 0.8333\n",
      "Epoch 444/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1505 - accuracy: 0.9375\n",
      "Epoch 00444: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1227 - accuracy: 0.9483 - val_loss: 0.8155 - val_accuracy: 0.8725\n",
      "Epoch 445/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0958 - accuracy: 0.9375\n",
      "Epoch 00445: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1535 - accuracy: 0.9163 - val_loss: 0.6928 - val_accuracy: 0.8529\n",
      "Epoch 446/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1043 - accuracy: 0.9062\n",
      "Epoch 00446: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.9015 - val_loss: 0.6591 - val_accuracy: 0.8333\n",
      "Epoch 447/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1990 - accuracy: 0.9062\n",
      "Epoch 00447: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.9064 - val_loss: 0.6198 - val_accuracy: 0.8725\n",
      "Epoch 448/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1490 - accuracy: 0.9375\n",
      "Epoch 00448: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9187 - val_loss: 0.6609 - val_accuracy: 0.8431\n",
      "Epoch 449/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3262 - accuracy: 0.9688\n",
      "Epoch 00449: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9163 - val_loss: 0.6148 - val_accuracy: 0.8627\n",
      "Epoch 450/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1452 - accuracy: 0.9062\n",
      "Epoch 00450: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9015 - val_loss: 0.5535 - val_accuracy: 0.8333\n",
      "Epoch 451/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1123 - accuracy: 0.9688\n",
      "Epoch 00451: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9138 - val_loss: 0.6457 - val_accuracy: 0.8333\n",
      "Epoch 452/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1084 - accuracy: 0.9688\n",
      "Epoch 00452: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9310 - val_loss: 0.7154 - val_accuracy: 0.8235\n",
      "Epoch 453/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1342 - accuracy: 0.9375\n",
      "Epoch 00453: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9187 - val_loss: 0.6489 - val_accuracy: 0.7941\n",
      "Epoch 454/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1568 - accuracy: 0.9062\n",
      "Epoch 00454: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.8842 - val_loss: 0.6131 - val_accuracy: 0.7941\n",
      "Epoch 455/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0793 - accuracy: 0.9688\n",
      "Epoch 00455: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.8990 - val_loss: 0.5841 - val_accuracy: 0.8137\n",
      "Epoch 456/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3180 - accuracy: 0.8438\n",
      "Epoch 00456: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1937 - accuracy: 0.9163 - val_loss: 0.6072 - val_accuracy: 0.8235\n",
      "Epoch 457/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1966 - accuracy: 0.8438\n",
      "Epoch 00457: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.8842 - val_loss: 0.5831 - val_accuracy: 0.8137\n",
      "Epoch 458/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1362 - accuracy: 0.9062\n",
      "Epoch 00458: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.8916 - val_loss: 0.6820 - val_accuracy: 0.8137\n",
      "Epoch 459/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1752 - accuracy: 0.9375\n",
      "Epoch 00459: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9138 - val_loss: 0.6654 - val_accuracy: 0.8529\n",
      "Epoch 460/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2228 - accuracy: 0.8750\n",
      "Epoch 00460: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1591 - accuracy: 0.9212 - val_loss: 0.6834 - val_accuracy: 0.8431\n",
      "Epoch 461/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2017 - accuracy: 0.8750\n",
      "Epoch 00461: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9187 - val_loss: 0.6222 - val_accuracy: 0.8137\n",
      "Epoch 462/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1704 - accuracy: 0.9062\n",
      "Epoch 00462: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.9187 - val_loss: 0.6731 - val_accuracy: 0.8529\n",
      "Epoch 463/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1562 - accuracy: 0.9375\n",
      "Epoch 00463: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9286 - val_loss: 0.7026 - val_accuracy: 0.8333\n",
      "Epoch 464/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2632 - accuracy: 0.9375\n",
      "Epoch 00464: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9089 - val_loss: 0.7265 - val_accuracy: 0.8333\n",
      "Epoch 465/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.5475 - accuracy: 0.7812\n",
      "Epoch 00465: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9261 - val_loss: 0.5673 - val_accuracy: 0.8333\n",
      "Epoch 466/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0724 - accuracy: 0.9375\n",
      "Epoch 00466: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9286 - val_loss: 0.5817 - val_accuracy: 0.8333\n",
      "Epoch 467/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1526 - accuracy: 0.9375\n",
      "Epoch 00467: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1479 - accuracy: 0.9335 - val_loss: 0.5530 - val_accuracy: 0.8431\n",
      "Epoch 468/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1526 - accuracy: 0.9375\n",
      "Epoch 00468: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1797 - accuracy: 0.9187 - val_loss: 0.6335 - val_accuracy: 0.8529\n",
      "Epoch 469/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1979 - accuracy: 0.8750\n",
      "Epoch 00469: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9138 - val_loss: 0.6013 - val_accuracy: 0.8137\n",
      "Epoch 470/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1722 - accuracy: 0.9062\n",
      "Epoch 00470: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1499 - accuracy: 0.9360 - val_loss: 0.6004 - val_accuracy: 0.8137\n",
      "Epoch 471/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2983 - accuracy: 0.8438\n",
      "Epoch 00471: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 0.9187 - val_loss: 0.6585 - val_accuracy: 0.8431\n",
      "Epoch 472/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9688\n",
      "Epoch 00472: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1619 - accuracy: 0.9163 - val_loss: 0.6527 - val_accuracy: 0.8137\n",
      "Epoch 473/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1856 - accuracy: 0.9062\n",
      "Epoch 00473: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9212 - val_loss: 0.6803 - val_accuracy: 0.8333\n",
      "Epoch 474/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1278 - accuracy: 0.9375\n",
      "Epoch 00474: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9138 - val_loss: 0.6880 - val_accuracy: 0.7941\n",
      "Epoch 475/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1785 - accuracy: 0.9375\n",
      "Epoch 00475: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9335 - val_loss: 0.7537 - val_accuracy: 0.8431\n",
      "Epoch 476/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3351 - accuracy: 0.8750\n",
      "Epoch 00476: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9409 - val_loss: 0.7946 - val_accuracy: 0.8431\n",
      "Epoch 477/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1739 - accuracy: 0.9375\n",
      "Epoch 00477: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9335 - val_loss: 0.7310 - val_accuracy: 0.8431\n",
      "Epoch 478/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1899 - accuracy: 0.8438\n",
      "Epoch 00478: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9310 - val_loss: 0.7382 - val_accuracy: 0.8235\n",
      "Epoch 479/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1864 - accuracy: 0.9688\n",
      "Epoch 00479: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1681 - accuracy: 0.9089 - val_loss: 0.6477 - val_accuracy: 0.8235\n",
      "Epoch 480/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1937 - accuracy: 0.9375\n",
      "Epoch 00480: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9261 - val_loss: 0.6945 - val_accuracy: 0.8431\n",
      "Epoch 481/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2422 - accuracy: 0.8750\n",
      "Epoch 00481: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1522 - accuracy: 0.9286 - val_loss: 0.7272 - val_accuracy: 0.8529\n",
      "Epoch 482/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1378 - accuracy: 0.9688\n",
      "Epoch 00482: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2085 - accuracy: 0.9163 - val_loss: 0.7496 - val_accuracy: 0.8235\n",
      "Epoch 483/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9688\n",
      "Epoch 00483: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.8941 - val_loss: 0.6291 - val_accuracy: 0.8431\n",
      "Epoch 484/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1612 - accuracy: 0.9062\n",
      "Epoch 00484: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1733 - accuracy: 0.9187 - val_loss: 0.6944 - val_accuracy: 0.8333\n",
      "Epoch 485/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0611 - accuracy: 0.9688\n",
      "Epoch 00485: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.9286 - val_loss: 0.7446 - val_accuracy: 0.8333\n",
      "Epoch 486/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1324 - accuracy: 0.9062\n",
      "Epoch 00486: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9089 - val_loss: 0.6798 - val_accuracy: 0.8431\n",
      "Epoch 487/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1219 - accuracy: 0.9375\n",
      "Epoch 00487: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1561 - accuracy: 0.9113 - val_loss: 0.7264 - val_accuracy: 0.8529\n",
      "Epoch 488/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0906 - accuracy: 0.9375\n",
      "Epoch 00488: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1890 - accuracy: 0.9138 - val_loss: 0.6548 - val_accuracy: 0.8627\n",
      "Epoch 489/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0758 - accuracy: 0.9375\n",
      "Epoch 00489: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1754 - accuracy: 0.9113 - val_loss: 0.6848 - val_accuracy: 0.8529\n",
      "Epoch 490/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1310 - accuracy: 0.9062\n",
      "Epoch 00490: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9015 - val_loss: 0.6571 - val_accuracy: 0.8529\n",
      "Epoch 491/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1485 - accuracy: 0.8750\n",
      "Epoch 00491: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1394 - accuracy: 0.9236 - val_loss: 0.6104 - val_accuracy: 0.8627\n",
      "Epoch 492/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1269 - accuracy: 0.9688\n",
      "Epoch 00492: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1704 - accuracy: 0.8966 - val_loss: 0.6283 - val_accuracy: 0.8627\n",
      "Epoch 493/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0944 - accuracy: 0.9688\n",
      "Epoch 00493: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9310 - val_loss: 0.6352 - val_accuracy: 0.8725\n",
      "Epoch 494/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3125 - accuracy: 0.7812\n",
      "Epoch 00494: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9064 - val_loss: 0.7017 - val_accuracy: 0.8824\n",
      "Epoch 495/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1804 - accuracy: 0.8750\n",
      "Epoch 00495: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1773 - accuracy: 0.9064 - val_loss: 0.7772 - val_accuracy: 0.8431\n",
      "Epoch 496/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1836 - accuracy: 0.8438\n",
      "Epoch 00496: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9187 - val_loss: 0.6949 - val_accuracy: 0.8529\n",
      "Epoch 497/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0751 - accuracy: 0.9688\n",
      "Epoch 00497: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1540 - accuracy: 0.9261 - val_loss: 0.7755 - val_accuracy: 0.8529\n",
      "Epoch 498/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 00498: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.9310 - val_loss: 0.7453 - val_accuracy: 0.8529\n",
      "Epoch 499/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2072 - accuracy: 0.8750\n",
      "Epoch 00499: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9236 - val_loss: 0.7696 - val_accuracy: 0.8627\n",
      "Epoch 500/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1376 - accuracy: 0.9062\n",
      "Epoch 00500: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.9335 - val_loss: 0.6428 - val_accuracy: 0.8333\n",
      "Epoch 501/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3000 - accuracy: 0.9375\n",
      "Epoch 00501: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9187 - val_loss: 0.6977 - val_accuracy: 0.8431\n",
      "Epoch 502/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1959 - accuracy: 0.9062\n",
      "Epoch 00502: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9236 - val_loss: 0.6288 - val_accuracy: 0.8627\n",
      "Epoch 503/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2409 - accuracy: 0.8750\n",
      "Epoch 00503: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1800 - accuracy: 0.9286 - val_loss: 0.6741 - val_accuracy: 0.8529\n",
      "Epoch 504/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1584 - accuracy: 0.9375\n",
      "Epoch 00504: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9261 - val_loss: 0.7316 - val_accuracy: 0.8529\n",
      "Epoch 505/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1278 - accuracy: 0.8750\n",
      "Epoch 00505: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9236 - val_loss: 0.6244 - val_accuracy: 0.8627\n",
      "Epoch 506/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1215 - accuracy: 0.9375\n",
      "Epoch 00506: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9212 - val_loss: 0.6995 - val_accuracy: 0.8725\n",
      "Epoch 507/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1098 - accuracy: 0.9375\n",
      "Epoch 00507: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9064 - val_loss: 0.7105 - val_accuracy: 0.8627\n",
      "Epoch 508/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0879 - accuracy: 0.9375\n",
      "Epoch 00508: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1682 - accuracy: 0.9261 - val_loss: 0.6601 - val_accuracy: 0.8725\n",
      "Epoch 509/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0732 - accuracy: 0.9375\n",
      "Epoch 00509: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9261 - val_loss: 0.6998 - val_accuracy: 0.8431\n",
      "Epoch 510/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0630 - accuracy: 1.0000\n",
      "Epoch 00510: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1659 - accuracy: 0.8990 - val_loss: 0.7052 - val_accuracy: 0.8627\n",
      "Epoch 511/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0482 - accuracy: 1.0000\n",
      "Epoch 00511: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1693 - accuracy: 0.9261 - val_loss: 0.7257 - val_accuracy: 0.8725\n",
      "Epoch 512/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1401 - accuracy: 0.9375\n",
      "Epoch 00512: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9113 - val_loss: 0.7147 - val_accuracy: 0.8627\n",
      "Epoch 513/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2288 - accuracy: 0.9062\n",
      "Epoch 00513: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9163 - val_loss: 0.7472 - val_accuracy: 0.8431\n",
      "Epoch 514/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2139 - accuracy: 0.9375\n",
      "Epoch 00514: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2048 - accuracy: 0.9286 - val_loss: 0.7345 - val_accuracy: 0.8627\n",
      "Epoch 515/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1949 - accuracy: 0.9375\n",
      "Epoch 00515: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9163 - val_loss: 0.7247 - val_accuracy: 0.8333\n",
      "Epoch 516/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1264 - accuracy: 0.9375\n",
      "Epoch 00516: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1824 - accuracy: 0.9015 - val_loss: 0.7390 - val_accuracy: 0.8333\n",
      "Epoch 517/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1393 - accuracy: 0.9375\n",
      "Epoch 00517: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9236 - val_loss: 0.7750 - val_accuracy: 0.8529\n",
      "Epoch 518/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0973 - accuracy: 0.9062\n",
      "Epoch 00518: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1667 - accuracy: 0.9039 - val_loss: 0.7888 - val_accuracy: 0.8529\n",
      "Epoch 519/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1743 - accuracy: 0.9062\n",
      "Epoch 00519: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1505 - accuracy: 0.9335 - val_loss: 0.8178 - val_accuracy: 0.8529\n",
      "Epoch 520/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2661 - accuracy: 0.9062\n",
      "Epoch 00520: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1590 - accuracy: 0.9089 - val_loss: 0.8241 - val_accuracy: 0.8431\n",
      "Epoch 521/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1096 - accuracy: 0.9062\n",
      "Epoch 00521: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9089 - val_loss: 0.7988 - val_accuracy: 0.8627\n",
      "Epoch 522/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0683 - accuracy: 0.9688\n",
      "Epoch 00522: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1302 - accuracy: 0.9384 - val_loss: 0.8172 - val_accuracy: 0.8235\n",
      "Epoch 523/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1608 - accuracy: 0.9375\n",
      "Epoch 00523: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1491 - accuracy: 0.9384 - val_loss: 0.7876 - val_accuracy: 0.8333\n",
      "Epoch 524/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0678 - accuracy: 0.9688\n",
      "Epoch 00524: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.9360 - val_loss: 0.8169 - val_accuracy: 0.8039\n",
      "Epoch 525/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1009 - accuracy: 1.0000\n",
      "Epoch 00525: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.9335 - val_loss: 0.8248 - val_accuracy: 0.8235\n",
      "Epoch 526/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2657 - accuracy: 0.9062\n",
      "Epoch 00526: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9483 - val_loss: 0.7934 - val_accuracy: 0.8235\n",
      "Epoch 527/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1941 - accuracy: 0.9375\n",
      "Epoch 00527: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1654 - accuracy: 0.9113 - val_loss: 0.6827 - val_accuracy: 0.8431\n",
      "Epoch 528/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1104 - accuracy: 0.9688\n",
      "Epoch 00528: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1325 - accuracy: 0.9483 - val_loss: 0.7954 - val_accuracy: 0.8235\n",
      "Epoch 529/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1576 - accuracy: 0.9062\n",
      "Epoch 00529: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1516 - accuracy: 0.9310 - val_loss: 0.8865 - val_accuracy: 0.8431\n",
      "Epoch 530/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1606 - accuracy: 0.9062\n",
      "Epoch 00530: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.9335 - val_loss: 0.7880 - val_accuracy: 0.8431\n",
      "Epoch 531/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1886 - accuracy: 0.9375\n",
      "Epoch 00531: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1479 - accuracy: 0.9335 - val_loss: 0.8132 - val_accuracy: 0.8235\n",
      "Epoch 532/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0758 - accuracy: 0.9375\n",
      "Epoch 00532: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9335 - val_loss: 0.7487 - val_accuracy: 0.8333\n",
      "Epoch 533/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1028 - accuracy: 1.0000\n",
      "Epoch 00533: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9310 - val_loss: 0.7351 - val_accuracy: 0.8529\n",
      "Epoch 534/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1785 - accuracy: 0.9375\n",
      "Epoch 00534: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.9433 - val_loss: 0.8003 - val_accuracy: 0.8725\n",
      "Epoch 535/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1242 - accuracy: 0.9375\n",
      "Epoch 00535: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9335 - val_loss: 0.8766 - val_accuracy: 0.8725\n",
      "Epoch 536/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0890 - accuracy: 0.9688\n",
      "Epoch 00536: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1332 - accuracy: 0.9384 - val_loss: 0.8635 - val_accuracy: 0.8725\n",
      "Epoch 537/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3330 - accuracy: 0.8750\n",
      "Epoch 00537: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1621 - accuracy: 0.9261 - val_loss: 0.8103 - val_accuracy: 0.8333\n",
      "Epoch 538/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2303 - accuracy: 0.8750\n",
      "Epoch 00538: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.9335 - val_loss: 0.9193 - val_accuracy: 0.8235\n",
      "Epoch 539/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9688\n",
      "Epoch 00539: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1382 - accuracy: 0.9409 - val_loss: 0.8375 - val_accuracy: 0.8627\n",
      "Epoch 540/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1035 - accuracy: 0.9688\n",
      "Epoch 00540: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9360 - val_loss: 0.8183 - val_accuracy: 0.8137\n",
      "Epoch 541/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1619 - accuracy: 0.9688\n",
      "Epoch 00541: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1750 - accuracy: 0.9310 - val_loss: 0.7806 - val_accuracy: 0.8137\n",
      "Epoch 542/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2468 - accuracy: 0.9062\n",
      "Epoch 00542: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1466 - accuracy: 0.9310 - val_loss: 0.6716 - val_accuracy: 0.8431\n",
      "Epoch 543/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2920 - accuracy: 0.9062\n",
      "Epoch 00543: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1819 - accuracy: 0.9310 - val_loss: 0.6855 - val_accuracy: 0.8431\n",
      "Epoch 544/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1314 - accuracy: 0.9375\n",
      "Epoch 00544: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1266 - accuracy: 0.9384 - val_loss: 0.7326 - val_accuracy: 0.8529\n",
      "Epoch 545/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1424 - accuracy: 0.9688\n",
      "Epoch 00545: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9261 - val_loss: 0.7702 - val_accuracy: 0.8627\n",
      "Epoch 546/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1037 - accuracy: 0.9375\n",
      "Epoch 00546: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.9384 - val_loss: 0.7519 - val_accuracy: 0.8824\n",
      "Epoch 547/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1055 - accuracy: 1.0000\n",
      "Epoch 00547: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9433 - val_loss: 0.8305 - val_accuracy: 0.8529\n",
      "Epoch 548/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1396 - accuracy: 0.9688\n",
      "Epoch 00548: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9458 - val_loss: 0.7942 - val_accuracy: 0.8137\n",
      "Epoch 549/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1009 - accuracy: 0.9375\n",
      "Epoch 00549: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9433 - val_loss: 0.8099 - val_accuracy: 0.8333\n",
      "Epoch 550/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1574 - accuracy: 0.9375\n",
      "Epoch 00550: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9458 - val_loss: 0.8691 - val_accuracy: 0.8529\n",
      "Epoch 551/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 00551: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9384 - val_loss: 0.8479 - val_accuracy: 0.8333\n",
      "Epoch 552/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1599 - accuracy: 0.9062\n",
      "Epoch 00552: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9458 - val_loss: 0.9094 - val_accuracy: 0.8529\n",
      "Epoch 553/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0834 - accuracy: 0.9375\n",
      "Epoch 00553: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9433 - val_loss: 0.8026 - val_accuracy: 0.8529\n",
      "Epoch 554/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2098 - accuracy: 0.9062\n",
      "Epoch 00554: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1976 - accuracy: 0.9113 - val_loss: 0.7802 - val_accuracy: 0.8529\n",
      "Epoch 555/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1715 - accuracy: 0.9375\n",
      "Epoch 00555: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9409 - val_loss: 0.7961 - val_accuracy: 0.8333\n",
      "Epoch 556/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1527 - accuracy: 0.9062\n",
      "Epoch 00556: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9113 - val_loss: 0.6885 - val_accuracy: 0.8627\n",
      "Epoch 557/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1092 - accuracy: 0.9375\n",
      "Epoch 00557: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.9163 - val_loss: 0.7092 - val_accuracy: 0.8627\n",
      "Epoch 558/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1922 - accuracy: 0.9375\n",
      "Epoch 00558: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9310 - val_loss: 0.7188 - val_accuracy: 0.8235\n",
      "Epoch 559/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1199 - accuracy: 1.0000\n",
      "Epoch 00559: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.9138 - val_loss: 0.7731 - val_accuracy: 0.8333\n",
      "Epoch 560/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2345 - accuracy: 0.9062\n",
      "Epoch 00560: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9163 - val_loss: 0.7820 - val_accuracy: 0.8039\n",
      "Epoch 561/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1850 - accuracy: 0.9062\n",
      "Epoch 00561: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1460 - accuracy: 0.9360 - val_loss: 0.8562 - val_accuracy: 0.8137\n",
      "Epoch 562/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1623 - accuracy: 0.9375\n",
      "Epoch 00562: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.9015 - val_loss: 0.7043 - val_accuracy: 0.7941\n",
      "Epoch 563/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2648 - accuracy: 0.8125\n",
      "Epoch 00563: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.8916 - val_loss: 0.7275 - val_accuracy: 0.7745\n",
      "Epoch 564/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1676 - accuracy: 0.8750\n",
      "Epoch 00564: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1394 - accuracy: 0.9409 - val_loss: 0.7320 - val_accuracy: 0.8333\n",
      "Epoch 565/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1796 - accuracy: 0.9062\n",
      "Epoch 00565: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9433 - val_loss: 0.8262 - val_accuracy: 0.8137\n",
      "Epoch 566/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0530 - accuracy: 0.9688\n",
      "Epoch 00566: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1451 - accuracy: 0.9433 - val_loss: 0.9016 - val_accuracy: 0.8137\n",
      "Epoch 567/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0728 - accuracy: 0.9688\n",
      "Epoch 00567: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9310 - val_loss: 0.8005 - val_accuracy: 0.8333\n",
      "Epoch 568/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 00568: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9236 - val_loss: 0.7756 - val_accuracy: 0.8137\n",
      "Epoch 569/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1677 - accuracy: 0.9375\n",
      "Epoch 00569: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1479 - accuracy: 0.9212 - val_loss: 0.7025 - val_accuracy: 0.8137\n",
      "Epoch 570/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0993 - accuracy: 0.9688\n",
      "Epoch 00570: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1460 - accuracy: 0.9433 - val_loss: 0.7656 - val_accuracy: 0.7843\n",
      "Epoch 571/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1769 - accuracy: 0.8438\n",
      "Epoch 00571: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9163 - val_loss: 0.8245 - val_accuracy: 0.8137\n",
      "Epoch 572/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1426 - accuracy: 0.9375\n",
      "Epoch 00572: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9310 - val_loss: 0.8960 - val_accuracy: 0.8039\n",
      "Epoch 573/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2067 - accuracy: 0.9375\n",
      "Epoch 00573: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9384 - val_loss: 0.9242 - val_accuracy: 0.8137\n",
      "Epoch 574/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0626 - accuracy: 1.0000\n",
      "Epoch 00574: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1459 - accuracy: 0.9384 - val_loss: 0.8956 - val_accuracy: 0.8137\n",
      "Epoch 575/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2208 - accuracy: 0.8438\n",
      "Epoch 00575: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9261 - val_loss: 0.8788 - val_accuracy: 0.8137\n",
      "Epoch 576/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1591 - accuracy: 0.9062\n",
      "Epoch 00576: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9335 - val_loss: 0.7948 - val_accuracy: 0.8725\n",
      "Epoch 577/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1269 - accuracy: 0.9375\n",
      "Epoch 00577: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9360 - val_loss: 0.8227 - val_accuracy: 0.8529\n",
      "Epoch 578/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2494 - accuracy: 0.8125\n",
      "Epoch 00578: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1611 - accuracy: 0.9212 - val_loss: 0.7469 - val_accuracy: 0.8333\n",
      "Epoch 579/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1472 - accuracy: 0.9375\n",
      "Epoch 00579: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.1414 - accuracy: 0.9286 - val_loss: 0.8361 - val_accuracy: 0.8333\n",
      "Epoch 580/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1765 - accuracy: 0.9062\n",
      "Epoch 00580: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1477 - accuracy: 0.9310 - val_loss: 0.8467 - val_accuracy: 0.8529\n",
      "Epoch 581/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1653 - accuracy: 0.9375\n",
      "Epoch 00581: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1449 - accuracy: 0.9286 - val_loss: 0.8956 - val_accuracy: 0.8333\n",
      "Epoch 582/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 00582: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1472 - accuracy: 0.9335 - val_loss: 0.9700 - val_accuracy: 0.8235\n",
      "Epoch 583/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1179 - accuracy: 0.9688\n",
      "Epoch 00583: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9384 - val_loss: 0.8249 - val_accuracy: 0.8039\n",
      "Epoch 584/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1580 - accuracy: 0.8750\n",
      "Epoch 00584: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1507 - accuracy: 0.9138 - val_loss: 0.7194 - val_accuracy: 0.8235\n",
      "Epoch 585/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1042 - accuracy: 0.9375\n",
      "Epoch 00585: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.9138 - val_loss: 0.8041 - val_accuracy: 0.8431\n",
      "Epoch 586/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1864 - accuracy: 0.8750\n",
      "Epoch 00586: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1937 - accuracy: 0.9089 - val_loss: 0.7148 - val_accuracy: 0.8529\n",
      "Epoch 587/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1384 - accuracy: 0.9062\n",
      "Epoch 00587: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1989 - accuracy: 0.8990 - val_loss: 0.6354 - val_accuracy: 0.8333\n",
      "Epoch 588/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1935 - accuracy: 0.8750\n",
      "Epoch 00588: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9163 - val_loss: 0.6757 - val_accuracy: 0.8333\n",
      "Epoch 589/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1447 - accuracy: 0.9375\n",
      "Epoch 00589: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9384 - val_loss: 0.6736 - val_accuracy: 0.8824\n",
      "Epoch 590/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.2471 - accuracy: 0.8750\n",
      "Epoch 00590: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1902 - accuracy: 0.9310 - val_loss: 0.6618 - val_accuracy: 0.8431\n",
      "Epoch 591/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9688\n",
      "Epoch 00591: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9286 - val_loss: 0.6046 - val_accuracy: 0.8529\n",
      "Epoch 592/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1918 - accuracy: 0.9062\n",
      "Epoch 00592: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1333 - accuracy: 0.9384 - val_loss: 0.7536 - val_accuracy: 0.8235\n",
      "Epoch 593/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1044 - accuracy: 0.9688\n",
      "Epoch 00593: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9261 - val_loss: 0.7636 - val_accuracy: 0.8333\n",
      "Epoch 594/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1416 - accuracy: 0.9062\n",
      "Epoch 00594: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1529 - accuracy: 0.9261 - val_loss: 0.6886 - val_accuracy: 0.8529\n",
      "Epoch 595/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1248 - accuracy: 0.9688\n",
      "Epoch 00595: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1463 - accuracy: 0.9360 - val_loss: 0.6611 - val_accuracy: 0.8627\n",
      "Epoch 596/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 00596: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9458 - val_loss: 0.6850 - val_accuracy: 0.8725\n",
      "Epoch 597/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1445 - accuracy: 0.9062\n",
      "Epoch 00597: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1466 - accuracy: 0.9384 - val_loss: 0.6600 - val_accuracy: 0.8627\n",
      "Epoch 598/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.1011 - accuracy: 0.9375\n",
      "Epoch 00598: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1311 - accuracy: 0.9360 - val_loss: 0.7325 - val_accuracy: 0.8725\n",
      "Epoch 599/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0726 - accuracy: 1.0000\n",
      "Epoch 00599: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1299 - accuracy: 0.9507 - val_loss: 0.7071 - val_accuracy: 0.8627\n",
      "Epoch 600/600\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.0698 - accuracy: 0.9375\n",
      "Epoch 00600: val_loss did not improve from 0.47994\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9458 - val_loss: 0.8039 - val_accuracy: 0.8725\n",
      "Training completed in time:  0:00:40.712139\n"
     ]
    }
   ],
   "source": [
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 600\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.25489974021912\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3424139e+02,  1.1067206e+02, -6.6630416e+00,  4.1293573e+00,\n",
       "       -1.1308483e+01, -1.0980946e+01, -1.5250362e+01,  1.9236928e+00,\n",
       "       -1.4809805e+01,  2.3802032e-01, -7.7702599e+00,  3.6280439e+00,\n",
       "       -5.6303148e+00, -6.6174512e+00, -7.5932426e+00, -5.3561540e+00,\n",
       "       -2.5885587e+00, -5.4182224e+00, -5.7990346e+00, -5.9867935e+00,\n",
       "       -1.1707730e+00, -2.6819625e+00, -3.1655645e-01,  2.5990316e-01,\n",
       "       -9.8999280e-01,  5.7512655e+00, -1.5841848e+00,  4.2109928e-01,\n",
       "        1.8964884e+00,  3.1722341e+00,  2.6149845e+00,  5.7462988e+00,\n",
       "        2.8583210e+00,  1.4889371e+00,  3.4892721e+00, -8.1730686e-02,\n",
       "       -3.9798820e-01,  8.0889344e-01, -1.4317317e+00, -6.2499964e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-bc459dba29cd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Some Test Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.7407809e+02  9.4333420e+01 -1.8600550e+01  3.6770454e+01\n",
      " -8.4028921e+00 -1.6089230e+01 -1.5080973e+01 -2.5797531e+01\n",
      " -8.6793633e+00 -1.1694519e+01 -1.3656996e+01 -1.4920211e+01\n",
      " -1.0464228e+01 -6.5972204e+00 -8.0304260e+00 -5.9242659e+00\n",
      " -3.3941271e+00 -2.4859211e+00 -8.9847240e+00 -4.2518182e+00\n",
      " -2.0214176e+00 -5.9973407e+00  3.7480056e-01 -2.4540784e+00\n",
      " -4.6795554e+00 -1.7582238e+00 -4.5259929e+00 -2.8423886e+00\n",
      " -6.7155737e-01  1.0814966e-01  2.2921562e+00  2.6345947e+00\n",
      "  2.1705348e+00 -5.2751881e-01 -6.9644945e-03  1.1093524e+00\n",
      " -3.1303546e+00 -2.4162803e+00 -5.6212240e-01 -2.0515184e+00]\n",
      "[[-3.7407809e+02  9.4333420e+01 -1.8600550e+01  3.6770454e+01\n",
      "  -8.4028921e+00 -1.6089230e+01 -1.5080973e+01 -2.5797531e+01\n",
      "  -8.6793633e+00 -1.1694519e+01 -1.3656996e+01 -1.4920211e+01\n",
      "  -1.0464228e+01 -6.5972204e+00 -8.0304260e+00 -5.9242659e+00\n",
      "  -3.3941271e+00 -2.4859211e+00 -8.9847240e+00 -4.2518182e+00\n",
      "  -2.0214176e+00 -5.9973407e+00  3.7480056e-01 -2.4540784e+00\n",
      "  -4.6795554e+00 -1.7582238e+00 -4.5259929e+00 -2.8423886e+00\n",
      "  -6.7155737e-01  1.0814966e-01  2.2921562e+00  2.6345947e+00\n",
      "   2.1705348e+00 -5.2751881e-01 -6.9644945e-03  1.1093524e+00\n",
      "  -3.1303546e+00 -2.4162803e+00 -5.6212240e-01 -2.0515184e+00]]\n",
      "(1, 40)\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"audio/test/S15_P11_F.wav\"\n",
    "# S53_P16_F.wav\n",
    "# S15_P12_F\n",
    "# S47_P5_M\n",
    "# S66_P13_F\n",
    "# S80_P9_F\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "print(mfccs_scaled_features)\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "print(mfccs_scaled_features)\n",
    "print(mfccs_scaled_features.shape)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "prediction_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (prediction_class == 1):\n",
    "#     print(\"Well Done.You have pronunced words correctly.\")\n",
    "# else:\n",
    "#     print(\"Your pronunciation need to be improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with some text data to give feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on...\n",
      "new serials transition from FTP to mmp demonstrate the benefits and drawbacks of each system in the 1970 New Zealand growth disillusioned with the two-party system if PP did not provide water with another viable option however the late in third party received a considerable 16% of the word in 1978 but again only one of the two seats in Parliament three years later they would she was up to 21% but they gain only to seeds a royal Commission subsequent recommended the shift to m m p and in 1993 straight right refund amount was held that was in favour of the reform\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "if (prediction_class == 1):\n",
    "    print(\"Well Done.You have pronunced words correctly.\")\n",
    "else:\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(filename) as source:\n",
    "        audio = r.listen(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio)\n",
    "            print('working on...')\n",
    "            print(text)\n",
    "        except:\n",
    "            print('Sorry...run again...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Use of [] in place of [s]..Have pronunciation errors when speaking cell, sell, seat like words.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "\n",
    "txt = text\n",
    "\n",
    "#Check if \"ain\" is present at the end of a WORD:\n",
    "\n",
    "# re.findall(r\"\\band\\b|\\bor\\b|\\bnot\\b\", \"These are oranges and apples and pears, but not pinapples or ..\")\n",
    "\n",
    "# torsional mode of vibration is a consequence of\n",
    "set1 = re.findall(r\"\\bbowling\\b|\\bball\\b|\\browing\\b|\\brow\\b|\\bhall\\b|\\bholes\\b\", txt)\n",
    "set2 = re.findall(r\"\\btelephone\\b|\\bphones\\b\", txt)\n",
    "set3 = re.findall(r\"\\bfruit\\b|\\bfly\\b|\\bfraction\\b|\\bfan\\b\", txt)\n",
    "set4 = re.findall(r\"\\bpool\\b|\\bpenalties\\b|\\btransportation\\b|\\btransported\\b\", txt)\n",
    "set5 = re.findall(r\"\\bseat\\b|\\bseats\\b|\\bcent\\b\", txt)\n",
    "set6 = re.findall(r\"\\bzones\\b|\\blizard\\b|\\bzoo\\b\", txt)\n",
    "set7 = re.findall(r\"\\bwaist\\b|\\bvibration\\b|\\bwind\\b|\\bweight\\b\", txt)\n",
    "\n",
    "# print(x)\n",
    "\n",
    "\n",
    "if set1:\n",
    "  print(\"Error: Confusing /o/ and / /..Have pronunciation errors when speaking bowling, rowers, hall like words.\")\n",
    "if set2:\n",
    "  print(\"Error: Overuse of / / rather than /o/..Have pronunciation errors when speaking phone, yoghut like words.\")\n",
    "if set3:\n",
    "  print(\"Error: Confusing /p/ and /f/..Have pronunciation errors when speaking fan, pan, profit like words.\")\n",
    "if set4:\n",
    "  print(\"Error: Overuse of /f /..Have pronunciation errors when speaking Airport, pool like words.\")\n",
    "if set5:\n",
    "  print(\"Error: Use of [] in place of [s]..Have pronunciation errors when speaking cell, sell, seat like words.\")\n",
    "if set6:\n",
    "  print(\"Error: Use of [s] in place of [z]..Have pronunciation errors when speaking zoo, zip like words.\")\n",
    "if set7:\n",
    "  print(\"Error: Use of [wr] and [ar]..Have pronunciation errors when speaking winter, win, west, vest, vantage like words.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
